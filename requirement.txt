import pandas as pd
import requests

# Step 1: Define retriever configurations
retriever_configs = {
    "config_1": {"retriever_type": "faiss", "top_k": 3, "mode": "similarity"},
    "config_2": {"retriever_type": "faiss", "top_k": 5, "mode": "mmr"},
    "config_3": {"retriever_type": "bm25", "top_k": 3, "k1": 1.2, "b": 0.75},
    "config_4": {"retriever_type": "bm25", "top_k": 5, "k1": 2.0, "b": 1.0},
    "config_5": {"retriever_type": "multiquery", "top_k": 5, "merge_strategy": "concatenate", "llm_variant": "gpt-3.5"},
}

# Step 2: Define evaluation questions
questions = [
    "What is the role of fiduciary duties?",
    "Explain the concept of risk-adjusted returns.",
    "How does SSGA manage index tracking error?",
    "What is the purpose of an investment policy statement?",
    "Describe the difference between active and passive management."
]

# Step 3: Replace this with your actual dev endpoint
base_url = "http://your-dev-app/api/ask"

# Step 4: Collect responses
all_results = []

for q in questions:
    row = {"question": q}
    for config_id, config in retriever_configs.items():
        try:
            payload = {
                "question": q,
                "retriever_config": config
            }
            response = requests.post(base_url, json=payload, timeout=30)
            result = response.json().get("answer", "No response")
        except Exception as e:
            result = f"Error: {str(e)}"
        row[config_id + "_response"] = result
    all_results.append(row)

# Step 5: Export to Excel
df = pd.DataFrame(all_results)
df.to_excel("retriever_comparison_results.xlsx", index=False)
print("Saved results to retriever_comparison_results.xlsx")






import pandas as pd
import tiktoken

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        return len(encoding.encode(text))
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return 0

# Load the Excel file
df = pd.read_excel("your_input_file.xlsx")  # Change to your actual file path

# Calculate tokens for each Answer
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: calculate_tokens(str(x)) if pd.notna(x) else 0)

# Export the updated DataFrame to a new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)





import pandas as pd
import tiktoken  # for token counting

# Load Excel file
df = pd.read_excel("your_file.xlsx")  # Update with your file path

# Choose tokenizer (e.g., for OpenAI's gpt-3.5-turbo or gpt-4)
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# Count tokens in 'Answer' column
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: len(encoding.encode(str(x))) if pd.notna(x) else 0)

# Save to new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)



On the call, you can ask me different scenarios, and I will demonstrate them. If it's not up to your expectations, I will stop and work on it. I'm requesting the call because I want to avoid facing similar issues again in the future.





For testing, I injected a document related to the SOP. After the injection, I asked a few questions based on that document, but it did not return the answers. Instead, it gave the following response to the questions:


For testing the procedural questions, I have rerun the pipeline for the Discrimination Report domain. The QA pairs were generated successfully, and I am currently verifying the answers to the procedural questions




# LLaMA-3.1-8B-Instruct Fine-Tuning with LoRA and MLflow

# Step 1: Environment Setup
import random, os, torch, numpy as np, pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
SEED = 42
seed_everything(SEED)

# Step 2: Load Tokenizer and Model
MODEL_PATH = "/path/to/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, model_max_length=1024, padding_side="left", truncation=True, padding=True
)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
model.resize_token_embeddings(len(tokenizer))

# Step 3: Load and Format Dataset
df = pd.read_csv("qa_dataset.csv")[["Question", "Answer"]].dropna().sample(frac=1).reset_index(drop=True)
def format_example(row):
    messages = [
        {"role": "system", "content": "Answer the question"},
        {"role": "user", "content": row["Question"]},
        {"role": "assistant", "content": row["Answer"]}
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False)
df["text"] = df.apply(format_example, axis=1)

# Step 4: Token Count Check
def count_tokens(row):
    return len(tokenizer(row["text"], add_special_tokens=True)["input_ids"])
df["token_count"] = df.apply(count_tokens, axis=1)

# Step 5: Dataset Split and Convert to HF Format
val, test = train_test_split(df, test_size=0.2, random_state=SEED)
dataset = DatasetDict({
    "train": Dataset.from_pandas(df),
    "validation": Dataset.from_pandas(val),
    "test": Dataset.from_pandas(test)
})

# Step 6: Configure LoRA
from peft import LoraConfig, TaskType, get_peft_model
lora_config = LoraConfig(
    r=64, lora_alpha=128, target_modules="all-linear", lora_dropout=0.1,
    bias="none", task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# Step 7: Define Training Config with MLflow
from trl import SFTConfig
OUTPUT_DIR = "/path/to/output"
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR, dataset_text_field="text", num_train_epochs=4,
    max_seq_length=1024, gradient_checkpointing=False, per_device_train_batch_size=2,
    per_device_eval_batch_size=2, gradient_accumulation_steps=4, learning_rate=1e-4,
    fp16=True, eval_strategy="steps", logging_steps=10, save_strategy="steps",
    save_steps=0.2, eval_steps=0.2, lr_scheduler_type="cosine", report_to="mlflow", seed=SEED
)

# Step 8: Train with SFTTrainer and Log to MLflow
from trl import SFTTrainer
import mlflow
with mlflow.start_run():
    trainer = SFTTrainer(
        model=model,
        args=sft_config,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    mlflow.log_params(sft_config.to_dict())
    mlflow.log_artifact(OUTPUT_DIR)

# Step 9: Merge LoRA Adapters
from peft import PeftModel
peft_model = PeftModel.from_pretrained(model, OUTPUT_DIR)
merged_model = peft_model.merge_and_unload()

# Step 10: Register Final Model to MLflow
from transformers import pipeline
mlflow.transformers.log_model(
    transformers_model=pipeline("text-generation", model=merged_model, tokenizer=tokenizer),
    artifact_path="merged_model_without_model_config",
    task="llm/v1/chat"
)

# Step 11: Run Inference Example
mlflow_model = mlflow.pyfunc.load_model("runs:/<run_id>/merged_model_without_model_config")
mlflow_model.predict({
    "messages": [
        {"role": "system", "content": "Answer the question."},
        {"role": "user", "content": "What is LoRA?"}
    ],
    "temperature": 0.5,
    "max_tokens": 100
})

# Step 12: What is LoRA?
# LoRA (Low-Rank Adaptation) updates a small set of trainable matrices A and B:
# W' = W + A @ B
# W = frozen weight matrix
# A = [r x d], B = [d x k], where r is much smaller
# Benefits: 90%+ parameter savings, low memory, no catastrophic forgetting, modular
# Use merge_and_unload() to finalize model after training






### LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)

---

#### 1. Environment Initialization

* **Purpose**: Set up the Python environment and load libraries.
* **Tools**: `transformers`, `datasets`, `torch`, `pandas`, `mlflow`, `seaborn`, `matplotlib`
* **Action**: Import required packages, set a global random seed (`SEED = 42`), and configure plotting.

---

#### 2. Load Tokenizer and Model

* **Purpose**: Initialize tokenizer and quantized base model for fine-tuning.
* **Tools**: `AutoTokenizer`, `AutoModelForCausalLM`
* **Action**:

  * Load tokenizer from the base model path.
  * Set `pad_token` = `eos_token`, padding side = "left" (required for causal LMs).
  * Load the quantized model (e.g., 4-bit with `BitsAndBytesConfig` if required).

---

#### 3. Dataset Preparation

* **Purpose**: Read, clean, and prepare data for fine-tuning.
* **Tools**: `pandas`
* **Action**:

  * Load QA dataset from CSV.
  * Keep only `Question` and `Answer` columns.
  * Drop missing values and shuffle the rows.

---

#### 4. Format Prompts for Chat

* **Purpose**: Convert QA rows into structured multi-turn chat format for LLM training.
* **Tools**: `tokenizer.apply_chat_template()`
* **Action**:

  * Construct `[system → user → assistant]` style chat messages.
  * Generate text prompts for training using tokenizer templates.

---

#### 5. Analyze Token Length

* **Purpose**: Ensure inputs fit within model's `max_seq_length` (1024 tokens).
* **Tools**: `tokenizer`, `matplotlib`, `PercentFormatter`
* **Action**:

  * Tokenize each example to count tokens.
  * Plot histograms to visualize token distribution.

---

#### 6. Split Dataset

* **Purpose**: Create training, validation, and test sets.
* **Tools**: `train_test_split` from `sklearn`
* **Action**:

  * Split into 80% train and 20% validation sets.
  * Convert them into HuggingFace `Dataset` objects using `Dataset.from_pandas()`.

---

#### 7. Configure PEFT (LoRA)

* **Purpose**: Reduce memory cost using parameter-efficient fine-tuning.
* **Tools**: `peft.LoraConfig`, `get_peft_model()`
* **Action**:

  * Apply LoRA only on linear layers.
  * Configure rank (`r`), alpha, dropout, and target modules.
  * Prepare model for training.

---

#### 8. Define Training Configuration

* **Purpose**: Set training hyperparameters and logging behavior.
* **Tools**: `trl.SFTConfig`
* **Key Settings**:

  * `max_seq_length = 1024`
  * `batch_size = 2`
  * `gradient_accumulation = 4`
  * `learning_rate = 1e-4`
  * `eval_steps = 0.2`, `save_steps = 0.2`
  * `fp16 = True`, `lr_scheduler = cosine`
  * `report_to = "mlflow"`

---

#### 9. Train Model

* **Purpose**: Fine-tune the base model with LoRA layers.
* **Tools**: `SFTTrainer`
* **Action**:

  * Train on the `train` set, evaluate on `validation`.
  * Save the model and tokenizer to `OUTPUT_DIR`.
  * Log everything to MLflow inside `with mlflow.start_run()`.

---

#### 10. Merge LoRA with Base Model

* **Purpose**: Convert adapter-based model into a standalone fine-tuned model.
* **Tools**: `PeftModel`, `merge_and_unload()`
* **Action**:

  * Load trained LoRA model.
  * Merge adapters into base model to remove dependency on LoRA.

---

#### 11. Register and Log Model with MLflow

* **Purpose**: Store, version, and serve model in production.
* **Tools**: `mlflow.transformers.log_model()`, `mlflow.register_model()`
* **Action**:

  * Log the `pipeline("text-generation")` model.
  * Register it to Databricks model registry.

---

#### 12. Copy Artifacts to UC Volume (Optional)

* **Purpose**: Store model artifacts in a Unified Catalog (UC) for sharing or backup.
* **Tools**: `mlflow.tracking.MlflowClient()`
* **Action**:

  * Download all model artifacts and copy to desired UC directory path.

---

#### 13. Inference

* **Purpose**: Test the model with a prompt.
* **Tools**: `mlflow_model.predict()`
* **Input**: JSON with `messages` (system, user), `temperature`, and `max_tokens`
* **Output**: Model-generated response.



LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)
Environment Setup:
Import all required libraries (transformers, datasets, torch, mlflow, etc.).

Tokenizer & Model Loading:
Load the tokenizer and quantized LLaMA model. Set pad token as EOS and padding side as "left" for causal LM.

Dataset Loading:
Read the question-answer dataset from a CSV file. Drop nulls, shuffle, and retain relevant columns.

Prompt Formatting:
Format data into structured "chat" format (system, user, assistant roles) using the tokenizer’s chat template.

Token Count Analysis:
Calculate and visualize token lengths to ensure most samples are under 1024 tokens (model limit).

Train/Validation Split:
Use train_test_split() to divide data into training and validation sets for evaluation.

Dataset Conversion:
Convert Pandas DataFrames to HuggingFace DatasetDict for training compatibility.

PEFT (LoRA) Configuration:
Define LoraConfig to fine-tune only a small subset of layers efficiently using LoRA adapters.

Training Configuration:
Set hyperparameters like batch size, epochs, learning rate, warmup ratio, and enable MLflow tracking.

Model Training:
Fine-tune the model using SFTTrainer with chat-formatted inputs. Log metrics and artifacts via MLflow.

Merge Adapters:
After training, merge LoRA adapters with the base model to create a fully functional fine-tuned model.

Model Logging:
Log the merged model to MLflow using transformers.log_model() for future deployment.

Model Registration:
Register the model version in Databricks MLflow Registry with a custom name.

Artifact Copy (Optional):
Download and copy model artifacts from MLflow run to a Unified Catalog directory.

Inference:
Load the logged model and run inference using mlflow_model.predict() with system-user prompts.

Let me know if you want this exported to a .md or .pdf file.
















ChatGPT can make mistakes. Check important info. See Cookie Preferences.








Hi Chandreshkumar Dedani, Vivek. The results from the previous run were good. I have now changed the dataset to include 3 diverse procedural questions. Please use this dataset for your training, and also use the parameters: context length = 1024 and epochs = 4."






import pandas as pd

# Load your Excel or CSV file
df = pd.read_excel('your_dataset.xlsx')  # or use read_csv if it's a CSV

# Group by the 'Original Question' column and select the first 3 entries per group
filtered_df = df.groupby('Original Question', group_keys=False).head(3)

# Save the result if needed
filtered_df.to_excel('filtered_output.xlsx', index=False)

print("Filtered dataset with first 3 questions per original question.")





import json
import pandas as pd

# Step 1: Load JSON file
with open('your_file.json', 'r') as file:
    data = json.load(file)

# Step 2: Normalize (flatten) the JSON data
df = pd.json_normalize(data, sep='_')

# Step 3: Save to Excel
df.to_excel('converted_output.xlsx', index=False)

print("Conversion complete. File saved as 'converted_output.xlsx'")






import os
import time
import json
import dotenv
import pandas as pd
from langchain.chat_models import AzureChatOpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables
dotenv.load_dotenv()

# Azure LLM Configuration
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

llm = AzureChatOpenAI(
    temperature=0,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name="ssgpt-4"
)

# ========================== Step 1: Convert Excel to JSON ==========================

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")
    json_data = [{"question": row["Question"], "answer": row["Answer"]} for _, row in df.iterrows()]
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=4, ensure_ascii=False)
    print(f"Converted Excel to JSON at {output_json_path}")
    return output_json_path

# ========================== Step 2: Read JSON ==========================

def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"Read {len(data)} records from {file_path}")
        return data
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

# ========================== Step 3: Build Prompt ==========================

def build_rating_prompt(question, answer):
    return f"""
Rate each question-answer pair on a scale from 1-10, based on:
- Accuracy (0-3): factual correctness
- Relevance (0-2): relevance to content
- Clarity (0-2): clear language
- Usefulness (0-3): value for model learning

YOU MUST RETURN A VALID JSON OBJECT WITH THIS EXACT SCHEMA:
{{
  "question": "{question}",
  "answer": "{answer}",
  "rating": <1-10>
}}

*** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE ***
"""

# ========================== Step 4: Validate with LLM ==========================

def validate_with_llm(entry, index, retries=3):
    prompt = build_rating_prompt(entry["question"], entry["answer"])
    for attempt in range(retries):
        try:
            response = llm.predict(prompt)
            result = json.loads(response)
            return index, result
        except Exception as e:
            print(f"Retry {attempt+1} failed: {e}")
            time.sleep(2)
    return index, {
        "question": entry["question"],
        "answer": entry["answer"],
        "rating": 0
    }

# ========================== Step 5: Run Pipeline ==========================

def full_pipeline(input_excel, final_excel_output, final_json_output, max_workers=5):
    temp_json_path = convert_excel_to_json(input_excel)
    qa_data = read_json_file(temp_json_path)
    results = [None] * len(qa_data)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(validate_with_llm, entry, i) for i, entry in enumerate(qa_data)]
        for future in as_completed(futures):
            index, result = future.result()
            results[index] = result

    # Save to Excel and JSON
    pd.DataFrame(results).to_excel(final_excel_output, index=False)
    with open(final_json_output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"Pipeline complete. Results saved to {final_excel_output} and {final_json_output}")









Following up on the productivity improvement, here’s a summary of the value delivered by migrating from the Hugging Face summarization model to the new LLM-based chatbot:

1. Key Difference Observed
Previous (Hugging Face Model): Users faced delays due to slower response times and limited interactivity (only summary generation).

Current (LLM Chatbot): Delivers instant, context-aware answers through natural conversation, significantly reducing turnaround time.

2. Quantified Benefit (Illustrative Example)
Average handling time (per query):

Hugging Face: ~12 mins (includes manual reading + follow-ups)

LLM Chatbot: ~4 mins (direct answers with follow-up support)

Monthly queries handled: 2,000

FTE rate: $30/hour

Productivity Gain = (12 – 4)/60 × 2,000 × $30 = $8,000/month

3. Next Steps
I will engage with actual users to validate the above assumptions and share a data-backed benefit report.

Let me know if you'd like this structured as a slide or summary deck as well.

Best regards,
[Your Name]

Would you like me to help you turn this into a one-slide summary for stakeholders?
















ChatGPT can make mistakes. Check important info. See Cookie Preferences






To quantify productivity improvement from replacing the Hugging Face summarization model with the new LLM-based chatbot, here's the proposed approach:

1. User Experience Comparison
We’re reaching out to users to compare:

Response speed and accuracy before (summarization only) vs. now (interactive LLM chatbot).

How the LLM chatbot helps resolve end-customer queries faster and more completely.

2. Estimated Productivity Gains
Based on early indicators:

Previous model: Provided passive summaries, required human interpretation.

Current LLM chatbot: Enables direct question-answering and multi-turn conversations, reducing manual effort.

3. Quantifiable Impact (Illustrative)
Let’s assume:

Time saved per interaction: Increased from 4 mins (summarization) to 10 mins (LLM chatbot)

Monthly interactions: 2,000

FTE cost/hour: $30

Productivity Gain:
LLM chatbot: (10/60) × 2,000 × $30 = $10,000/month
Hugging Face: (4/60) × 2,000 × $30 = $4,000/month
Net gain: ~$6,000/month

We’ll validate these assumptions with real user input and share a finalized benefit report shortly.









Regarding the productivity improvement through our chatbot (built using Hugging Face summarization models), here's the approach to quantify the benefit:

User Feedback Gathering
I’ll speak with end-users to understand:

How frequently they use the chatbot.

Whether it helps them respond faster and more accurately to customer queries.

Specific tasks where the summarization feature reduces manual effort.

Productivity Quantification
Based on initial assumptions (to be validated with users):

Time saved per interaction: ~5–10 minutes

Monthly usage: e.g., 1,500 summarization requests

FTE cost/hour: $30

Sample benefit calculation:
(8 min saved / 60) × 1,500 × $30 = $6,000/month

This is a conservative estimate. I’ll validate the figures with users and refine the benefit calculation accordingly.

Best regards,
[Your Name]

Would you like help preparing a quick user feedback form or survey to gather this data faster?






Do you like this personality?














ChatGPT can make mistakes. Check important info. See Cookie Preference





# Objective:
You are an AI assistant that extracts data from document images and returns it in structured markdown format, preserving the layout and formatting exactly as shown in the image.

# Instructions:

1. You are provided with images of PDF pages. Go through **each image carefully** and extract **all visible information**.

2. Format the extracted information in markdown style, **replicating the visual layout of the PDF** (text structure, tables, images, bullet points).

3. For **images, tables, and screenshots**, preserve:
   - Borders
   - Cell alignment
   - Font styles (bold, italics if applicable)
   - Highlighted or bold text
   - If present, add an inline description:
     - Use `image_description:` for images
     - Use `table_description:` for tables
     - Use `screenshot_description:` for screenshots

4. Store all extracted images in a folder named `images/` in `.png` format, and insert them in markdown as:

5. **Do not include any footers, headers, or page numbers** from the PDF.

6. **Extract the text exactly** as it appears in the image, including punctuation, symbols, and formatting.

7. If the document contains **errors, warnings, or stack traces**, skip them unless explicitly asked to extract them.

8. **Important Markdown Formatting Rules**:
- Never use `#`, `##`, `###`, or `**bold**` for headings unless the line is a clear section title (e.g., appears top-centered with larger font or boxed).
- If the content is a **list of similar items** (e.g., report names, fund types, field labels), always format them as bullet points using `-`, even if they are bold or appear like headings.
- Example:
  ```
  - Base Equivalent Cash Statement
  - Earned Income by Asset ID
  ```

9. If two or more items appear one after another (e.g., 17 report names), even across pages, **treat them as a continuous bullet list**.

10. Do not return the markdown content inside code blocks (no triple backticks).

# Output:
A structured markdown representation of all PDF content with:
- Correct visual layout
- Tables, images, and screenshots with descriptions
- Lists as bullet points
- No extra formatting like headings unless visually justified





Download 17 MYSS excel reports and run MYSS recon macro for external and internal funds.

Use the same method to create template and download 17 MYSS excel reports as below.

Holdings Alpha

Net Asset Value

Dividend Payable

Dividend Receivable

FX Pending Forward Activity

FX Pending Spot Activity

Interest Payable

Interest Receivable

MBS Interest Payable

MBS Interest Receivable

Open Trades

Tax Expense Payable

Tax Reclaim Receivable

Dividend Income Summary

Realized Gain Loss

Processing Page 8

c:\venv\Lib\site-packages\ssrai\utils\request utils.py:15: PydanticDeprecatedSince20: The `dict' method is depre

session.proxies = proxy.dict()

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is b

warnings.warn(

Response: **Base Equivalent Cash Statement**

**Earned Income by Asset ID**







import os
import base64
import pdfplumber
from PIL import Image
from io import BytesIO
from langchain.schema import HumanMessage
from ssrai import SSRAIClient

def extract_text_from_pdf(pdf_path, llm, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    extract_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"\n--- Processing Page {i + 1} ---\n")

            # Convert page to image
            page_image = page.to_image(resolution=300)
            image_path = os.path.join(output_folder, f"page_{i+1}.png")
            page_image.save(image_path, format="PNG")

            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": """You are an AI assistant that extracts data from documents and returns them as structured markdown.
                        
- Go through the image of the PDF page.
- Extract details exactly as seen.
- Preserve tables, styles, bold text, and structure.
- Add short inline descriptions starting with `image_description:`, `table_description:`, etc.
- Return in markdown format.
- Do not use code blocks."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            )

            response = llm.invoke([message])
            print("Response:", response.content)
            extract_list.append(response.content)

    return extract_list





import pandas as pd
import ast
from datasets import Dataset
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy
from ragas import evaluate

# === Preprocessing: Extract pageContentExtract ===
def extract_pageContentExtract(meta_val):
    try:
        if pd.isna(meta_val):
            return ""
        if isinstance(meta_val, str):
            parsed = ast.literal_eval(meta_val)
        elif isinstance(meta_val, list):
            parsed = meta_val
        else:
            return ""

        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
            return parsed[0].get("pageContent Extract", "")
        return ""
    except Exception as e:
        print(f"Failed to parse: {meta_val}\nError: {e}")
        return ""

# === Prepare Dataset for RAGAS ===
def prepare_dataset(df):
    records = {
        "question": df["user_query"].astype(str),
        "answer": df["bot_response"].astype(str),
        "contexts": df["context"].astype(str).apply(lambda x: [x]),  # list of one context string
    }
    return Dataset.from_dict(records)

# === Main RAGAS Evaluation Pipeline ===
def run_ragas_evaluation(input_excel_path, output_excel_path):
    # Load Excel
    df = pd.read_excel(input_excel_path)

    # Extract context using your custom function
    df["context"] = df["source_context"].apply(extract_pageContentExtract)

    # Fill NA to avoid crashes
    df.fillna("", inplace=True)

    # Prepare for RAGAS
    ragas_dataset = prepare_dataset(df)

    # Run evaluation
    results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_correctness, answer_relevancy]
    )

    # Add scores (converted to percentages)
    df["Faithfulness (%)"] = [round(score * 100, 2) for score in results["faithfulness"]]
    df["Answer Correctness (%)"] = [round(score * 100, 2) for score in results["answer_correctness"]]
    df["Answer Relevancy (%)"] = [round(score * 100, 2) for score in results["answer_relevancy"]]

    # Save output
    df.to_excel(output_excel_path, index=False)
    print(f"✅ Evaluation complete! Output saved to: {output_excel_path}")

# === Execute Script ===
if __name__ == "__main__":
    input_excel = r"\\mfgdcu02\p872643\Desktop\ssgenai_evalution\Audit assistance questions.xlsx"
    output_excel = "evaluation_output_ragas.xlsx"
    run_ragas_evaluation(input_excel, output_excel)

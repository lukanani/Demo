To quantify productivity improvement from replacing the Hugging Face summarization model with the new LLM-based chatbot, here's the proposed approach:

1. User Experience Comparison
We’re reaching out to users to compare:

Response speed and accuracy before (summarization only) vs. now (interactive LLM chatbot).

How the LLM chatbot helps resolve end-customer queries faster and more completely.

2. Estimated Productivity Gains
Based on early indicators:

Previous model: Provided passive summaries, required human interpretation.

Current LLM chatbot: Enables direct question-answering and multi-turn conversations, reducing manual effort.

3. Quantifiable Impact (Illustrative)
Let’s assume:

Time saved per interaction: Increased from 4 mins (summarization) to 10 mins (LLM chatbot)

Monthly interactions: 2,000

FTE cost/hour: $30

Productivity Gain:
LLM chatbot: (10/60) × 2,000 × $30 = $10,000/month
Hugging Face: (4/60) × 2,000 × $30 = $4,000/month
Net gain: ~$6,000/month

We’ll validate these assumptions with real user input and share a finalized benefit report shortly.









Regarding the productivity improvement through our chatbot (built using Hugging Face summarization models), here's the approach to quantify the benefit:

User Feedback Gathering
I’ll speak with end-users to understand:

How frequently they use the chatbot.

Whether it helps them respond faster and more accurately to customer queries.

Specific tasks where the summarization feature reduces manual effort.

Productivity Quantification
Based on initial assumptions (to be validated with users):

Time saved per interaction: ~5–10 minutes

Monthly usage: e.g., 1,500 summarization requests

FTE cost/hour: $30

Sample benefit calculation:
(8 min saved / 60) × 1,500 × $30 = $6,000/month

This is a conservative estimate. I’ll validate the figures with users and refine the benefit calculation accordingly.

Best regards,
[Your Name]

Would you like help preparing a quick user feedback form or survey to gather this data faster?






Do you like this personality?














ChatGPT can make mistakes. Check important info. See Cookie Preference





# Objective:
You are an AI assistant that extracts data from document images and returns it in structured markdown format, preserving the layout and formatting exactly as shown in the image.

# Instructions:

1. You are provided with images of PDF pages. Go through **each image carefully** and extract **all visible information**.

2. Format the extracted information in markdown style, **replicating the visual layout of the PDF** (text structure, tables, images, bullet points).

3. For **images, tables, and screenshots**, preserve:
   - Borders
   - Cell alignment
   - Font styles (bold, italics if applicable)
   - Highlighted or bold text
   - If present, add an inline description:
     - Use `image_description:` for images
     - Use `table_description:` for tables
     - Use `screenshot_description:` for screenshots

4. Store all extracted images in a folder named `images/` in `.png` format, and insert them in markdown as:

5. **Do not include any footers, headers, or page numbers** from the PDF.

6. **Extract the text exactly** as it appears in the image, including punctuation, symbols, and formatting.

7. If the document contains **errors, warnings, or stack traces**, skip them unless explicitly asked to extract them.

8. **Important Markdown Formatting Rules**:
- Never use `#`, `##`, `###`, or `**bold**` for headings unless the line is a clear section title (e.g., appears top-centered with larger font or boxed).
- If the content is a **list of similar items** (e.g., report names, fund types, field labels), always format them as bullet points using `-`, even if they are bold or appear like headings.
- Example:
  ```
  - Base Equivalent Cash Statement
  - Earned Income by Asset ID
  ```

9. If two or more items appear one after another (e.g., 17 report names), even across pages, **treat them as a continuous bullet list**.

10. Do not return the markdown content inside code blocks (no triple backticks).

# Output:
A structured markdown representation of all PDF content with:
- Correct visual layout
- Tables, images, and screenshots with descriptions
- Lists as bullet points
- No extra formatting like headings unless visually justified





Download 17 MYSS excel reports and run MYSS recon macro for external and internal funds.

Use the same method to create template and download 17 MYSS excel reports as below.

Holdings Alpha

Net Asset Value

Dividend Payable

Dividend Receivable

FX Pending Forward Activity

FX Pending Spot Activity

Interest Payable

Interest Receivable

MBS Interest Payable

MBS Interest Receivable

Open Trades

Tax Expense Payable

Tax Reclaim Receivable

Dividend Income Summary

Realized Gain Loss

Processing Page 8

c:\venv\Lib\site-packages\ssrai\utils\request utils.py:15: PydanticDeprecatedSince20: The `dict' method is depre

session.proxies = proxy.dict()

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is b

warnings.warn(

Response: **Base Equivalent Cash Statement**

**Earned Income by Asset ID**







import os
import base64
import pdfplumber
from PIL import Image
from io import BytesIO
from langchain.schema import HumanMessage
from ssrai import SSRAIClient

def extract_text_from_pdf(pdf_path, llm, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    extract_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"\n--- Processing Page {i + 1} ---\n")

            # Convert page to image
            page_image = page.to_image(resolution=300)
            image_path = os.path.join(output_folder, f"page_{i+1}.png")
            page_image.save(image_path, format="PNG")

            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": """You are an AI assistant that extracts data from documents and returns them as structured markdown.
                        
- Go through the image of the PDF page.
- Extract details exactly as seen.
- Preserve tables, styles, bold text, and structure.
- Add short inline descriptions starting with `image_description:`, `table_description:`, etc.
- Return in markdown format.
- Do not use code blocks."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            )

            response = llm.invoke([message])
            print("Response:", response.content)
            extract_list.append(response.content)

    return extract_list





import pandas as pd
import ast
from datasets import Dataset
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy
from ragas import evaluate

# === Preprocessing: Extract pageContentExtract ===
def extract_pageContentExtract(meta_val):
    try:
        if pd.isna(meta_val):
            return ""
        if isinstance(meta_val, str):
            parsed = ast.literal_eval(meta_val)
        elif isinstance(meta_val, list):
            parsed = meta_val
        else:
            return ""

        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
            return parsed[0].get("pageContent Extract", "")
        return ""
    except Exception as e:
        print(f"Failed to parse: {meta_val}\nError: {e}")
        return ""

# === Prepare Dataset for RAGAS ===
def prepare_dataset(df):
    records = {
        "question": df["user_query"].astype(str),
        "answer": df["bot_response"].astype(str),
        "contexts": df["context"].astype(str).apply(lambda x: [x]),  # list of one context string
    }
    return Dataset.from_dict(records)

# === Main RAGAS Evaluation Pipeline ===
def run_ragas_evaluation(input_excel_path, output_excel_path):
    # Load Excel
    df = pd.read_excel(input_excel_path)

    # Extract context using your custom function
    df["context"] = df["source_context"].apply(extract_pageContentExtract)

    # Fill NA to avoid crashes
    df.fillna("", inplace=True)

    # Prepare for RAGAS
    ragas_dataset = prepare_dataset(df)

    # Run evaluation
    results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_correctness, answer_relevancy]
    )

    # Add scores (converted to percentages)
    df["Faithfulness (%)"] = [round(score * 100, 2) for score in results["faithfulness"]]
    df["Answer Correctness (%)"] = [round(score * 100, 2) for score in results["answer_correctness"]]
    df["Answer Relevancy (%)"] = [round(score * 100, 2) for score in results["answer_relevancy"]]

    # Save output
    df.to_excel(output_excel_path, index=False)
    print(f"✅ Evaluation complete! Output saved to: {output_excel_path}")

# === Execute Script ===
if __name__ == "__main__":
    input_excel = r"\\mfgdcu02\p872643\Desktop\ssgenai_evalution\Audit assistance questions.xlsx"
    output_excel = "evaluation_output_ragas.xlsx"
    run_ragas_evaluation(input_excel, output_excel)

LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)
Environment Setup:
Import all required libraries (transformers, datasets, torch, mlflow, etc.).

Tokenizer & Model Loading:
Load the tokenizer and quantized LLaMA model. Set pad token as EOS and padding side as "left" for causal LM.

Dataset Loading:
Read the question-answer dataset from a CSV file. Drop nulls, shuffle, and retain relevant columns.

Prompt Formatting:
Format data into structured "chat" format (system, user, assistant roles) using the tokenizer’s chat template.

Token Count Analysis:
Calculate and visualize token lengths to ensure most samples are under 1024 tokens (model limit).

Train/Validation Split:
Use train_test_split() to divide data into training and validation sets for evaluation.

Dataset Conversion:
Convert Pandas DataFrames to HuggingFace DatasetDict for training compatibility.

PEFT (LoRA) Configuration:
Define LoraConfig to fine-tune only a small subset of layers efficiently using LoRA adapters.

Training Configuration:
Set hyperparameters like batch size, epochs, learning rate, warmup ratio, and enable MLflow tracking.

Model Training:
Fine-tune the model using SFTTrainer with chat-formatted inputs. Log metrics and artifacts via MLflow.

Merge Adapters:
After training, merge LoRA adapters with the base model to create a fully functional fine-tuned model.

Model Logging:
Log the merged model to MLflow using transformers.log_model() for future deployment.

Model Registration:
Register the model version in Databricks MLflow Registry with a custom name.

Artifact Copy (Optional):
Download and copy model artifacts from MLflow run to a Unified Catalog directory.

Inference:
Load the logged model and run inference using mlflow_model.predict() with system-user prompts.

Let me know if you want this exported to a .md or .pdf file.
















ChatGPT can make mistakes. Check important info. See Cookie Preferences.








Hi Chandreshkumar Dedani, Vivek. The results from the previous run were good. I have now changed the dataset to include 3 diverse procedural questions. Please use this dataset for your training, and also use the parameters: context length = 1024 and epochs = 4."






import pandas as pd

# Load your Excel or CSV file
df = pd.read_excel('your_dataset.xlsx')  # or use read_csv if it's a CSV

# Group by the 'Original Question' column and select the first 3 entries per group
filtered_df = df.groupby('Original Question', group_keys=False).head(3)

# Save the result if needed
filtered_df.to_excel('filtered_output.xlsx', index=False)

print("Filtered dataset with first 3 questions per original question.")





import json
import pandas as pd

# Step 1: Load JSON file
with open('your_file.json', 'r') as file:
    data = json.load(file)

# Step 2: Normalize (flatten) the JSON data
df = pd.json_normalize(data, sep='_')

# Step 3: Save to Excel
df.to_excel('converted_output.xlsx', index=False)

print("Conversion complete. File saved as 'converted_output.xlsx'")






import os
import time
import json
import dotenv
import pandas as pd
from langchain.chat_models import AzureChatOpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables
dotenv.load_dotenv()

# Azure LLM Configuration
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

llm = AzureChatOpenAI(
    temperature=0,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name="ssgpt-4"
)

# ========================== Step 1: Convert Excel to JSON ==========================

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")
    json_data = [{"question": row["Question"], "answer": row["Answer"]} for _, row in df.iterrows()]
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=4, ensure_ascii=False)
    print(f"Converted Excel to JSON at {output_json_path}")
    return output_json_path

# ========================== Step 2: Read JSON ==========================

def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"Read {len(data)} records from {file_path}")
        return data
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

# ========================== Step 3: Build Prompt ==========================

def build_rating_prompt(question, answer):
    return f"""
Rate each question-answer pair on a scale from 1-10, based on:
- Accuracy (0-3): factual correctness
- Relevance (0-2): relevance to content
- Clarity (0-2): clear language
- Usefulness (0-3): value for model learning

YOU MUST RETURN A VALID JSON OBJECT WITH THIS EXACT SCHEMA:
{{
  "question": "{question}",
  "answer": "{answer}",
  "rating": <1-10>
}}

*** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE ***
"""

# ========================== Step 4: Validate with LLM ==========================

def validate_with_llm(entry, index, retries=3):
    prompt = build_rating_prompt(entry["question"], entry["answer"])
    for attempt in range(retries):
        try:
            response = llm.predict(prompt)
            result = json.loads(response)
            return index, result
        except Exception as e:
            print(f"Retry {attempt+1} failed: {e}")
            time.sleep(2)
    return index, {
        "question": entry["question"],
        "answer": entry["answer"],
        "rating": 0
    }

# ========================== Step 5: Run Pipeline ==========================

def full_pipeline(input_excel, final_excel_output, final_json_output, max_workers=5):
    temp_json_path = convert_excel_to_json(input_excel)
    qa_data = read_json_file(temp_json_path)
    results = [None] * len(qa_data)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(validate_with_llm, entry, i) for i, entry in enumerate(qa_data)]
        for future in as_completed(futures):
            index, result = future.result()
            results[index] = result

    # Save to Excel and JSON
    pd.DataFrame(results).to_excel(final_excel_output, index=False)
    with open(final_json_output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"Pipeline complete. Results saved to {final_excel_output} and {final_json_output}")









Following up on the productivity improvement, here’s a summary of the value delivered by migrating from the Hugging Face summarization model to the new LLM-based chatbot:

1. Key Difference Observed
Previous (Hugging Face Model): Users faced delays due to slower response times and limited interactivity (only summary generation).

Current (LLM Chatbot): Delivers instant, context-aware answers through natural conversation, significantly reducing turnaround time.

2. Quantified Benefit (Illustrative Example)
Average handling time (per query):

Hugging Face: ~12 mins (includes manual reading + follow-ups)

LLM Chatbot: ~4 mins (direct answers with follow-up support)

Monthly queries handled: 2,000

FTE rate: $30/hour

Productivity Gain = (12 – 4)/60 × 2,000 × $30 = $8,000/month

3. Next Steps
I will engage with actual users to validate the above assumptions and share a data-backed benefit report.

Let me know if you'd like this structured as a slide or summary deck as well.

Best regards,
[Your Name]

Would you like me to help you turn this into a one-slide summary for stakeholders?
















ChatGPT can make mistakes. Check important info. See Cookie Preferences






To quantify productivity improvement from replacing the Hugging Face summarization model with the new LLM-based chatbot, here's the proposed approach:

1. User Experience Comparison
We’re reaching out to users to compare:

Response speed and accuracy before (summarization only) vs. now (interactive LLM chatbot).

How the LLM chatbot helps resolve end-customer queries faster and more completely.

2. Estimated Productivity Gains
Based on early indicators:

Previous model: Provided passive summaries, required human interpretation.

Current LLM chatbot: Enables direct question-answering and multi-turn conversations, reducing manual effort.

3. Quantifiable Impact (Illustrative)
Let’s assume:

Time saved per interaction: Increased from 4 mins (summarization) to 10 mins (LLM chatbot)

Monthly interactions: 2,000

FTE cost/hour: $30

Productivity Gain:
LLM chatbot: (10/60) × 2,000 × $30 = $10,000/month
Hugging Face: (4/60) × 2,000 × $30 = $4,000/month
Net gain: ~$6,000/month

We’ll validate these assumptions with real user input and share a finalized benefit report shortly.









Regarding the productivity improvement through our chatbot (built using Hugging Face summarization models), here's the approach to quantify the benefit:

User Feedback Gathering
I’ll speak with end-users to understand:

How frequently they use the chatbot.

Whether it helps them respond faster and more accurately to customer queries.

Specific tasks where the summarization feature reduces manual effort.

Productivity Quantification
Based on initial assumptions (to be validated with users):

Time saved per interaction: ~5–10 minutes

Monthly usage: e.g., 1,500 summarization requests

FTE cost/hour: $30

Sample benefit calculation:
(8 min saved / 60) × 1,500 × $30 = $6,000/month

This is a conservative estimate. I’ll validate the figures with users and refine the benefit calculation accordingly.

Best regards,
[Your Name]

Would you like help preparing a quick user feedback form or survey to gather this data faster?






Do you like this personality?














ChatGPT can make mistakes. Check important info. See Cookie Preference





# Objective:
You are an AI assistant that extracts data from document images and returns it in structured markdown format, preserving the layout and formatting exactly as shown in the image.

# Instructions:

1. You are provided with images of PDF pages. Go through **each image carefully** and extract **all visible information**.

2. Format the extracted information in markdown style, **replicating the visual layout of the PDF** (text structure, tables, images, bullet points).

3. For **images, tables, and screenshots**, preserve:
   - Borders
   - Cell alignment
   - Font styles (bold, italics if applicable)
   - Highlighted or bold text
   - If present, add an inline description:
     - Use `image_description:` for images
     - Use `table_description:` for tables
     - Use `screenshot_description:` for screenshots

4. Store all extracted images in a folder named `images/` in `.png` format, and insert them in markdown as:

5. **Do not include any footers, headers, or page numbers** from the PDF.

6. **Extract the text exactly** as it appears in the image, including punctuation, symbols, and formatting.

7. If the document contains **errors, warnings, or stack traces**, skip them unless explicitly asked to extract them.

8. **Important Markdown Formatting Rules**:
- Never use `#`, `##`, `###`, or `**bold**` for headings unless the line is a clear section title (e.g., appears top-centered with larger font or boxed).
- If the content is a **list of similar items** (e.g., report names, fund types, field labels), always format them as bullet points using `-`, even if they are bold or appear like headings.
- Example:
  ```
  - Base Equivalent Cash Statement
  - Earned Income by Asset ID
  ```

9. If two or more items appear one after another (e.g., 17 report names), even across pages, **treat them as a continuous bullet list**.

10. Do not return the markdown content inside code blocks (no triple backticks).

# Output:
A structured markdown representation of all PDF content with:
- Correct visual layout
- Tables, images, and screenshots with descriptions
- Lists as bullet points
- No extra formatting like headings unless visually justified





Download 17 MYSS excel reports and run MYSS recon macro for external and internal funds.

Use the same method to create template and download 17 MYSS excel reports as below.

Holdings Alpha

Net Asset Value

Dividend Payable

Dividend Receivable

FX Pending Forward Activity

FX Pending Spot Activity

Interest Payable

Interest Receivable

MBS Interest Payable

MBS Interest Receivable

Open Trades

Tax Expense Payable

Tax Reclaim Receivable

Dividend Income Summary

Realized Gain Loss

Processing Page 8

c:\venv\Lib\site-packages\ssrai\utils\request utils.py:15: PydanticDeprecatedSince20: The `dict' method is depre

session.proxies = proxy.dict()

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is b

warnings.warn(

Response: **Base Equivalent Cash Statement**

**Earned Income by Asset ID**







import os
import base64
import pdfplumber
from PIL import Image
from io import BytesIO
from langchain.schema import HumanMessage
from ssrai import SSRAIClient

def extract_text_from_pdf(pdf_path, llm, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    extract_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"\n--- Processing Page {i + 1} ---\n")

            # Convert page to image
            page_image = page.to_image(resolution=300)
            image_path = os.path.join(output_folder, f"page_{i+1}.png")
            page_image.save(image_path, format="PNG")

            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": """You are an AI assistant that extracts data from documents and returns them as structured markdown.
                        
- Go through the image of the PDF page.
- Extract details exactly as seen.
- Preserve tables, styles, bold text, and structure.
- Add short inline descriptions starting with `image_description:`, `table_description:`, etc.
- Return in markdown format.
- Do not use code blocks."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            )

            response = llm.invoke([message])
            print("Response:", response.content)
            extract_list.append(response.content)

    return extract_list





import pandas as pd
import ast
from datasets import Dataset
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy
from ragas import evaluate

# === Preprocessing: Extract pageContentExtract ===
def extract_pageContentExtract(meta_val):
    try:
        if pd.isna(meta_val):
            return ""
        if isinstance(meta_val, str):
            parsed = ast.literal_eval(meta_val)
        elif isinstance(meta_val, list):
            parsed = meta_val
        else:
            return ""

        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
            return parsed[0].get("pageContent Extract", "")
        return ""
    except Exception as e:
        print(f"Failed to parse: {meta_val}\nError: {e}")
        return ""

# === Prepare Dataset for RAGAS ===
def prepare_dataset(df):
    records = {
        "question": df["user_query"].astype(str),
        "answer": df["bot_response"].astype(str),
        "contexts": df["context"].astype(str).apply(lambda x: [x]),  # list of one context string
    }
    return Dataset.from_dict(records)

# === Main RAGAS Evaluation Pipeline ===
def run_ragas_evaluation(input_excel_path, output_excel_path):
    # Load Excel
    df = pd.read_excel(input_excel_path)

    # Extract context using your custom function
    df["context"] = df["source_context"].apply(extract_pageContentExtract)

    # Fill NA to avoid crashes
    df.fillna("", inplace=True)

    # Prepare for RAGAS
    ragas_dataset = prepare_dataset(df)

    # Run evaluation
    results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_correctness, answer_relevancy]
    )

    # Add scores (converted to percentages)
    df["Faithfulness (%)"] = [round(score * 100, 2) for score in results["faithfulness"]]
    df["Answer Correctness (%)"] = [round(score * 100, 2) for score in results["answer_correctness"]]
    df["Answer Relevancy (%)"] = [round(score * 100, 2) for score in results["answer_relevancy"]]

    # Save output
    df.to_excel(output_excel_path, index=False)
    print(f"✅ Evaluation complete! Output saved to: {output_excel_path}")

# === Execute Script ===
if __name__ == "__main__":
    input_excel = r"\\mfgdcu02\p872643\Desktop\ssgenai_evalution\Audit assistance questions.xlsx"
    output_excel = "evaluation_output_ragas.xlsx"
    run_ragas_evaluation(input_excel, output_excel)

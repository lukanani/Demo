pip install --proxy=http://proxy.statestr.com:80 \
  --trusted-host pypi.org \
  --trusted-host pypi.python.org \
  --trusted-host files.pythonhosted.org \
  --no-build-isolation \
  --upgrade \
  transformers==4.39.3 \
  peft==0.10.0 \
  trl==0.8.6 \
  torch==2.3.1+cu121 \
  bitsandbytes==0.43.1 \
  datasets==2.19.1 \
  accelerate==0.30.1 \
  sentencepiece==0.2.0 \
  protobuf==4.25.3











import os
import json
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient

# --- Setup LLM ---
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)
chat_model_name = "ssgpt-40"
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

# --- Prompt Template ---
DIVERSE_BY_TYPE_PROMPT = """
You are a question rephrasing assistant. Your task is to generate diverse versions of the input question-answer pair, but only of the **"{question_type}"** type.

### Input:
Original Question: {original_question}
Original Answer: {original_answer}

### Instructions:
- Generate exactly {num_questions} diverse **{question_type}** questions.
- Ensure the questions are not subsets or minor rewrites.
- Keep the meaning same as the answer.
- If it's Procedural, keep step-by-step formatting.

### Output Format:
{{
  "id": "{id}",
  "question_type": "{question_type}",
  "original_question": "{original_question}",
  "original_answer": "{original_answer}",
  "diverse_questions": [
    {{
      "diverse_question": "...",
      "diverse_answer": "..."
    }},
    ...
  ]
}}

Return only JSON. No markdown, no explanation.
"""

# --- Excel to JSON ---
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "id": str(i + 1),
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for i, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""

# --- JSON Utilities ---
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

def clear_output(path="responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)

def append_to_output(batch_responses, path="responses.json"):
    try:
        existing_data = []
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")

# --- LLM Call per Question Type ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions_per_type(qa, num_questions=3):
    try:
        prompt = PromptTemplate(
            input_variables=["id", "question_type", "original_question", "original_answer", "num_questions"],
            template=DIVERSE_BY_TYPE_PROMPT
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.predict(
            id=qa["id"],
            question_type=qa["Question_type"],
            original_question=qa["Question"],
            original_answer=qa["Answer"],
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except Exception as e:
        print("Parsing error:", e)
        return {}

# --- Batch Processing by Dynamic Type Count ---
def process_per_question(data, type_question_count: dict, output_path="responses.json"):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for qa in data:
            qtype = qa["Question_type"]
            num_q = type_question_count.get(qtype, 0)  # default 0 if not present
            if num_q > 0:
                futures.append(executor.submit(generate_diverse_questions_per_type, qa, num_q))
        for future in futures:
            try:
                result = future.result()
                if result:
                    append_to_output([result], output_path)
            except Exception as e:
                print("Error:", e)

# --- Save to Excel ---
def save_responses_to_excel(output_json_path, excel_file_path="output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for item in data:
        for dq in item["diverse_questions"]:
            excel_data.append({
                "ID": item["id"],
                "Question Type": item["question_type"],
                "Original Question": item["original_question"],
                "Original Answer": item["original_answer"],
                "Diverse Question": dq["diverse_question"],
                "Diverse Answer": dq["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel: {excel_file_path}")

# --- Main Runner ---
if __name__ == "__main__":
    file_path = r"your_input_file.xlsx"
    output_json = r"converted_data.json"
    final_json_output = r"responses_by_type.json"
    excel_output = r"diverse_questions_output.xlsx"

    # Define how many questions per type
    type_question_count = {
        "Factual": 1,
        "Procedural": 3,
        "Conceptual": 1,
        "Reasoning-Based": 3
    }

    json_path = convert_excel_to_json(file_path, output_json)
    data = read_json_file(json_path)

    if data:
        process_per_question(data, type_question_count, output_path=final_json_output)
        save_responses_to_excel(final_json_output, excel_output)







import os
import json
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient

# --- Setup LLM ---
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)
chat_model_name = "ssgpt-40"
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

# --- Prompt Template ---
DIVERSE_BY_TYPE_PROMPT = """
You are a question rephrasing assistant. Your task is to generate diverse versions of the input question-answer pair, but only of the **"{question_type}"** type.

### Input:
Original Question: {original_question}
Original Answer: {original_answer}

### Instructions:
- Generate exactly {num_questions} diverse **{question_type}** questions.
- Ensure the questions are not subsets or minor rewrites.
- Keep the meaning same as the answer.
- If it's Procedural, keep step-by-step formatting.

### Output Format:
{{
  "id": "{id}",
  "question_type": "{question_type}",
  "original_question": "{original_question}",
  "original_answer": "{original_answer}",
  "diverse_questions": [
    {{
      "diverse_question": "...",
      "diverse_answer": "..."
    }},
    ...
  ]
}}

Return only JSON. No markdown, no explanation.
"""

# --- Excel to JSON ---
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "id": str(i + 1),
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for i, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""

# --- JSON Utilities ---
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

def clear_output(path="responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)

def append_to_output(batch_responses, path="responses.json"):
    try:
        existing_data = []
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")

# --- LLM Call per Question Type ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions_per_type(qa, num_questions=3):
    try:
        prompt = PromptTemplate(
            input_variables=["id", "question_type", "original_question", "original_answer", "num_questions"],
            template=DIVERSE_BY_TYPE_PROMPT
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.predict(
            id=qa["id"],
            question_type=qa["Question_type"],
            original_question=qa["Question"],
            original_answer=qa["Answer"],
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except Exception as e:
        print("Parsing error:", e)
        return {}

# --- Batch Processing ---
def process_per_question(data, num_questions=3, output_path="responses.json"):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(generate_diverse_questions_per_type, qa, num_questions) for qa in data]
        for future in futures:
            try:
                result = future.result()
                if result:
                    append_to_output([result], output_path)
            except Exception as e:
                print("Error:", e)

# --- Save to Excel ---
def save_responses_to_excel(output_json_path, excel_file_path="output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for item in data:
        for dq in item["diverse_questions"]:
            excel_data.append({
                "ID": item["id"],
                "Question Type": item["question_type"],
                "Original Question": item["original_question"],
                "Original Answer": item["original_answer"],
                "Diverse Question": dq["diverse_question"],
                "Diverse Answer": dq["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel: {excel_file_path}")

# --- Main Runner ---
if __name__ == "__main__":
    file_path = r"your_input_file.xlsx"
    output_json = r"converted_data.json"
    final_json_output = r"responses_by_type.json"
    excel_output = r"diverse_questions_output.xlsx"

    json_path = convert_excel_to_json(file_path, output_json)
    data = read_json_file(json_path)

    if data:
        process_per_question(data, num_questions=3, output_path=final_json_output)
        save_responses_to_excel(final_json_output, excel_output)






DIVERSE_QUESTION_PROMPT = """
You are tasked with generating multiple types of diverse questions and answers for each input question-answer pair.

The input will contain a list of original questions and answers.

For each input pair, generate **separate JSON objects for each question type**, based on the required number below.

## Required distribution:
{type_count_description}

### Response Format:
Return a **list of JSON objects**, one for each question type per input pair. Each object must be in this exact format:

{{
  "id": "<unique_id>",
  "question_type": "<Factual | Procedural | Conceptual | Inferential | Reasoning-Based>",
  "original_question": "<Original Question>",
  "original_answer": "<Original Answer>",
  "diverse_questions": [
    {{
      "diverse_question": "<Alternative question>",
      "diverse_answer": "<Answer>"
    }},
    ...
  ]
}}

### Additional Requirements:
- Do **not** combine different question types into one object.
- Use exactly the number of diverse questions specified per type.
- Procedural answers should be formatted as step-by-step.
- Return only valid JSON array. No markdown or explanations.
- Use double quotes in all JSON strings.
"""






import os
import json
import base64
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient


# -------- Configuration --------
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)

chat_model_name = "ssgpt-40"  # Replace with actual
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs from input question-answer data.

For each original pair, generate diverse questions and answers based on the following distribution:
{type_count_description}

### Question Types:
1. Factual - Direct and fact-based
2. Conceptual - Based on understanding of principles
3. Inferential - Based on logical conclusions
4. Reasoning-Based - Based on logical reasoning
5. Procedural - Step-by-step process

### Requirements:
- Output format:
{{
  "id": "<unique_id>",
  "question_type": "<question_type>",
  "original_question": "<Original Question>",
  "original_answer": "<Original Answer>",
  "diverse_questions": [
    {{
      "diverse_question": "<New Question>",
      "diverse_answer": "<Answer>"
    }},
    ...
  ]
}}
- Procedural answers must retain step-by-step clarity.
- Avoid repeating or subset questions.
"""

prompt = PromptTemplate(
    input_variables=["input_json", "type_count_description"],
    template=DIVERSE_QUESTION_PROMPT
)

llm_chain = LLMChain(llm=llm, prompt=prompt)


# -------- Excel to JSON --------
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for _, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""


# -------- JSON Utils --------
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []


def split_into_batches(input_list, batch_size):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]


# -------- Prompt Helper --------
def get_type_count_description(type_counts: dict) -> str:
    return "\n".join([f"- {qtype}: {count}" for qtype, count in type_counts.items()])


# -------- Question Generation --------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions(input_json: str, type_counts: dict) -> list:
    try:
        type_desc = get_type_count_description(type_counts)
        response = llm_chain.predict(input_json=input_json, type_count_description=type_desc)
        return json.loads(response)
    except Exception as e:
        print(f"Parsing error: {e}")
        return []


# -------- Output Handling --------
def clear_output(path="incremental_responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)


def append_to_output(batch_responses, path="incremental_responses.json"):
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")


def save_responses_to_excel(output_json_path, excel_file_path="diverse_output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for response in data:
        for item in response["diverse_questions"]:
            excel_data.append({
                "ID": response["id"],
                "Question Type": response["question_type"],
                "Original Question": response["original_question"],
                "Original Answer": response["original_answer"],
                "Diverse Question": item["diverse_question"],
                "Diverse Answer": item["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.sort_values(by=["Question Type", "ID"], inplace=True)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel at {excel_file_path}")


# -------- Batch Processing --------
def process_batches(data, batch_size, type_counts, output_path="incremental_responses.json", max_workers=3):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for i, batch in enumerate(split_into_batches(data, batch_size), 1):
            print(f"Processing Batch {i} with {len(batch)} QA pairs")
            batch_input = json.dumps(batch)
            futures.append(executor.submit(generate_diverse_questions, batch_input, type_counts))
        for future in futures:
            try:
                result = future.result()
                append_to_output(result, output_path)
            except Exception as e:
                print(f"Batch failed: {e}")


# -------- Main Flow --------
if __name__ == "__main__":
    file_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_1.xlsx"
    output_json_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_1.json"
    incremental_output_path = r"ds_modules/incremental_responses_part.json"
    output_excel_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_out.xlsx"

    type_counts = {
        "Factual": 1,
        "Procedural": 3,
        "Conceptual": 2
    }

    json_path = convert_excel_to_json(file_path, output_json_path)
    data = read_json_file(json_path)
    if data:
        process_batches(data, batch_size=2, type_counts=type_counts, output_path=incremental_output_path)
        save_responses_to_excel(incremental_output_path, output_excel_path)







I have created a document related to the SOP, in which I prepared 50 questions divided evenly across different types:

Factual: 10

Procedural: 10

Inferential: 10

Conceptual: 10

Reasoning-based: 10

I used an automated retriever strategy pipeline to generate responses for these questions. Overall, the responses look satisfactory to me. However, I have observed a few key points:

When the input question is phrased correctly, the system returns accurate and relevant answers. If the question is not well-formed or deviates from the expected format, the system fails to respond effectively.

For factual questions, the accuracy is high—around 97%, and the answers are precise.

For procedural questions, the system also performs well, with an accuracy of approximately 95%, often providing exact steps and additional clarifications.

For inferential, conceptual, and reasoning-based questions, the responses are generally good. However, some answers tend to be generalized rather than specific, which slightly affects their usefulness.

Let me know if you'd like to present this as an email update or in a report format.






Is this conversation helpful so far?







Tools



ChatGPT can make mistakes. Chec




import fitz  # PyMuPDF
import pandas as pd
from tqdm import tqdm
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ---------- Configuration ----------
PDF_PATH = "your_financial_filing.pdf"
EXCEL_OUTPUT_PATH = "mcq_financial_questions.xlsx"

# Azure OpenAI settings
AZURE_DEPLOYMENT_NAME = "your-deployment-name"
AZURE_API_VERSION = "2023-05-15"
AZURE_ENDPOINT = "https://your-resource-name.openai.azure.com"
AZURE_API_KEY = "your-azure-openai-key"

# ---------- LLM Init ----------
llm = AzureChatOpenAI(
    deployment_name=AZURE_DEPLOYMENT_NAME,
    openai_api_version=AZURE_API_VERSION,
    openai_api_key=AZURE_API_KEY,
    openai_api_base=AZURE_ENDPOINT,
    temperature=0.7,
)

# ---------- Prompt Template ----------
prompt_template = """
You are a financial assistant. Based on the following passage from a financial filing, generate:

Multiple-choice Question (MCQ) that require reasoning.

4 answer options (A-D), only one of which is correct.

The correct answer letter.

A short rationale for why that answer is correct.

The question should require inference or trend understanding or complex chain of thought not direct lookup.

Passage:
\"\"\"
{paragraph}
\"\"\"

Output format:

Q: <Question text>

A:

A: <option A>

B: <option B>

C: <option C>

D: <option D>

Correct answer: <A/B/C/D>

Rationale: <why it's correct>
"""

template = PromptTemplate(input_variables=["paragraph"], template=prompt_template)
llm_chain = LLMChain(llm=llm, prompt=template)

# ---------- Extract PDF Text ----------
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    return [page.get_text() for page in doc]

# ---------- Parse MCQ Output ----------
def parse_response(response):
    lines = response.strip().split("\n")
    result = {"Question": "", "Option A": "", "Option B": "", "Option C": "", "Option D": "", "Correct Answer": "", "Rationale": ""}
    
    for line in lines:
        if line.startswith("Q:"):
            result["Question"] = line[2:].strip()
        elif line.startswith("A:") and result["Option A"] == "":
            result["Option A"] = line[2:].strip()
        elif line.startswith("B:"):
            result["Option B"] = line[2:].strip()
        elif line.startswith("C:"):
            result["Option C"] = line[2:].strip()
        elif line.startswith("D:"):
            result["Option D"] = line[2:].strip()
        elif line.startswith("Correct answer:"):
            result["Correct Answer"] = line.split(":")[1].strip()
        elif line.startswith("Rationale:"):
            result["Rationale"] = line.split(":", 1)[1].strip()
    return result

# ---------- Main Function ----------
def generate_mcqs_from_pdf(pdf_path):
    pages = extract_pdf_text(pdf_path)
    all_data = []

    for idx, page_text in enumerate(tqdm(pages, desc="Generating MCQs")):
        if len(page_text.strip()) < 100:
            continue
        try:
            response = llm_chain.predict(paragraph=page_text.strip())
            print(f"\nLLM Response (Page {idx+1}):\n{response}\n")
            parsed = parse_response(response)
            parsed["Page"] = idx + 1
            all_data.append(parsed)
        except Exception as e:
            print(f"Error on page {idx+1}: {e}")

    df = pd.DataFrame(all_data)
    df.to_excel(EXCEL_OUTPUT_PATH, index=False)
    print(f"✅ MCQs saved to: {EXCEL_OUTPUT_PATH}")

# ---------- Run ----------
if __name__ == "__main__":
    generate_mcqs_from_pdf(PDF_PATH)









import fitz  # PyMuPDF
import openai
import pandas as pd
from tqdm import tqdm

# ---------- Configuration ----------
PDF_PATH = "your_financial_filing.pdf"
EXCEL_OUTPUT_PATH = "mcq_financial_questions.xlsx"
OPENAI_API_KEY = "your-api-key"
MODEL = "gpt-4o"  # Or use Azure endpoint and model name
USE_AZURE = False  # Set to True if using Azure OpenAI

# ---------- Prompt Template ----------
PROMPT_TEMPLATE = """
You are a financial assistant. Based on the following passage from a financial filing, generate:

Multiple-choice Question (MCQ) that require reasoning.

4 answer options (A-D), only one of which is correct.

The correct answer letter.

A short rationale for why that answer is correct.

The question should require inference or trend understanding or complex chain of thought not direct lookup.

Passage:
\"\"\"
{page_text}
\"\"\"

Output format:

Q: <Question text>

A:

A: <option A>

B: <option B>

C: <option C>

D: <option D>

Correct answer: <A/B/C/D>

Rationale: <why it's correct>
"""

# ---------- Extract PDF Text ----------
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    return [page.get_text() for page in doc]

# ---------- LLM Call ----------
def call_llm(prompt):
    if USE_AZURE:
        openai.api_type = "azure"
        openai.api_base = "https://<your-azure-endpoint>.openai.azure.com/"
        openai.api_version = "2023-05-15"
        openai.api_key = OPENAI_API_KEY

        response = openai.ChatCompletion.create(
            engine="your-deployment-name",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
    else:
        openai.api_key = OPENAI_API_KEY
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )

    return response['choices'][0]['message']['content']

# ---------- Parse LLM Response ----------
def parse_response(response):
    lines = response.strip().split("\n")
    result = {"Question": "", "Option A": "", "Option B": "", "Option C": "", "Option D": "", "Correct Answer": "", "Rationale": ""}
    
    for line in lines:
        if line.startswith("Q:"):
            result["Question"] = line[2:].strip()
        elif line.startswith("A:") and result["Option A"] == "":
            result["Option A"] = line[2:].strip()
        elif line.startswith("B:"):
            result["Option B"] = line[2:].strip()
        elif line.startswith("C:"):
            result["Option C"] = line[2:].strip()
        elif line.startswith("D:"):
            result["Option D"] = line[2:].strip()
        elif line.startswith("Correct answer:"):
            result["Correct Answer"] = line.split(":")[1].strip()
        elif line.startswith("Rationale:"):
            result["Rationale"] = line.split(":", 1)[1].strip()
    return result

# ---------- Main Execution ----------
def generate_mcqs_from_pdf(pdf_path):
    pages = extract_pdf_text(pdf_path)
    all_data = []

    for idx, page_text in enumerate(tqdm(pages, desc="Processing Pages")):
        if len(page_text.strip()) < 100:
            continue  # Skip blank/empty pages
        prompt = PROMPT_TEMPLATE.format(page_text=page_text.strip())
        try:
            response = call_llm(prompt)
            parsed = parse_response(response)
            parsed["Page"] = idx + 1
            all_data.append(parsed)
        except Exception as e:
            print(f"Error on page {idx+1}: {e}")

    df = pd.DataFrame(all_data)
    df.to_excel(EXCEL_OUTPUT_PATH, index=False)
    print(f"✅ MCQs saved to: {EXCEL_OUTPUT_PATH}")

# ---------- Run ----------
if __name__ == "__main__":
    generate_mcqs_from_pdf(PDF_PATH)







After creating the app in the predev environment, an error message is displayed during the approval process: 'Not approved'. Requesting your assistance in looking into this issue.




I have completed generating original question-answer pairs for three domains. Additionally, yesterday I generated diverse question-answer pairs for the Pre-NAV Analysis domain, resulting in 40,000 samples. This morning, I started generating diverse questions and answers for the Trade Review domain.




While onboarding in the predev environment, the Retriever strategies are not showing up—it displays 'No data'. Could someone please look into this?"





# 2. Ask each question and collect answer
for question in questions:
    try:
        conv_payload = {"userQuery": question}
        conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
        conv_response.raise_for_status()

        response_json = conv_response.json()
        bot_response = response_json.get("botResponse", "")  # Only botResponse
        full_response = json.dumps(response_json)            # Entire raw JSON as string

    except Exception as e:
        bot_response = f"Error: {str(e)}"
        full_response = bot_response

    all_results.append({
        "Retriever Strategy": strategy_name,
        "Retriever Config": strategy_config,
        "Question": question,
        "Bot Response": bot_response,
        "Full Raw Response": full_response
    })






import torch
from torch.distributed import init_process_group, destroy_process_group
from torch.distributed._tensor import DeviceMesh, DTensor

# Initialize process group (required before creating device mesh)
init_process_group(backend="nccl")  # Or "gloo" for CPU testing

# Define a device mesh (e.g., for 2 GPUs)
device_mesh = DeviceMesh("cuda", list(range(torch.cuda.device_count()))[:2])

# Local tensor
local_tensor = torch.randn(4, 4).cuda()

# Distribute the tensor
dtensor = DTensor.from_local(local_tensor, device_mesh, placements=["replicate"])

print(dtensor)

# Clean up
destroy_process_group()
Hi Team,

The retriever strategy evaluation and corresponding configurations are stored in the Box folder shared earlier. I'm still working on this part.

For initial testing, I’m using a different question. I will now proceed to create the app, set up the data group in the predev environment, and fetch the results.

I'll use the six questions provided by the client for testing. Additionally, I will update the Excel sheet today to include the following details: retriever strategy, strategy configuration, question, and corresponding answer








import requests
import json
import copy
import pandas as pd

# --- Constants ---
MGMT_BASE_URL = "http://10.76.22.174:10005"
CONV_BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

UPDATE_ENDPOINT = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
CONV_ENDPOINT = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

HEADERS = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ADMIN_TOKEN"
}

# --- Base app metadata (everything except retrieverConfig) ---
BASE_METADATA = {
    "title": "Fund Accounting Q&A App",
    "prompt": "You are a fund accounting expert. Answer concisely.",
    "description": "Q&A for finance, investment, and NAV-related questions.",
    "category": "Finance",
    "metadata": {},
    "vectorStore": {},
    "introduction": "Welcome to the Fund Accounting LLM assistant.",
    "icon": "",
    "raiclient": {
        "eamClientId": "sample-client-id",
        "raiClientId": "sample-rai-client-id",
        "generative": {
            "model": "gpt-4",
            "params": {},
            "n": 1,
            "temperature": 0.7
        },
        "embedding": {
            "model": "embedding-model",
            "params": {}
        },
        "multimodel": {
            "model": "multi-model",
            "params": {}
        },
        "eamClientSecret": "sample-client-secret"
    },
    "entitlements": {}
}

# --- Load 50 retriever strategies from JSON file ---
with open("50_final_corrected_strategies.json", "r") as f:
    RETRIEVER_STRATEGY = json.load(f)

# --- Sample Questions ---
questions = [
    "What is NAV in fund accounting?",
    "What is the role of a transfer agent?",
    "How is tracking error calculated?",
    "Explain risk-adjusted return.",
    "How does portfolio diversification reduce risk?"
]

# --- Final results container ---
all_results = []

# --- Loop through each retriever strategy ---
for strategy in RETRIEVER_STRATEGY["items"]:
    strategy_name = strategy["name"]
    strategy_config = json.dumps(strategy, indent=2)  # Full strategy including name

    # 1. Update app metadata with this retriever config
    payload = copy.deepcopy(BASE_METADATA)
    payload["retrieverConfig"] = strategy

    print(f"\n[Updating retriever config: {strategy_name}]")
    try:
        update_response = requests.post(UPDATE_ENDPOINT, headers=HEADERS, json=payload, timeout=30)
        print("Update Status:", update_response.status_code)
    except Exception as e:
        print("Failed to update metadata:", str(e))
        continue

    # 2. Ask each question and collect answer
    for question in questions:
        try:
            conv_payload = {"userQuery": question}
            conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
            conv_response.raise_for_status()
            try:
                answer = conv_response.json().get("botResponse", json.dumps(conv_response.json()))
            except:
                answer = conv_response.text
        except Exception as e:
            answer = f"Error: {str(e)}"

        all_results.append({
            "Retriever Strategy": strategy_name,
            "Retriever Config": strategy_config,
            "Question": question,
            "Answer": answer
        })

# --- Save all results to Excel ---
df = pd.DataFrame(all_results)
df.to_excel("retriever_strategy_QA_output.xlsx", index=False)
print("\nSaved results to retriever_strategy_QA_output.xlsx")










# Extend the user's provided 5 strategies into 30 by copying and varying the configuration values

base_strategies = [
    {
        "name": "MultiQueryRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "prompt": (
                    "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives.\n"
                    "The user questions are focused on the documents that have been provided with tables, and text data.\n"
                    "Each query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results. "
                    "Provide these alternative questions separated by newlines Original question: (question)"
                ),
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    },
    {
        "name": "SelfQueryRetriever",
        "config": {
            "kwargs": {
                "score_threshold": 0.8,
                "document_contents": "The document contains tabular and textual information",
                "metadata_field_info": [
                    {"name": "dsId", "type": "string", "description": "The id of the datasource where the document is stored"},
                    {"name": "docId", "type": "string", "description": "An id for the document"},
                    {"name": "fileName", "type": "string", "description": "Name of the file"}
                ]
            },
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    },
    {
        "name": "ContextualCompressionRetriever",
        "config": {
            "kwargs": {
                "filters": "llmextractor",
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "embedding_model": "azure-openai.text-embedding-ada-002",
            "similarity_threshold": 0.76
        }
    },
    {
        "name": "EnsembleRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "weights": [0.9, 0.1],
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "numberOfBM25Documents": 5
        }
    },
    {
        "name": "NaiveRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    }
]

# Generate 30 variants
all_strategies = []
for i in range(30):
    base = base_strategies[i % len(base_strategies)]
    new_strategy = {
        "name": f"{base['name']}_{i+1}",
        "config": base["config"].copy()
    }
    config = new_strategy["config"]

    # Randomize configuration values
    config["top_k"] = random.choice([3, 5, 7, 10]) if "top_k" in config else 3
    config["search_type"] = random.choice(["similarity", "bm25", "hybrid"]) if "search_type" in config else "similarity"
    config["searchConfig"]["activateHybridSearch"] = random.choice([True, False])
    config["searchConfig"]["numberOfTextBasedResults"] = random.choice([3, 5, 7, 10])
    config["rerankerConfig"]["activateReranker"] = random.choice([True, False])
    config["rerankerConfig"]["numberOfRerankedResults"] = random.choice([3, 5, 7, 10])
    config["kwargs"]["score_threshold"] = round(random.uniform(0.6, 0.9), 2)

    all_strategies.append(new_strategy)

# Convert to DataFrame for preview
df_30_strategies = pd.DataFrame(all_strategies)
from ace_tools import display_dataframe_to_user
display_dataframe_to_user(name="30 Final Retriever Strategies", dataframe=df_30_strategies)





import requests
import json
import copy
import pandas as pd

# --- Constants ---
MGMT_BASE_URL = "http://10.76.22.174:10005"
CONV_BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

UPDATE_ENDPOINT = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
CONV_ENDPOINT = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

HEADERS = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ADMIN_TOKEN"
}

# --- Fixed app metadata (other than retrieverConfig) ---
BASE_METADATA = {
    "title": "Fund Accounting Q&A App",
    "prompt": "You are a fund accounting expert. Answer concisely.",
    "description": "Q&A for finance, investment, and NAV-related questions.",
    "category": "Finance",
    "metadata": {},
    "vectorStore": {},
    "introduction": "Welcome to the Fund Accounting LLM assistant.",
    "icon": "",
    "raiclient": {
        "eamClientId": "sample-client-id",
        "raiClientId": "sample-rai-client-id",
        "generative": {
            "model": "gpt-4",
            "params": {},
            "n": 1,
            "temperature": 0.7
        },
        "embedding": {
            "model": "embedding-model",
            "params": {}
        },
        "multimodel": {
            "model": "multi-model",
            "params": {}
        },
        "eamClientSecret": "sample-client-secret"
    },
    "entitlements": {}
}

# --- Questions to send after each retriever update ---
questions = [
    "What is NAV in fund accounting?",
    "What is the role of a transfer agent?",
    "How is tracking error calculated?",
    "Explain risk-adjusted return.",
    "How does portfolio diversification reduce risk?"
]

# --- Retriever Strategies ---
RETRIEVER_STRATEGY = {
    "items": [
        {
            "name": "MultiQueryRetriever",
            "config": {
                "top_k": 3,
                "kwargs": {
                    "prompt": "Generate 3 different search queries from different angles.",
                    "score_threshold": 0.8
                },
                "search_type": "similarity",
                "searchConfig": {
                    "activateHybridSearch": False,
                    "numberOfTextBasedResults": 4
                },
                "rerankerConfig": {
                    "activateReranker": False,
                    "numberOfRerankedResults": 3
                }
            }
        },
        {
            "name": "HybridRetriever",
            "config": {
                "top_k": 5,
                "search_type": "hybrid",
                "searchConfig": {
                    "activateHybridSearch": True,
                    "numberOfTextBasedResults": 5
                },
                "rerankerConfig": {
                    "activateReranker": True,
                    "numberOfRerankedResults": 5
                },
                "kwargs": {
                    "prompt": "Try different angles for query generation.",
                    "score_threshold": 0.7
                }
            }
        }
    ]
}

# --- Final results ---
all_results = []

# --- Loop: update retriever + send questions ---
for strategy in RETRIEVER_STRATEGY["items"]:
    strategy_name = strategy["name"]

    # 1. Update App Metadata with current retriever strategy
    payload = copy.deepcopy(BASE_METADATA)
    payload["retrieverConfig"] = strategy

    print(f"\n[Updating retriever config: {strategy_name}]")
    update_response = requests.post(UPDATE_ENDPOINT, headers=HEADERS, json=payload, timeout=30)
    print("Update Status:", update_response.status_code)

    # 2. Run questions using updated retriever config
    for question in questions:
        try:
            conv_payload = { "userQuery": question }
            conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
            conv_response.raise_for_status()

            try:
                answer = conv_response.json()
            except:
                answer = {"error": conv_response.text}

        except Exception as e:
            answer = {"error": str(e)}

        all_results.append({
            "Retriever Strategy": strategy_name,
            "Question": question,
            "Answer": json.dumps(answer)
        })

# --- Save final output to Excel ---
df = pd.DataFrame(all_results)
df.to_excel("retriever_strategy_QA_output.xlsx", index=False)
print("\nSaved results to retriever_strategy_QA_output.xlsx")






import requests
import pandas as pd

# API endpoint
ASK_ENDPOINT = "https://rai.it.statestr.com:4327/agi/query"

# Optional headers (include token if needed)
headers = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ACCESS_TOKEN"
}

# Application ID
app_id = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

# Sample questions
questions = [
    "What is NAV in fund accounting?",
    "Explain the role of a custodian.",
    "What is tracking error in passive investment?",
]

# Define multiple retriever strategies
retriever_strategies = [
    {
        "name": "MultiQuery Retriever",
        "config": {
            "top_k": 3,
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "kwargs": {
                "prompt": "Generate queries from multiple viewpoints.",
                "score_threshold": 0.8
            }
        }
    },
    {
        "name": "BM25 Retriever",
        "config": {
            "top_k": 3,
            "search_type": "bm25",
            "searchConfig": {
                "activateHybridSearch": False
            },
            "rerankerConfig": {
                "activateReranker": False
            },
            "kwargs": {}
        }
    },
    {
        "name": "Hybrid Retriever",
        "config": {
            "top_k": 3,
            "search_type": "hybrid",
            "searchConfig": {
                "activateHybridSearch": True,
                "numberOfTextBasedResults": 3
            },
            "rerankerConfig": {
                "activateReranker": True,
                "numberOfRerankedResults": 3
            },
            "kwargs": {
                "score_threshold": 0.75
            }
        }
    }
]

# Collect results
results = []

for strategy in retriever_strategies:
    strategy_name = strategy["name"]
    
    for question in questions:
        payload = {
            "question": question,
            "appId": app_id,
            "retriever_config": strategy
        }

        try:
            response = requests.post(ASK_ENDPOINT, headers=headers, json=payload, timeout=30, verify=False)
            response.raise_for_status()
            answer = response.json().get("answer", "No answer found")
        except Exception as e:
            answer = f"Error: {str(e)}"

        results.append({
            "Retriever": strategy_name,
            "Question": question,
            "Answer": answer
        })

# Export to Excel
df = pd.DataFrame(results)
df.to_excel("multi_strategy_answers.xlsx", index=False)
print("Saved results to multi_strategy_answers.xlsx")









import requests
import pandas as pd

# Constants
BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"
ENDPOINT = f"{BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

# Headers (add Authorization if needed)
headers = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_TOKEN"  # Uncomment if API is protected
}

# List of questions
questions = [
    "What is NAV in fund accounting?",
    "Explain the role of a transfer agent.",
    "What is tracking error in investment portfolios?",
    "Define risk-adjusted return.",
    "How does portfolio diversification work?"
]

results = []

for question in questions:
    payload = {
        "userQuery": question
    }

    try:
        response = requests.post(ENDPOINT, headers=headers, json=payload, timeout=30)
        response.raise_for_status()

        # Print and store result
        print("Response:", response.json())
        results.append({
            "Question": question,
            "Answer": response.json()
        })

    except Exception as e:
        print("Error:", str(e))
        results.append({
            "Question": question,
            "Answer": f"Error: {str(e)}"
        })

# Save to Excel
df = pd.DataFrame(results)
df.to_excel("conversations_output.xlsx", index=False)
print("Saved results to conversations_output.xlsx")








The original question-answer pairs for the discrimination report domain have been generated. The files are available at the specified location
"I have started generating original question-answer pairs for the Fund Accounting domain."






import pandas as pd
import requests

# Step 1: Define retriever strategies in the new format
retriever_strategy = {
    "items": [
        {
            "name": "MultiQuery Retriever",
            "config": {
                "top_k": 3,
                "kwargs": {
                    "prompt": "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives. The user questions are focused on the documents that have been provided with tables and text data. Each query MUST tackle the question from a different viewpoint. We want to get a variety of RELEVANT search results. Provide these alternative questions separated by newlines.\nOriginal question: (question)",
                    "score_threshold": 0.8
                },
                "search_type": "similarity",
                "searchConfig": {
                    "activateHybridSearch": False,
                    "numberOfTextBasedResults": 4
                },
                "rerankerConfig": {
                    "activateReranker": False,
                    "numberOfRerankedResults": 3
                }
            }
        }
        # You can add more strategies to this list
    ]
}

# Step 2: Define evaluation questions
questions = [
    "What is the role of fiduciary duties?",
    "Explain the concept of risk-adjusted returns.",
    "How does SSGA manage index tracking error?",
    "What is the purpose of an investment policy statement?",
    "Describe the difference between active and passive management."
]

# Step 3: Replace with your actual deployed API endpoint
base_url = "http://your-dev-app/api/ask"

# Step 4: Query and collect responses
all_results = []

for question in questions:
    row = {"question": question}
    for strategy in retriever_strategy["items"]:
        try:
            payload = {
                "question": question,
                "retriever_config": strategy
            }
            response = requests.post(base_url, json=payload, timeout=30)
            result = response.json().get("answer", "No response")
        except Exception as e:
            result = f"Error: {str(e)}"
        row[strategy["name"].replace(" ", "_").lower() + "_response"] = result
    all_results.append(row)

# Step 5: Save results to Excel
df = pd.DataFrame(all_results)
df.to_excel("ssgenai_rag_retriever_results.xlsx", index=False)
print("Results saved to ssgenai_rag_retriever_results.xlsx")






row[strategy["name"].replace




import pandas as pd
import requests

# Step 1: Define retriever configurations
retriever_configs = {
    "config_1": {"retriever_type": "faiss", "top_k": 3, "mode": "similarity"},
    "config_2": {"retriever_type": "faiss", "top_k": 5, "mode": "mmr"},
    "config_3": {"retriever_type": "bm25", "top_k": 3, "k1": 1.2, "b": 0.75},
    "config_4": {"retriever_type": "bm25", "top_k": 5, "k1": 2.0, "b": 1.0},
    "config_5": {"retriever_type": "multiquery", "top_k": 5, "merge_strategy": "concatenate", "llm_variant": "gpt-3.5"},
}

# Step 2: Define evaluation questions
questions = [
    "What is the role of fiduciary duties?",
    "Explain the concept of risk-adjusted returns.",
    "How does SSGA manage index tracking error?",
    "What is the purpose of an investment policy statement?",
    "Describe the difference between active and passive management."
]

# Step 3: Replace this with your actual dev endpoint
base_url = "http://your-dev-app/api/ask"

# Step 4: Collect responses
all_results = []

for q in questions:
    row = {"question": q}
    for config_id, config in retriever_configs.items():
        try:
            payload = {
                "question": q,
                "retriever_config": config
            }
            response = requests.post(base_url, json=payload, timeout=30)
            result = response.json().get("answer", "No response")
        except Exception as e:
            result = f"Error: {str(e)}"
        row[config_id + "_response"] = result
    all_results.append(row)

# Step 5: Export to Excel
df = pd.DataFrame(all_results)
df.to_excel("retriever_comparison_results.xlsx", index=False)
print("Saved results to retriever_comparison_results.xlsx")






import pandas as pd
import tiktoken

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        return len(encoding.encode(text))
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return 0

# Load the Excel file
df = pd.read_excel("your_input_file.xlsx")  # Change to your actual file path

# Calculate tokens for each Answer
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: calculate_tokens(str(x)) if pd.notna(x) else 0)

# Export the updated DataFrame to a new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)





import pandas as pd
import tiktoken  # for token counting

# Load Excel file
df = pd.read_excel("your_file.xlsx")  # Update with your file path

# Choose tokenizer (e.g., for OpenAI's gpt-3.5-turbo or gpt-4)
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# Count tokens in 'Answer' column
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: len(encoding.encode(str(x))) if pd.notna(x) else 0)

# Save to new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)



On the call, you can ask me different scenarios, and I will demonstrate them. If it's not up to your expectations, I will stop and work on it. I'm requesting the call because I want to avoid facing similar issues again in the future.





For testing, I injected a document related to the SOP. After the injection, I asked a few questions based on that document, but it did not return the answers. Instead, it gave the following response to the questions:


For testing the procedural questions, I have rerun the pipeline for the Discrimination Report domain. The QA pairs were generated successfully, and I am currently verifying the answers to the procedural questions




# LLaMA-3.1-8B-Instruct Fine-Tuning with LoRA and MLflow

# Step 1: Environment Setup
import random, os, torch, numpy as np, pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
SEED = 42
seed_everything(SEED)

# Step 2: Load Tokenizer and Model
MODEL_PATH = "/path/to/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, model_max_length=1024, padding_side="left", truncation=True, padding=True
)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
model.resize_token_embeddings(len(tokenizer))

# Step 3: Load and Format Dataset
df = pd.read_csv("qa_dataset.csv")[["Question", "Answer"]].dropna().sample(frac=1).reset_index(drop=True)
def format_example(row):
    messages = [
        {"role": "system", "content": "Answer the question"},
        {"role": "user", "content": row["Question"]},
        {"role": "assistant", "content": row["Answer"]}
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False)
df["text"] = df.apply(format_example, axis=1)

# Step 4: Token Count Check
def count_tokens(row):
    return len(tokenizer(row["text"], add_special_tokens=True)["input_ids"])
df["token_count"] = df.apply(count_tokens, axis=1)

# Step 5: Dataset Split and Convert to HF Format
val, test = train_test_split(df, test_size=0.2, random_state=SEED)
dataset = DatasetDict({
    "train": Dataset.from_pandas(df),
    "validation": Dataset.from_pandas(val),
    "test": Dataset.from_pandas(test)
})

# Step 6: Configure LoRA
from peft import LoraConfig, TaskType, get_peft_model
lora_config = LoraConfig(
    r=64, lora_alpha=128, target_modules="all-linear", lora_dropout=0.1,
    bias="none", task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# Step 7: Define Training Config with MLflow
from trl import SFTConfig
OUTPUT_DIR = "/path/to/output"
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR, dataset_text_field="text", num_train_epochs=4,
    max_seq_length=1024, gradient_checkpointing=False, per_device_train_batch_size=2,
    per_device_eval_batch_size=2, gradient_accumulation_steps=4, learning_rate=1e-4,
    fp16=True, eval_strategy="steps", logging_steps=10, save_strategy="steps",
    save_steps=0.2, eval_steps=0.2, lr_scheduler_type="cosine", report_to="mlflow", seed=SEED
)

# Step 8: Train with SFTTrainer and Log to MLflow
from trl import SFTTrainer
import mlflow
with mlflow.start_run():
    trainer = SFTTrainer(
        model=model,
        args=sft_config,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    mlflow.log_params(sft_config.to_dict())
    mlflow.log_artifact(OUTPUT_DIR)

# Step 9: Merge LoRA Adapters
from peft import PeftModel
peft_model = PeftModel.from_pretrained(model, OUTPUT_DIR)
merged_model = peft_model.merge_and_unload()

# Step 10: Register Final Model to MLflow
from transformers import pipeline
mlflow.transformers.log_model(
    transformers_model=pipeline("text-generation", model=merged_model, tokenizer=tokenizer),
    artifact_path="merged_model_without_model_config",
    task="llm/v1/chat"
)

# Step 11: Run Inference Example
mlflow_model = mlflow.pyfunc.load_model("runs:/<run_id>/merged_model_without_model_config")
mlflow_model.predict({
    "messages": [
        {"role": "system", "content": "Answer the question."},
        {"role": "user", "content": "What is LoRA?"}
    ],
    "temperature": 0.5,
    "max_tokens": 100
})

# Step 12: What is LoRA?
# LoRA (Low-Rank Adaptation) updates a small set of trainable matrices A and B:
# W' = W + A @ B
# W = frozen weight matrix
# A = [r x d], B = [d x k], where r is much smaller
# Benefits: 90%+ parameter savings, low memory, no catastrophic forgetting, modular
# Use merge_and_unload() to finalize model after training






### LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)

---

#### 1. Environment Initialization

* **Purpose**: Set up the Python environment and load libraries.
* **Tools**: `transformers`, `datasets`, `torch`, `pandas`, `mlflow`, `seaborn`, `matplotlib`
* **Action**: Import required packages, set a global random seed (`SEED = 42`), and configure plotting.

---

#### 2. Load Tokenizer and Model

* **Purpose**: Initialize tokenizer and quantized base model for fine-tuning.
* **Tools**: `AutoTokenizer`, `AutoModelForCausalLM`
* **Action**:

  * Load tokenizer from the base model path.
  * Set `pad_token` = `eos_token`, padding side = "left" (required for causal LMs).
  * Load the quantized model (e.g., 4-bit with `BitsAndBytesConfig` if required).

---

#### 3. Dataset Preparation

* **Purpose**: Read, clean, and prepare data for fine-tuning.
* **Tools**: `pandas`
* **Action**:

  * Load QA dataset from CSV.
  * Keep only `Question` and `Answer` columns.
  * Drop missing values and shuffle the rows.

---

#### 4. Format Prompts for Chat

* **Purpose**: Convert QA rows into structured multi-turn chat format for LLM training.
* **Tools**: `tokenizer.apply_chat_template()`
* **Action**:

  * Construct `[system → user → assistant]` style chat messages.
  * Generate text prompts for training using tokenizer templates.

---

#### 5. Analyze Token Length

* **Purpose**: Ensure inputs fit within model's `max_seq_length` (1024 tokens).
* **Tools**: `tokenizer`, `matplotlib`, `PercentFormatter`
* **Action**:

  * Tokenize each example to count tokens.
  * Plot histograms to visualize token distribution.

---

#### 6. Split Dataset

* **Purpose**: Create training, validation, and test sets.
* **Tools**: `train_test_split` from `sklearn`
* **Action**:

  * Split into 80% train and 20% validation sets.
  * Convert them into HuggingFace `Dataset` objects using `Dataset.from_pandas()`.

---

#### 7. Configure PEFT (LoRA)

* **Purpose**: Reduce memory cost using parameter-efficient fine-tuning.
* **Tools**: `peft.LoraConfig`, `get_peft_model()`
* **Action**:

  * Apply LoRA only on linear layers.
  * Configure rank (`r`), alpha, dropout, and target modules.
  * Prepare model for training.

---

#### 8. Define Training Configuration

* **Purpose**: Set training hyperparameters and logging behavior.
* **Tools**: `trl.SFTConfig`
* **Key Settings**:

  * `max_seq_length = 1024`
  * `batch_size = 2`
  * `gradient_accumulation = 4`
  * `learning_rate = 1e-4`
  * `eval_steps = 0.2`, `save_steps = 0.2`
  * `fp16 = True`, `lr_scheduler = cosine`
  * `report_to = "mlflow"`

---

#### 9. Train Model

* **Purpose**: Fine-tune the base model with LoRA layers.
* **Tools**: `SFTTrainer`
* **Action**:

  * Train on the `train` set, evaluate on `validation`.
  * Save the model and tokenizer to `OUTPUT_DIR`.
  * Log everything to MLflow inside `with mlflow.start_run()`.

---

#### 10. Merge LoRA with Base Model

* **Purpose**: Convert adapter-based model into a standalone fine-tuned model.
* **Tools**: `PeftModel`, `merge_and_unload()`
* **Action**:

  * Load trained LoRA model.
  * Merge adapters into base model to remove dependency on LoRA.

---

#### 11. Register and Log Model with MLflow

* **Purpose**: Store, version, and serve model in production.
* **Tools**: `mlflow.transformers.log_model()`, `mlflow.register_model()`
* **Action**:

  * Log the `pipeline("text-generation")` model.
  * Register it to Databricks model registry.

---

#### 12. Copy Artifacts to UC Volume (Optional)

* **Purpose**: Store model artifacts in a Unified Catalog (UC) for sharing or backup.
* **Tools**: `mlflow.tracking.MlflowClient()`
* **Action**:

  * Download all model artifacts and copy to desired UC directory path.

---

#### 13. Inference

* **Purpose**: Test the model with a prompt.
* **Tools**: `mlflow_model.predict()`
* **Input**: JSON with `messages` (system, user), `temperature`, and `max_tokens`
* **Output**: Model-generated response.



LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)
Environment Setup:
Import all required libraries (transformers, datasets, torch, mlflow, etc.).

Tokenizer & Model Loading:
Load the tokenizer and quantized LLaMA model. Set pad token as EOS and padding side as "left" for causal LM.

Dataset Loading:
Read the question-answer dataset from a CSV file. Drop nulls, shuffle, and retain relevant columns.

Prompt Formatting:
Format data into structured "chat" format (system, user, assistant roles) using the tokenizer’s chat template.

Token Count Analysis:
Calculate and visualize token lengths to ensure most samples are under 1024 tokens (model limit).

Train/Validation Split:
Use train_test_split() to divide data into training and validation sets for evaluation.

Dataset Conversion:
Convert Pandas DataFrames to HuggingFace DatasetDict for training compatibility.

PEFT (LoRA) Configuration:
Define LoraConfig to fine-tune only a small subset of layers efficiently using LoRA adapters.

Training Configuration:
Set hyperparameters like batch size, epochs, learning rate, warmup ratio, and enable MLflow tracking.

Model Training:
Fine-tune the model using SFTTrainer with chat-formatted inputs. Log metrics and artifacts via MLflow.

Merge Adapters:
After training, merge LoRA adapters with the base model to create a fully functional fine-tuned model.

Model Logging:
Log the merged model to MLflow using transformers.log_model() for future deployment.

Model Registration:
Register the model version in Databricks MLflow Registry with a custom name.

Artifact Copy (Optional):
Download and copy model artifacts from MLflow run to a Unified Catalog directory.

Inference:
Load the logged model and run inference using mlflow_model.predict() with system-user prompts.

Let me know if you want this exported to a .md or .pdf file.
















ChatGPT can make mistakes. Check important info. See Cookie Preferences.








Hi Chandreshkumar Dedani, Vivek. The results from the previous run were good. I have now changed the dataset to include 3 diverse procedural questions. Please use this dataset for your training, and also use the parameters: context length = 1024 and epochs = 4."






import pandas as pd

# Load your Excel or CSV file
df = pd.read_excel('your_dataset.xlsx')  # or use read_csv if it's a CSV

# Group by the 'Original Question' column and select the first 3 entries per group
filtered_df = df.groupby('Original Question', group_keys=False).head(3)

# Save the result if needed
filtered_df.to_excel('filtered_output.xlsx', index=False)

print("Filtered dataset with first 3 questions per original question.")





import json
import pandas as pd

# Step 1: Load JSON file
with open('your_file.json', 'r') as file:
    data = json.load(file)

# Step 2: Normalize (flatten) the JSON data
df = pd.json_normalize(data, sep='_')

# Step 3: Save to Excel
df.to_excel('converted_output.xlsx', index=False)

print("Conversion complete. File saved as 'converted_output.xlsx'")






import os
import time
import json
import dotenv
import pandas as pd
from langchain.chat_models import AzureChatOpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables
dotenv.load_dotenv()

# Azure LLM Configuration
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

llm = AzureChatOpenAI(
    temperature=0,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name="ssgpt-4"
)

# ========================== Step 1: Convert Excel to JSON ==========================

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")
    json_data = [{"question": row["Question"], "answer": row["Answer"]} for _, row in df.iterrows()]
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=4, ensure_ascii=False)
    print(f"Converted Excel to JSON at {output_json_path}")
    return output_json_path

# ========================== Step 2: Read JSON ==========================

def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"Read {len(data)} records from {file_path}")
        return data
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

# ========================== Step 3: Build Prompt ==========================

def build_rating_prompt(question, answer):
    return f"""
Rate each question-answer pair on a scale from 1-10, based on:
- Accuracy (0-3): factual correctness
- Relevance (0-2): relevance to content
- Clarity (0-2): clear language
- Usefulness (0-3): value for model learning

YOU MUST RETURN A VALID JSON OBJECT WITH THIS EXACT SCHEMA:
{{
  "question": "{question}",
  "answer": "{answer}",
  "rating": <1-10>
}}

*** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE ***
"""

# ========================== Step 4: Validate with LLM ==========================

def validate_with_llm(entry, index, retries=3):
    prompt = build_rating_prompt(entry["question"], entry["answer"])
    for attempt in range(retries):
        try:
            response = llm.predict(prompt)
            result = json.loads(response)
            return index, result
        except Exception as e:
            print(f"Retry {attempt+1} failed: {e}")
            time.sleep(2)
    return index, {
        "question": entry["question"],
        "answer": entry["answer"],
        "rating": 0
    }

# ========================== Step 5: Run Pipeline ==========================

def full_pipeline(input_excel, final_excel_output, final_json_output, max_workers=5):
    temp_json_path = convert_excel_to_json(input_excel)
    qa_data = read_json_file(temp_json_path)
    results = [None] * len(qa_data)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(validate_with_llm, entry, i) for i, entry in enumerate(qa_data)]
        for future in as_completed(futures):
            index, result = future.result()
            results[index] = result

    # Save to Excel and JSON
    pd.DataFrame(results).to_excel(final_excel_output, index=False)
    with open(final_json_output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"Pipeline complete. Results saved to {final_excel_output} and {final_json_output}")









Following up on the productivity improvement, here’s a summary of the value delivered by migrating from the Hugging Face summarization model to the new LLM-based chatbot:

1. Key Difference Observed
Previous (Hugging Face Model): Users faced delays due to slower response times and limited interactivity (only summary generation).

Current (LLM Chatbot): Delivers instant, context-aware answers through natural conversation, significantly reducing turnaround time.

2. Quantified Benefit (Illustrative Example)
Average handling time (per query):

Hugging Face: ~12 mins (includes manual reading + follow-ups)

LLM Chatbot: ~4 mins (direct answers with follow-up support)

Monthly queries handled: 2,000

FTE rate: $30/hour

Productivity Gain = (12 – 4)/60 × 2,000 × $30 = $8,000/month

3. Next Steps
I will engage with actual users to validate the above assumptions and share a data-backed benefit report.

Let me know if you'd like this structured as a slide or summary deck as well.

Best regards,
[Your Name]

Would you like me to help you turn this into a one-slide summary for stakeholders?
















ChatGPT can make mistakes. Check important info. See Cookie Preferences






To quantify productivity improvement from replacing the Hugging Face summarization model with the new LLM-based chatbot, here's the proposed approach:

1. User Experience Comparison
We’re reaching out to users to compare:

Response speed and accuracy before (summarization only) vs. now (interactive LLM chatbot).

How the LLM chatbot helps resolve end-customer queries faster and more completely.

2. Estimated Productivity Gains
Based on early indicators:

Previous model: Provided passive summaries, required human interpretation.

Current LLM chatbot: Enables direct question-answering and multi-turn conversations, reducing manual effort.

3. Quantifiable Impact (Illustrative)
Let’s assume:

Time saved per interaction: Increased from 4 mins (summarization) to 10 mins (LLM chatbot)

Monthly interactions: 2,000

FTE cost/hour: $30

Productivity Gain:
LLM chatbot: (10/60) × 2,000 × $30 = $10,000/month
Hugging Face: (4/60) × 2,000 × $30 = $4,000/month
Net gain: ~$6,000/month

We’ll validate these assumptions with real user input and share a finalized benefit report shortly.









Regarding the productivity improvement through our chatbot (built using Hugging Face summarization models), here's the approach to quantify the benefit:

User Feedback Gathering
I’ll speak with end-users to understand:

How frequently they use the chatbot.

Whether it helps them respond faster and more accurately to customer queries.

Specific tasks where the summarization feature reduces manual effort.

Productivity Quantification
Based on initial assumptions (to be validated with users):

Time saved per interaction: ~5–10 minutes

Monthly usage: e.g., 1,500 summarization requests

FTE cost/hour: $30

Sample benefit calculation:
(8 min saved / 60) × 1,500 × $30 = $6,000/month

This is a conservative estimate. I’ll validate the figures with users and refine the benefit calculation accordingly.

Best regards,
[Your Name]

Would you like help preparing a quick user feedback form or survey to gather this data faster?






Do you like this personality?














ChatGPT can make mistakes. Check important info. See Cookie Preference





# Objective:
You are an AI assistant that extracts data from document images and returns it in structured markdown format, preserving the layout and formatting exactly as shown in the image.

# Instructions:

1. You are provided with images of PDF pages. Go through **each image carefully** and extract **all visible information**.

2. Format the extracted information in markdown style, **replicating the visual layout of the PDF** (text structure, tables, images, bullet points).

3. For **images, tables, and screenshots**, preserve:
   - Borders
   - Cell alignment
   - Font styles (bold, italics if applicable)
   - Highlighted or bold text
   - If present, add an inline description:
     - Use `image_description:` for images
     - Use `table_description:` for tables
     - Use `screenshot_description:` for screenshots

4. Store all extracted images in a folder named `images/` in `.png` format, and insert them in markdown as:

5. **Do not include any footers, headers, or page numbers** from the PDF.

6. **Extract the text exactly** as it appears in the image, including punctuation, symbols, and formatting.

7. If the document contains **errors, warnings, or stack traces**, skip them unless explicitly asked to extract them.

8. **Important Markdown Formatting Rules**:
- Never use `#`, `##`, `###`, or `**bold**` for headings unless the line is a clear section title (e.g., appears top-centered with larger font or boxed).
- If the content is a **list of similar items** (e.g., report names, fund types, field labels), always format them as bullet points using `-`, even if they are bold or appear like headings.
- Example:
  ```
  - Base Equivalent Cash Statement
  - Earned Income by Asset ID
  ```

9. If two or more items appear one after another (e.g., 17 report names), even across pages, **treat them as a continuous bullet list**.

10. Do not return the markdown content inside code blocks (no triple backticks).

# Output:
A structured markdown representation of all PDF content with:
- Correct visual layout
- Tables, images, and screenshots with descriptions
- Lists as bullet points
- No extra formatting like headings unless visually justified





Download 17 MYSS excel reports and run MYSS recon macro for external and internal funds.

Use the same method to create template and download 17 MYSS excel reports as below.

Holdings Alpha

Net Asset Value

Dividend Payable

Dividend Receivable

FX Pending Forward Activity

FX Pending Spot Activity

Interest Payable

Interest Receivable

MBS Interest Payable

MBS Interest Receivable

Open Trades

Tax Expense Payable

Tax Reclaim Receivable

Dividend Income Summary

Realized Gain Loss

Processing Page 8

c:\venv\Lib\site-packages\ssrai\utils\request utils.py:15: PydanticDeprecatedSince20: The `dict' method is depre

session.proxies = proxy.dict()

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is b

warnings.warn(

Response: **Base Equivalent Cash Statement**

**Earned Income by Asset ID**







import os
import base64
import pdfplumber
from PIL import Image
from io import BytesIO
from langchain.schema import HumanMessage
from ssrai import SSRAIClient

def extract_text_from_pdf(pdf_path, llm, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    extract_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"\n--- Processing Page {i + 1} ---\n")

            # Convert page to image
            page_image = page.to_image(resolution=300)
            image_path = os.path.join(output_folder, f"page_{i+1}.png")
            page_image.save(image_path, format="PNG")

            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": """You are an AI assistant that extracts data from documents and returns them as structured markdown.
                        
- Go through the image of the PDF page.
- Extract details exactly as seen.
- Preserve tables, styles, bold text, and structure.
- Add short inline descriptions starting with `image_description:`, `table_description:`, etc.
- Return in markdown format.
- Do not use code blocks."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            )

            response = llm.invoke([message])
            print("Response:", response.content)
            extract_list.append(response.content)

    return extract_list





import pandas as pd
import ast
from datasets import Dataset
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy
from ragas import evaluate

# === Preprocessing: Extract pageContentExtract ===
def extract_pageContentExtract(meta_val):
    try:
        if pd.isna(meta_val):
            return ""
        if isinstance(meta_val, str):
            parsed = ast.literal_eval(meta_val)
        elif isinstance(meta_val, list):
            parsed = meta_val
        else:
            return ""

        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
            return parsed[0].get("pageContent Extract", "")
        return ""
    except Exception as e:
        print(f"Failed to parse: {meta_val}\nError: {e}")
        return ""

# === Prepare Dataset for RAGAS ===
def prepare_dataset(df):
    records = {
        "question": df["user_query"].astype(str),
        "answer": df["bot_response"].astype(str),
        "contexts": df["context"].astype(str).apply(lambda x: [x]),  # list of one context string
    }
    return Dataset.from_dict(records)

# === Main RAGAS Evaluation Pipeline ===
def run_ragas_evaluation(input_excel_path, output_excel_path):
    # Load Excel
    df = pd.read_excel(input_excel_path)

    # Extract context using your custom function
    df["context"] = df["source_context"].apply(extract_pageContentExtract)

    # Fill NA to avoid crashes
    df.fillna("", inplace=True)

    # Prepare for RAGAS
    ragas_dataset = prepare_dataset(df)

    # Run evaluation
    results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_correctness, answer_relevancy]
    )

    # Add scores (converted to percentages)
    df["Faithfulness (%)"] = [round(score * 100, 2) for score in results["faithfulness"]]
    df["Answer Correctness (%)"] = [round(score * 100, 2) for score in results["answer_correctness"]]
    df["Answer Relevancy (%)"] = [round(score * 100, 2) for score in results["answer_relevancy"]]

    # Save output
    df.to_excel(output_excel_path, index=False)
    print(f"✅ Evaluation complete! Output saved to: {output_excel_path}")

# === Execute Script ===
if __name__ == "__main__":
    input_excel = r"\\mfgdcu02\p872643\Desktop\ssgenai_evalution\Audit assistance questions.xlsx"
    output_excel = "evaluation_output_ragas.xlsx"
    run_ragas_evaluation(input_excel, output_excel)

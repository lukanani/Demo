UPDATE fts_tasks
SET status = '"Not Started"'
WHERE id = '1370fb9a-1021-48d7-9676-dab2a4d684d7';




SELECT * FROM data_curation_strategies WHERE id = '73e4de99-5de6-4a8f-91b3-52dd7a2482eb';

import os

# Define log formats
standard_format = '[%(asctime)s] [%(threadName)s:%(thread)d] [task_id:%(name)s] [%(filename)s:%(lineno)d]'
simple_format = '[%(levelname)s] [%(asctime)s] [PID: %(process)d TID: %(thread)d] [%(filename)s:%(lineno)d] %(message)s'

# Read environment variables or set defaults
LOG_PATH = os.getenv('LOG_PATH', '../logs')
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
LOG_COUNT = int(os.getenv('LOG_COUNT', 10))
LOG_FILE = os.getenv('LOG_FILE_NAME', 'fts.log')
AZURE_SERVICES_LOG_FILE = os.getenv('AZURE_SERVICES_LOG_FILE', 'azure-services.log')

# Ensure log directory exists
if not os.path.isdir(LOG_PATH):
    os.makedirs(LOG_PATH)

azure_services_logfile_path = os.path.join(LOG_PATH, AZURE_SERVICES_LOG_FILE)

# Full Logging Configuration Dictionary
LOG_CONF = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "simple": {
            "format": simple_format
        },
        "standard": {
            "format": standard_format
        }
    },
    "handlers": {
        "stdout": {
            "class": "logging.StreamHandler",
            "level": LOG_LEVEL,
            "formatter": "simple",
            "stream": "ext://sys.stdout"
        },
        "stdout_error_handler": {
            "class": "logging.StreamHandler",
            "level": "ERROR",
            "formatter": "simple",
            "stream": "ext://sys.stdout"
        },
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": LOG_LEVEL,
            "formatter": "simple",
            "filename": os.path.join(LOG_PATH, LOG_FILE),
            "maxBytes": 1024 * 1024 * 5,
            "backupCount": LOG_COUNT,
            "encoding": "utf8"
        },
        "azure_services_logfile": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "INFO",
            "formatter": "simple",
            "filename": azure_services_logfile_path,
            "maxBytes": 1024 * 1024 * 5,
            "backupCount": 5,
            "encoding": "utf-8"
        }
    },
    "loggers": {
        "": {
            "handlers": ["file", "stdout"],
            "level": LOG_LEVEL,
            "propagate": True
        },
        "azure": {
            "handlers": ["stdout_error_handler", "azure_services_logfile"],
            "level": "INFO",
            "propagate": True
        },
        "azure.eventhub._pyamqp.management_link": {
            "handlers": ["stdout_error_handler", "azure_services_logfile"],
            "level": "INFO",
            "propagate": True
        },
        "sqlalchemy": {
            "handlers": ["stdout_error_handler", "azure_services_logfile"],
            "level": "INFO",
            "propagate": True
        }
    }
}






import os
import time
import logging
import warnings

from promptflow.client import PFClient, load_flow

from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.storage.blob_service import BlobService
from common_svc.config.config_store import BlobConfig
from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao
from fts_mgmt.dao.data_curation_mgmt_dao import DataCurationServiceDAO
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao
from fts_mgmt.utils.util import flatten_config, get_strategy_storage_path
from fts_mgmt.utils.objects import TaskStatusEnum, RuntimeConfigModel

# Suppress warnings
warnings.filterwarnings("ignore")

# Configure logging
configure_loggers()
logger = logging.getLogger("PromptflowDaemon")

# Set environment
os.environ.setdefault("ENV", "PREDEV")

# Ensure promptflow root directory exists
PROMPTFLOW_ROOT_PATH = os.path.abspath("./promptflows")
os.makedirs(PROMPTFLOW_ROOT_PATH, exist_ok=True)

def merge_runtime_with_dynamic(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict):
    merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": RuntimeConfigModel(**merged_dynamic).dict()
    }

def process_promptflow_task(task):
    task_dao = TaskMgmtDao()
    dcs_dao = DataCurationServiceDAO()
    ds_dao = DataMgmtServiceDao()

    try:
        logger.info(f"Processing task: {task.id}")
        logger.info(f"TASK CONFIG: {task.config}")

        config = task.config or {}

        if 'dataset_id' not in config or 'data_curation_strategy_id' not in config:
            raise KeyError("Missing required keys in task.config. Required: 'dataset_id', 'data_curation_strategy_id'")

        dataset = ds_dao.get_dataset_info(config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        blob_service = BlobService()
        blob_location = blob_service.blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=get_strategy_storage_path(dcs.id, dcs.name)
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_path):
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {result.output}")
        output_path = flat_inputs.get("dynamic_blob_path", "")

        task_dao.update_task(
            task_id=task.id,
            status="SUCCESS",
            output={"outputDirectory": output_path, "results": result.output}
        )

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status="FAILURE",
            output={"error": str(e)}
        )

def daemon_loop():
    logger.info("Promptflow Daemon Started")
    task_dao = TaskMgmtDao()

    while True:
        try:
            task = task_dao.fetch_and_lock_pending_task()
            if task:
                process_promptflow_task(task)
            else:
                logger.info("No pending tasks. Sleeping for 60 seconds.")
                time.sleep(60)
        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)

if __name__ == "__main__":
    daemon_loop()











def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        task_dao = TaskMgmtDao()
        dcs_dao = DataCurationServiceDAO()
        ds_dao = DataMgmtServiceDao()

        dataset = ds_dao.get_dataset_info(task.config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(task.config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        # Use BlobService class to get the blob_location
        blob_service = BlobService()
        blob_location = blob_service.blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=get_strategy_storage_path(dcs.id, dcs.name)
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_path):
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = task.config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {result.output}")
        output_path = flat_inputs.get("dynamic_blob_path", "")

        task_dao.update_task(
            task_id=task.id,
            status="SUCCESS",
            output={"outputDirectory": output_path, "results": result.output}
        )

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status="FAILURE",
            output={"error": str(e)}
        )





I have implemented several injection strategies in an automated manner using different retrieval methods, and the generated results appear accurate and satisfactory."





SELECT * FROM data_curation_strategies WHERE id = 'd5410ad8-e361-4129-8bee-cc47cef7acbd';




def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        logger.info(f"TASK CONFIG: {task.config}")

        task_dao = TaskMgmtDao()
        dcs_dao = DataCurationServiceDAO()
        ds_dao = DataMgmtServiceDao()

        # Defensive check
        config = task.config or {}

        if 'dataset_id' not in config or 'data_curation_strategy_id' not in config:
            raise KeyError("Missing required keys in task.config. Required: 'dataset_id', 'data_curation_strategy_id'")

        dataset = ds_dao.get_dataset_info(config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        strategy_path = get_strategy_storage_path(dcs.id, dcs.name)
        blob_location = setup_blob_resource().blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_path):
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {result.output}")
        output_path = flat_inputs.get("dynamic_blob_path", "")

        task_dao.update_task(
            task_id=task.id,
            status="SUCCESS",
            output={"outputDirectory": output_path, "results": result.output}
        )

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status="FAILURE",
            output={"error": str(e)}
        )












from psycopg2.sql import SQL, Literal
import psycopg2
import jsonfrom sqlalchemy import literal
from sqlalchemy.dialects.postgresql import JSONB




from sqlalchemy import cast, String

def fetch_and_lock_pending_task(self):
    """Fetches the next NOT_STARTED task and marks it IN_PROGRESS"""
    try:
        with DBSession() as db:
            tasks = db.session.query(Task).all()
            logger.info(f"Total tasks in DB: {len(tasks)}")

            for t in tasks:
                logger.info(f"TASK: id={t.id}, status={t.status}")

            # Match JSON string value by wrapping in double quotes
            task = (
                db.session.query(Task)
                .filter(cast(Task.status, String) == '"NOT_STARTED"')
                .order_by(Task.modified_ts.asc())
                .first()
            )

            if task:
                task.status = '"IN_PROGRESS"'  # Also in JSON-compatible format
                task.modified_ts = get_current_datetime()
                db.session.commit()
                logger.info(f"Locked and fetched task ID: {task.id}")
            else:
                logger.info("No tasks found with status NOT_STARTED")

            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")









def fetch_and_lock_pending_task(self):
    """Fetch the next NOT_STARTED task and mark it IN_PROGRESS"""
    task = None
    try:
        with DBSession() as session:
            task = (
                session.query(Task)
                .filter(Task.status == TaskStatusEnum.NOT_STARTED)
                .order_by(Task.modified_ts.asc())
                .first()
            )

            if not task:
                logger.info("No NOT_STARTED tasks available.")
                return None

            logger.info(f"Fetched task ID={task.id}, marking as IN_PROGRESS")
            task.status = TaskStatusEnum.IN_PROGRESS
            task.modified_ts = get_current_datetime()
            session.commit()
    except Exception as e:
        logger.error(f"Error while fetching and locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock task.")

    return task







from fts_commons.db.model import Task
from fts_commons.utils.util import get_current_datetime
from fts_mgmt.utils.objects import TaskStatusEnum
from common_svc.db.base import DBSession
import logging

logger = logging.getLogger(__name__)

class TaskMgmtDao:
    
    def fetch_and_lock_pending_task(self):
        """Fetch the next NOT_STARTED task and mark it IN_PROGRESS"""
        task = None
        session = DBSession().start_session()
        try:
            task = (
                session.query(Task)
                .filter(Task.status == TaskStatusEnum.NOT_STARTED)
                .order_by(Task.modified_ts.asc())
                .first()
            )

            if not task:
                logger.warning("Couldn't find any NOT_STARTED tasks for daemon processing.")
                return None

            logger.info(f"Fetched task: ID={task.id}, updating status to IN_PROGRESS")

            task.status = TaskStatusEnum.IN_PROGRESS
            task.modified_ts = get_current_datetime()
            session.commit()

        except Exception as e:
            session.rollback()
            logger.error(f"Exception while locking task: {e}", exc_info=True)
            raise Exception("Failed to fetch and lock task.")

        finally:
            session.close()

        return task







def fetch_and_lock_pending_task(self):
    """Fetches the next NOT_STARTED task and marks it IN_PROGRESS"""
    try:
        with DBSession() as db:
            tasks = db.session.query(Task).all()
            logger.info(f"Total tasks in DB: {len(tasks)}")

            for t in tasks:
                logger.info(f"TASK: id={t.id}, status={t.status}")

            task = (
                db.session.query(Task)
                .filter(Task.status == TaskStatusEnum.NOT_STARTED)
                .order_by(Task.modified_ts.asc())
                .first()
            )
            if task:
                task.status = TaskStatusEnum.IN_PROGRESS
                task.modified_ts = get_current_datetime()
                db.session.commit()
                logger.info(f"Locked and fetched task ID: {task.id}")
            else:
                logger.info("No tasks found with status NOT_STARTED")
            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")







from sqlalchemy import func

def fetch_and_lock_pending_task(self):
    """Fetches the next NOT_STARTED task and marks it IN_PROGRESS"""
    try:
        with DBSession() as db:
            task = (
                db.session.query(Task)
                .filter(func.json_extract_path_text(Task.status, 'value') == "NOT_STARTED")
                .order_by(Task.modified_ts.asc())
                .first()
            )
            if task:
                task.status = {"value": "IN_PROGRESS"}  # update as JSON
                task.modified_ts = get_current_datetime()
                db.session.commit()
                logger.info(f"Locked and fetched task ID: {task.id}")
            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")






def fetch_and_lock_pending_task(self):
    """Fetches the next NOT_STARTED task and marks it IN_PROGRESS"""
    try:
        with DBSession() as db:
            tasks = db.session.query(Task).all()
            logger.info(f"Total tasks in DB: {len(tasks)}")

            for t in tasks:
                logger.info(f"TASK: id={t.id}, status={t.status}")

            task = (
                db.session.query(Task)
                .filter(Task.status == "NOT_STARTED")
                .order_by(Task.modified_ts.asc())
                .first()
            )
            if task:
                task.status = "IN_PROGRESS"
                task.modified_ts = get_current_datetime()
                db.session.commit()
                logger.info(f"Locked and fetched task ID: {task.id}")
            else:
                logger.info("No tasks found with status NOT_STARTED")
            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")







SELECT id, status FROM fts_tasks WHERE status = 'NOT_STARTED';



task = (
    db.session.query(Task)
    .filter(Task.status['value'].astext == "NOT_STARTED")
    .order_by(Task.modified_ts.asc())
    .first()
)



def fetch_and_lock_pending_task(self):
    """Fetches the next NOT_STARTED task and marks it IN_PROGRESS"""
    try:
        with DBSession() as db:
            task = (
                db.session.query(Task)
                .filter(Task.status == "NOT_STARTED")
                .order_by(Task.modified_ts.asc())
                .first()
            )
            if task:
                task.status = "IN_PROGRESS"
                task.modified_ts = get_current_datetime()
                db.session.commit()
                logger.info(f"Locked and fetched task ID: {task.id}")
            return task
    except Exception as e:
        logger.error(f"Error locking task: {e}", exc_info=True)
        raise Exception("Failed to fetch and lock pending task.")




import os
import time
import logging
from promptflow.client import PFClient, load_flow

from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.config.config_store import BlobConfig

from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao
from fts_mgmt.dao.data_curation_mgmt_dao import DataCurationServiceDAO
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao

from fts_mgmt.utils.util import flatten_config, get_strategy_storage_path
from fts_mgmt.utils.objects import TaskStatusEnum, RuntimeConfigModel

# Configure logging
configure_loggers()
logger = logging.getLogger("PromptflowDaemon")

# Use a safe default path (no ENV required)
PROMPTFLOW_ROOT_PATH = os.path.abspath("./promptflows")
os.makedirs(PROMPTFLOW_ROOT_PATH, exist_ok=True)

def merge_runtime_with_dynamic(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict):
    merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": RuntimeConfigModel(**merged_dynamic).dict()
    }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        task_dao = TaskMgmtDao()
        dcs_dao = DataCurationServiceDAO()
        ds_dao = DataMgmtServiceDao()

        dataset = ds_dao.get_dataset_info(task.config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(task.config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        strategy_path = get_strategy_storage_path(dcs.id, dcs.name)
        blob_location = setup_blob_resource().blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_path):
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = task.config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {result.output}")
        output_path = flat_inputs.get("dynamic_blob_path", "")

        task_dao.update_task(
            task_id=task.id,
            status="SUCCESS",
            output={"outputDirectory": output_path, "results": result.output}
        )

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status="FAILURE",
            output={"error": str(e)}
        )

def daemon_loop():
    logger.info("Promptflow Daemon Started")
    task_dao = TaskMgmtDao()

    while True:
        try:
            task = task_dao.fetch_and_lock_pending_task()
            if task:
                process_promptflow_task(task)
            else:
                logger.info("No pending tasks. Sleeping for 60 seconds.")
                time.sleep(60)
        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)

if __name__ == "__main__":
    daemon_loop()








import os
import time
import logging
from promptflow.client import PFClient, load_flow

from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.config.config_store import BlobConfig  # Assumes this doesn't need ENV

from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao
from fts_mgmt.dao.data_curation_mgmt_dao import DataCurationServiceDAO
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao

from fts_mgmt.utils.util import flatten_config, get_strategy_storage_path
from fts_mgmt.utils.objects import TaskStatusEnum, RuntimeConfigModel

# Configure logging
configure_loggers()
logger = logging.getLogger("DaemonExecutor")

# Hardcoded Promptflow folder
PROMPTFLOW_ROOT_PATH = "/tmp/promptflows"  # ✅ No os.getenv here

def merge_runtime_with_dynamic(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict):
    merged_dynamic = {**(dynamic_cfg or {}), **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": RuntimeConfigModel(**merged_dynamic).dict()
    }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")
        task_dao = TaskMgmtDao()
        dcs_dao = DataCurationServiceDAO()
        ds_dao = DataMgmtServiceDao()

        dataset = ds_dao.get_dataset_info(task.config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(task.config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        strategy_path = get_strategy_storage_path(dcs.id, dcs.name)
        blob_location = setup_blob_resource().blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )

        local_path = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_path):
            setup_blob_resource().copy_blob_dir_to_local(blob_location, local_path)

        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = task.config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        pf = PFClient()
        flow = load_flow(local_path)
        result = flow.invoke(inputs=flat_inputs)

        if not result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {result.output}")
        output_path = flat_inputs.get("dynamic_blob_path", "")

        task_dao.update_task(
            task_id=task.id,
            status=TaskStatusEnum.SUCCESS,
            output={"outputDirectory": output_path, "results": result.output}
        )

    except Exception as e:
        logger.error(f"Promptflow failed for task {task.id}: {str(e)}", exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status=TaskStatusEnum.FAILURE,
            output={"error": str(e)}
        )

def daemon_loop():
    logger.info("Daemon executor started...")
    task_dao = TaskMgmtDao()

    while True:
        try:
            task = task_dao.fetch_and_lock_pending_task()
            if task:
                process_promptflow_task(task)
            else:
                logger.info("No pending tasks. Sleeping 60s...")
                time.sleep(60)
        except Exception as e:
            logger.error(f"Daemon loop error: {e}", exc_info=True)
            time.sleep(60)

if __name__ == "__main__":
    daemon_loop()








import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../fts_mgmt/src')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../fts_commons/src')))


Yesterday, I resolved some dependency issues that were blocking progress. In addition, I’ve written part of the daemon processing code and will continue building on it today.




from fts_mgmt.db.models import TaskModel  # your ORM model
from fts_mgmt.utils.objects import TaskStatusEnum
from common_svc.db.base import DBConnection

class TaskMgmtDao:
    def fetch_and_lock_pending_task(self):
        session = DBConnection().start_session()
        try:
            task = (
                session.query(TaskModel)
                .filter(TaskModel.status == TaskStatusEnum.NOT_STARTED)
                .order_by(TaskModel.modified_ts.asc())
                .with_for_update(skip_locked=True)
                .first()
            )
            if task:
                task.status = TaskStatusEnum.IN_PROGRESS
                session.commit()
            return task
        except Exception as e:
            session.rollback()
            raise Exception(f"Failed to fetch and lock task: {e}")
        finally:
            session.close()

    def update_task(self, task_id, status, output):
        session = DBConnection().start_session()
        try:
            session.query(TaskModel).filter(TaskModel.id == task_id).update({
                TaskModel.status: status,
                TaskModel.output: output
            })
            session.commit()
        except Exception as e:
            session.rollback()
            raise Exception(f"Failed to update task: {e}")
        finally:
            session.close()








import os
import time
import json
import logging
from promptflow.client import PFClient, load_flow

from common_svc.logger.log_util import configure_loggers
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.config.config_store import BlobConfig
from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao
from fts_mgmt.dao.data_curation_mgmt_dao import DataCurationServiceDAO
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao
from fts_mgmt.utils.util import flatten_config, get_strategy_storage_path
from fts_mgmt.utils.objects import TaskStatusEnum, DCSConfig, RuntimeConfigModel

# Configure logging
configure_loggers()
logger = logging.getLogger("DaemonExecutor")

# Constants
PROMPTFLOW_ROOT_PATH = os.getenv("WORKFLOW_PATH", "/promptflow")

def merge_runtime_with_dynamic(static_cfg: dict, dynamic_cfg: dict, runtime_cfg: dict):
    # Merge runtime config into dynamic config
    merged_dynamic = {**dynamic_cfg, **(runtime_cfg or {})}
    return {
        "static": static_cfg,
        "dynamic": RuntimeConfigModel(**merged_dynamic).dict()
    }

def process_promptflow_task(task):
    try:
        logger.info(f"Processing task: {task.id}")

        # DAO objects
        task_dao = TaskMgmtDao()
        dcs_dao = DataCurationServiceDAO()
        ds_dao = DataMgmtServiceDao()

        dataset = ds_dao.get_dataset_info(task.config['dataset_id'])
        dcs = dcs_dao.get_data_curation_strategy_by_id(task.config['data_curation_strategy_id'])

        if not dataset or not dcs:
            raise Exception("Dataset or Data Curation Strategy not found")

        strategy_path = get_strategy_storage_path(strategy_id=dcs.id, strategy_name=dcs.name)
        absolute_blob_path = setup_blob_resource().blob_location(
            container_name=BlobConfig().BLOB_DOC_CONTAINER,
            absolute_path=strategy_path
        )

        # Download strategy locally
        local_dir = os.path.join(PROMPTFLOW_ROOT_PATH, str(task.id))
        if not os.path.exists(local_dir):
            setup_blob_resource().copy_blob_dir_to_local(
                storage_location=absolute_blob_path,
                target_local_path=local_dir
            )

        # Merge config
        static_cfg = dcs.config.get("static", {})
        dynamic_cfg = dcs.config.get("dynamic", {})
        runtime_cfg = task.config.get("data_curation_strategy_dynamic_config", {})

        if not runtime_cfg.get("blob_path"):
            runtime_cfg["blob_path"] = dataset.storage_path

        merged_config = merge_runtime_with_dynamic(static_cfg, dynamic_cfg, runtime_cfg)
        flat_inputs = flatten_config(merged_config)

        # Run Promptflow
        pf_client = PFClient()
        flow = load_flow(source=local_dir)
        flow_result = flow.invoke(inputs=flat_inputs)

        if not flow_result.output:
            raise Exception("Promptflow returned no output.")

        logger.info(f"Flow result: {flow_result.output}")
        output_blob_path = flat_inputs.get('dynamic_blob_path')

        task_dao.update_task(
            task_id=task.id,
            status=TaskStatusEnum.SUCCESS,
            output={"outputDirectory": output_blob_path, "results": flow_result.output}
        )

    except Exception as e:
        error_message = f"Promptflow failed for task {task.id}: {str(e)}"
        logger.error(error_message, exc_info=True)
        task_dao.update_task(
            task_id=task.id,
            status=TaskStatusEnum.FAILURE,
            output={"error": error_message}
        )

def daemon_loop():
    logger.info("Daemon executor started...")

    task_dao = TaskMgmtDao()

    while True:
        try:
            task = task_dao.get_pending_task()
            if task:
                process_promptflow_task(task)
            else:
                logger.info("No pending tasks. Sleeping 60s...")
                time.sleep(60)
        except Exception as e:
            logger.error(f"Daemon cycle error: {e}", exc_info=True)
            time.sleep(60)

# Entry Point
if __name__ == "__main__":
    daemon_loop()










import os
import time
import json
import logging
from promptflow.client import PFClient, load_flow

from common_svc.logger.log_util import configure_loggers
from common_svc.config.config_store import BlobConfig
from common_svc.storage.blob_client import setup_blob_resource
from common_svc.db.base import DBConnection

from fts_mgmt.dao.task_mgmt_dao import TaskMgmtDao
from fts_mgmt.dao.dataset_mgmt_dao import DataMgmtServiceDao
from fts_mgmt.dao.data_curation_mgmt_dao import DataCurationServiceDAO

from fts_mgmt.utils.objects import DCSConfig, RuntimeConfigModel, TaskStatusEnum
from fts_mgmt.utils.util import flatten_config, get_strategy_storage_path

# ------------------------ Setup ------------------------
configure_loggers()
logger = logging.getLogger(__name__)

blob_config = BlobConfig()
container_name = blob_config.BLOB_DOC_CONTAINER

task_dao = TaskMgmtDao()
dataset_dao = DataMgmtServiceDao()
dcs_dao = DataCurationServiceDAO()

# ------------------------ Daemon Logic ------------------------

def fetch_queued_task():
    """Fetch one queued task from DB"""
    session = DBConnection().start_session()
    try:
        task = session.query(task_dao.model).filter(task_dao.model.status == TaskStatusEnum.NOT_STARTED)\
                      .order_by(task_dao.model.modified_ts.desc()).first()
        return task
    except Exception as e:
        logger.error(f"Error fetching queued task: {e}", exc_info=True)
    finally:
        session.close()

def execute_promptflow(task):
    try:
        task_id = task.id
        logger.info(f"Processing task: {task_id}")

        # 1. Extract required fields from task config
        config = task.config
        project_id = config.get("project_id")
        dataset_id = config.get("dataset_id")
        dcs_id = config.get("data_curation_strategy_id")
        runtime_config = config.get("data_curation_strategy_dynamic_config")

        # 2. Dataset path
        dataset = dataset_dao.get_dataset_info(dataset_id)
        dataset_path = dataset.storage_path
        runtime_config["blob_path"] = runtime_config.get("blob_path") or dataset_path

        # 3. Load Data Curation Strategy (DCS)
        dcs = dcs_dao.get_data_curation_strategy_by_id(dcs_id)
        strategy_name = dcs.name
        dcs_config = DCSConfig(
            static=dcs.config.get("static"),
            dynamic=dcs.config.get("dynamic")
        )

        # 4. Merge dynamic config
        updated_dynamic = {**(dcs_config.dynamic.dict() if dcs_config.dynamic else {}), **runtime_config}
        dcs_config.dynamic = RuntimeConfigModel(**updated_dynamic)

        # 5. Flatten config for Promptflow
        flattened_config = flatten_config(dcs_config.dict())

        # 6. Download strategy ZIP from Blob
        strategy_path = get_strategy_storage_path(strategy_id=dcs_id, strategy_name=strategy_name)
        blob_abs_path = setup_blob_resource().blob_location(
            container_name=container_name,
            absolute_path=strategy_path
        )

        flow_local_path = f"/promptflow/{task_id}"
        setup_blob_resource().copy_blob_dir_to_local(
            storage_location=blob_abs_path,
            target_local_path=flow_local_path
        )

        # 7. Run Promptflow
        pf_client = PFClient()
        pf_flow = load_flow(source=flow_local_path)
        result = pf_flow.invoke(inputs=flattened_config)

        if result and result.output:
            output_path = flattened_config.get("dynamic_blob_path", "N/A")
            task_dao.update_task(task_id=task_id, status=TaskStatusEnum.SUCCESS, output={"outputDirectory": output_path})
            logger.info(f"Task {task_id} completed successfully.")
        else:
            raise Exception("Promptflow returned empty result.")

    except Exception as e:
        error_msg = f"Promptflow execution failed for Task ID {task_id}: {e}"
        logger.error(error_msg, exc_info=True)
        task_dao.update_task(task_id=task_id, status=TaskStatusEnum.FAILURE, output={"error": str(e)})

# ------------------------ Main Daemon Runner ------------------------

def daemon():
    task = fetch_queued_task()
    if not task:
        logger.info("No queued tasks found. Sleeping for 60 seconds...")
        time.sleep(60)
        return

    execute_promptflow(task)

if __name__ == "__main__":
    logger.info("Promptflow Daemon Started.")
    while True:
        daemon()








import os
import json
import requests
import pandas as pd
import copy

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
MGMT_BASE_URL = "https://rai-uat.statestr.com"
CONV_BASE_URL = "https://rai-uat.statestr.com"
APP_ID = "62603385-7504-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <YOUR_VALID_TOKEN_HERE>"

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}
HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

STRATEGY_PATH = "injection_strategies.json"
FILE_PATHS = [
    r"c:\\Users\\P872643\\Downloads\\rebal.pdf",
    r"c:\\Users\\P872643\\Downloads\\Centric Wealth Quarterly Distribution SOP (Old CUP tool).pdf"
]

# === STEP 1: CREATE DATA GROUP ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "Data2",
        "description": "Created via Python script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    return response.json().get("dsId"), payload["name"]

# === STEP 2: LOAD STRATEGIES ===
def load_all_strategies(filepath):
    with open(filepath, "r") as f:
        data = json.load(f)
        return data["items"]

# === STEP 3: UPDATE INGESTION STRATEGY ===
def update_ingestion_strategy(datagroup_id, strategy, ingestion_name):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"
    payload = {
        "name": ingestion_name,
        "description": strategy.get("description", "Updated ingestion strategy"),
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": strategy.get("md", {}),
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()

# === STEP 4: UPLOAD FILES ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    user_metadata = {"uploadedBy": "p872643"}
    data = {"userMetadata": json.dumps(user_metadata)}
    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    response.raise_for_status()
    for f in files:
        f[1].close()
    json_resp = response.json()
    file_ids = []
    if isinstance(json_resp, list):
        file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
    elif isinstance(json_resp, dict):
        if "fileId" in json_resp:
            file_ids = [json_resp["fileId"]]
        elif "fileIds" in json_resp:
            file_ids = json_resp["fileIds"]
    return file_ids

# === STEP 5: INGEST FILES ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {"documentIds": file_ids}
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()

# === STEP 6: LIST FILES ===
def list_uploaded_files(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/entities"
    response = requests.get(url, headers=HEADERS_JSON, verify=False)
    response.raise_for_status()
    return response.json()

# === STEP 7: EVALUATE RETRIEVER STRATEGIES ===
def evaluate_retriever_strategies(strategy_name, strategy_config, excel_path, ingestion_strategy_name):
    df_questions = pd.read_excel(excel_path)
    questions = df_questions["Question"].dropna().tolist()
    all_results = []

    update_url = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
    conv_url = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

    payload = copy.deepcopy({
        "retrieverConfig": strategy_config,
        "metadata": {},
        "category": "",
        "description": "",
        "prompt": ".",
        "title": strategy_name,
        "icon": "",
        "vectorStore": {},
        "raiclient": {
            "eamClientId": "",
            "raiClientId": "",
            "eamClientSecret": ""
        },
        "entitlements": {},
        "embedding": {
            "model": "",
            "params": {}
        },
        "generative": {
            "model": "",
            "params": {},
            "n": 1,
            "temperature": 0.7
        },
        "multimodel": {
            "model": "",
            "params": {}
        }
    })

    response = requests.post(update_url, headers=HEADERS_JSON, json=payload, verify=False)
    print(f"Updated retriever config: {strategy_name} => {response.status_code}")

    for question in questions:
        try:
            conv_payload = {"userQuery": question}
            conv_response = requests.post(conv_url, headers=HEADERS_JSON, json=conv_payload, verify=False)
            conv_response.raise_for_status()
            response_json = conv_response.json()
            bot_response = response_json.get("botResponse", "")
            full_response = json.dumps(response_json)
        except Exception as e:
            bot_response = f"Error: {str(e)}"
            full_response = bot_response

        all_results.append({
            "Ingestion Strategy": ingestion_strategy_name,
            "Retriever Strategy": strategy_name,
            "Retriever Config": json.dumps(strategy_config, indent=2),
            "Question": question,
            "Bot Response": bot_response,
            "Full Raw Response": full_response
        })

    return all_results

# === MAIN EXECUTION ===
if __name__ == "__main__":
    try:
        print("\n=== Starting Full Upload & Ingest Pipeline ===")
        datagroup_id, ingestion_name = create_data_group()
        file_ids = upload_files(datagroup_id, FILE_PATHS)
        print(f"Uploaded files: {file_ids}")
        ingest_documents(datagroup_id, file_ids)
        print("Ingestion triggered successfully.")

        all_strategies = load_all_strategies(STRATEGY_PATH)
        all_results = []

        for strategy in all_strategies:
            strategy_name = strategy["name"]
            strategy_md = strategy["md"]
            if isinstance(strategy_md, str):
                strategy_md = json.loads(strategy_md)
            update_ingestion_strategy(datagroup_id, strategy, strategy_name)
            print(f"Ingestion strategy updated: {strategy_name}")
            results = evaluate_retriever_strategies(
                strategy_name=strategy_name,
                strategy_config=strategy,
                excel_path=r"\\mfgdcu02\p872643\Desktop\ssgenai_testing\01. Asset&Cash Transfer Check 50P V7.8 questions.xlsx",
                ingestion_strategy_name=strategy_name
            )
            all_results.extend(results)

        df_output = pd.DataFrame(all_results, columns=[
            "Ingestion Strategy", "Retriever Strategy", "Retriever Config", "Question", "Bot Response", "Full Raw Response"
        ])
        df_output.to_excel("Naive_retriever_output_with_strategy.xlsx", index=False)
        print("Saved results to Naive_retriever_output_with_strategy.xlsx")

    except Exception as e:
        print("Pipeline failed:", str(e))










Subject: Ongoing VDI Performance Issues Impacting Productivity

Dear [IT Support/Manager Name],

We are consistently facing several issues with the VDI environment, which are significantly impacting our day-to-day productivity. Below are the key concerns:

Slow Login Times: Logging into the VDI takes an unusually long time, often exceeding 5-10 minutes.

System Performance Lag: Once logged in, the VDI system runs extremely slow, affecting all operations.

Excel File Opening Delay: Opening even moderately sized Excel files takes around 3 to 4 minutes.

Audio Quality Issues: The audio is frequently distorted or delayed, especially during meetings or calls.

Insufficient Memory/Resources: The system often becomes unresponsive due to low memory allocation, leading to frequent lags or freezes.

Frequent Disruptions: We face random slowdowns and unresponsiveness throughout the day, leading to frustration and loss of productive time.

Application Freezes: Basic applications like Outlook, Teams, or Chrome frequently hang or crash.

Screen Refresh Delays: There is a noticeable lag in screen updates while switching between applications.

Copy-Paste Issues: Basic functions like copy-paste between local and VDI systems often fail or are delayed.

High Latency: Even with a stable internet connection, the response time is very poor.

We kindly request your assistance in analyzing and resolving these persistent VDI issues. Upgrading resource allocation or exploring alternative solutions would greatly help in improving performance.

Looking forward to your support on this matter.

Best regards,
[Your Name]
[Your Team / Department]
[Your Contact Information]

Let me know if you want to convert this into an email format or tailor it for escalation to a specific team or manager.







Do you like this personality?







Tools



ChatGPT can make mistakes. Check important info. See Cookie Preferences.



import os
import json
import requests
import pandas as pd
import copy

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
MGMT_BASE_URL = "https://rai-uat.statestr.com"
CONV_BASE_URL = "https://rai-uat.statestr.com"
APP_ID = "62603385-7504-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <YOUR_VALID_TOKEN_HERE>"

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}
HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

STRATEGY_PATH = "injection_strategies.json"
FILE_PATHS = [
    r"c:\\Users\\P872643\\Downloads\\rebal.pdf",
    r"c:\\Users\\P872643\\Downloads\\Centric Wealth Quarterly Distribution SOP (Old CUP tool).pdf"
]

# === STEP 1: CREATE DATA GROUP ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "Data2",
        "description": "Created via Python script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    return response.json().get("dsId"), payload["name"]

# === STEP 2: LOAD STRATEGY ===
def load_first_strategy(filepath):
    with open(filepath, "r") as f:
        data = json.load(f)
        strategy = data["items"][0]
        md_parsed = strategy["md"]
        if isinstance(md_parsed, str):
            md_parsed = json.loads(md_parsed)
        return strategy["name"], strategy.get("description", ""), md_parsed

# === STEP 3: UPDATE INGESTION STRATEGY ===
def update_ingestion_strategy(datagroup_id, strategy, ingestion_name):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"
    payload = {
        "name": ingestion_name,
        "description": "Updated ingestion strategy",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": strategy,
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()

# === STEP 4: UPLOAD FILES ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    user_metadata = {"uploadedBy": "p872643"}
    data = {"userMetadata": json.dumps(user_metadata)}
    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    response.raise_for_status()
    for f in files:
        f[1].close()
    json_resp = response.json()
    file_ids = []
    if isinstance(json_resp, list):
        file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
    elif isinstance(json_resp, dict):
        if "fileId" in json_resp:
            file_ids = [json_resp["fileId"]]
        elif "fileIds" in json_resp:
            file_ids = json_resp["fileIds"]
    return file_ids

# === STEP 5: INGEST FILES ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {"documentIds": file_ids}
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()

# === STEP 6: LIST FILES ===
def list_uploaded_files(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/entities"
    response = requests.get(url, headers=HEADERS_JSON, verify=False)
    response.raise_for_status()
    return response.json()

# === STEP 7: EVALUATE RETRIEVER STRATEGIES ===
def evaluate_retriever_strategies(strategy_file, excel_path, ingestion_strategy_name):
    with open(strategy_file, "r") as f:
        retriever_strategies = json.load(f)

    df_questions = pd.read_excel(excel_path)
    questions = df_questions["Question"].dropna().tolist()
    all_results = []

    for strategy in retriever_strategies["items"]:
        strategy_name = strategy["name"]
        strategy_config = json.dumps(strategy, indent=2)

        update_url = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
        conv_url = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

        payload = copy.deepcopy({
            "retrieverConfig": strategy,
            "metadata": {},
            "category": "",
            "description": "",
            "prompt": ".",
            "title": strategy_name,
            "icon": "",
            "vectorStore": {},
            "raiclient": {
                "eamClientId": "",
                "raiClientId": "",
                "eamClientSecret": ""
            },
            "entitlements": {},
            "embedding": {
                "model": "",
                "params": {}
            },
            "generative": {
                "model": "",
                "params": {},
                "n": 1,
                "temperature": 0.7
            },
            "multimodel": {
                "model": "",
                "params": {}
            }
        })

        response = requests.post(update_url, headers=HEADERS_JSON, json=payload, verify=False)
        print(f"Updated retriever config: {strategy_name} => {response.status_code}")

        for question in questions:
            try:
                conv_payload = {"userQuery": question}
                conv_response = requests.post(conv_url, headers=HEADERS_JSON, json=conv_payload, verify=False)
                conv_response.raise_for_status()
                response_json = conv_response.json()
                bot_response = response_json.get("botResponse", "")
                full_response = json.dumps(response_json)
            except Exception as e:
                bot_response = f"Error: {str(e)}"
                full_response = bot_response

            all_results.append({
                "Ingestion Strategy": ingestion_strategy_name,
                "Retriever Strategy": strategy_name,
                "Retriever Config": strategy_config,
                "Question": question,
                "Bot Response": bot_response,
                "Full Raw Response": full_response
            })

    df_output = pd.DataFrame(all_results, columns=[
        "Ingestion Strategy", "Retriever Strategy", "Retriever Config", "Question", "Bot Response", "Full Raw Response"
    ])
    df_output.to_excel("Naive_retriever_output_with_strategy.xlsx", index=False)
    print("Saved results to Naive_retriever_output_with_strategy.xlsx")

# === MAIN EXECUTION ===
if __name__ == "__main__":
    try:
        print("\n=== Starting Full Upload & Ingest Pipeline ===")
        datagroup_id, ingestion_name = create_data_group()
        file_ids = upload_files(datagroup_id, FILE_PATHS)
        print(f"Uploaded files: {file_ids}")
        ingest_documents(datagroup_id, file_ids)
        print("Ingestion triggered successfully.")
        _, _, strategy = load_first_strategy(STRATEGY_PATH)
        update_ingestion_strategy(datagroup_id, strategy, ingestion_name)
        print("Ingestion strategy updated successfully.")
        uploaded_files = list_uploaded_files(datagroup_id)
        print("Uploaded Files:", uploaded_files)

        # === Execute Retriever Strategy Evaluation ===
        evaluate_retriever_strategies(
            strategy_file=r"\\mfgdcu82\p872643\Desktop\Rsults\retriever_strategies_NaiveRetriever.json",
            excel_path=r"\\mfgdcu02\p872643\Desktop\ssgenai_testing\01. Asset&Cash Transfer Check 50P V7.8 questions.xlsx",
            ingestion_strategy_name=ingestion_name
        )
    except Exception as e:
        print("Pipeline failed:", str(e))



def update_ingestion_strategy(datagroup_id, strategy):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"

    payload = {
        "name": "Data2",  # Same as in datagroup creation or updated value
        "description": "Updated ingestion strategy",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": strategy,
        "config": {}
    }

    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("✔ Strategy Updated:", response.status_code, response.text)
    response.raise_for_status()





import os
import json
import requests

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-7504-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer YOUR_REAL_TOKEN_HERE"  # Replace with valid token

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}
HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

STRATEGY_PATH = "injection_strategies.json"
FILE_PATHS = [
    r"c:\Users\P872643\Downloads\rebal.pdf",
    r"c:\Users\P872643\Downloads\Centric Wealth Quarterly Distribution SOP (Old CUP tool).pdf"
]

# === STEP 1: CREATE DATA GROUP ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "Data2",
        "description": "Created via Python script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Create Data Group:", response.status_code, response.text)
    response.raise_for_status()
    ds_id = response.json().get("dsId")
    print(f"✔ Data group created: {ds_id}")
    return ds_id

# === STEP 2: LOAD FIRST INGESTION STRATEGY ===
def load_first_strategy(filepath):
    with open(filepath, "r") as f:
        strategies = json.load(f)["items"]
    md_raw = strategies[0]["md"]
    md_parsed = json.loads(md_raw)
    return strategies[0]["name"], strategies[0].get("description", ""), md_parsed

# === STEP 3: UPDATE INGESTION STRATEGY FOR DATA GROUP ===
def update_ingestion_strategy(datagroup_id, strategy):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"
    payload = {
        "ingestionStrategy": strategy
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("✔ Strategy Updated:", response.status_code, response.text)
    response.raise_for_status()

# === STEP 4: UPLOAD FILES TO DATA GROUP ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print(f"Uploading to: {url}")
    files = []

    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf" if filename.endswith(".pdf") else "application/octet-stream"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    user_metadata = {"uploadedBy": "p872643"}
    data = {
        "userMetadata": json.dumps(user_metadata)
    }

    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    print("Upload Response:", response.status_code, response.text)
    response.raise_for_status()

    # Close all file handles
    for f in files:
        f[1].close()

    # Parse response to extract file IDs
    json_resp = response.json()
    file_ids = []

    if isinstance(json_resp, list):
        file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
    elif isinstance(json_resp, dict):
        if "fileId" in json_resp:
            file_ids = [json_resp["fileId"]]
        elif "fileIds" in json_resp:
            file_ids = json_resp["fileIds"]

    if not file_ids:
        raise ValueError(f"No fileId(s) returned. Got: {json_resp}")

    print(f"✔ Uploaded {len(file_ids)} file(s): {file_ids}")
    return file_ids

# === STEP 5: INGEST DOCUMENTS ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    if isinstance(file_ids, str):
        file_ids = [file_ids]
    payload = {
        "documentIds": file_ids
    }
    print("Ingesting with payload:", json.dumps(payload, indent=2))
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Ingest Response:", response.status_code)
    print("Body:", response.text)
    response.raise_for_status()
    print("✔ Ingestion triggered successfully.")

# === STEP 6: LIST UPLOADED FILES ===
def list_uploaded_files(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/entities"
    response = requests.get(url, headers=HEADERS_JSON, verify=False)
    print("List Files Response:", response.status_code)
    response.raise_for_status()

    result = response.json()
    for i, item in enumerate(result.get("items", []), start=1):
        print(f"{i}. File ID: {item.get('documentId')} | Name: {item.get('name')} | Status: {item.get('status')}")
    return result

# === MAIN EXECUTION ===
if __name__ == "__main__":
    # Step 1
    dg_id = create_data_group()

    # Step 2
    _, _, strategy = load_first_strategy(STRATEGY_PATH)

    # Step 3
    update_ingestion_strategy(dg_id, strategy)

    # Step 4
    file_ids = upload_files(dg_id, FILE_PATHS)

    # Step 5
    ingest_documents(dg_id, file_ids)

    # Step 6
    list_uploaded_files(dg_id)









import requests
import json
import urllib3

# === Suppress SSL Warnings ===
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# === CONFIGURATION ===
BASE_URL = "https://your-api-host.com/agi/igs/api/v1"
APP_ID = "your-app-id"                   # Replace with actual App ID
ACCESS_TOKEN = "your-access-token"       # Replace with your real token
STRATEGY_FILE = "strategies.json"        # Your local strategy list

# === HEADERS ===
HEADERS = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Content-Type": "application/json"
}


# === STEP 1: CREATE DATAGROUP ===
def create_datagroup():
    url = f"{BASE_URL}/apps/{APP_ID}/dgs"
    payload = {
        "name": "AutoCreated-Datagroup",
        "description": "Created programmatically",
        "dsType": "BLOB",
        "dataClassification": "general",
        "metadata": {},
        "config": {}
    }
    res = requests.post(url, headers=HEADERS, json=payload, verify=False)
    res.raise_for_status()
    datagroup_id = res.json().get("id")
    print("✅ Created Datagroup:", datagroup_id)
    return datagroup_id


# === STEP 2: LOAD FIRST STRATEGY ===
def load_first_strategy(filepath):
    with open(filepath, "r") as f:
        strategies = json.load(f)["items"]
        md_raw = strategies[0]["md"]
        md_parsed = json.loads(md_raw)
        return strategies[0]["name"], strategies[0].get("description", ""), md_parsed


# === STEP 3: ASSIGN STRATEGY TO DATAGROUP ===
def update_datagroup(datagroup_id, strategy_name, strategy_desc, strategy_config):
    url = f"{BASE_URL}/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"
    payload = {
        "name": strategy_name,
        "description": strategy_desc,
        "dsType": "BLOB",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": [
            {
                "strategyType": "RAI_PIPELINE",
                "config": strategy_config
            }
        ],
        "config": {}
    }
    res = requests.post(url, headers=HEADERS, json=payload, verify=False)
    print("✅ Strategy assigned. Status:", res.status_code)
    try:
        print("Response:", json.dumps(res.json(), indent=2))
    except:
        print("Raw Response:", res.text)


# === MAIN RUN ===
if __name__ == "__main__":
    datagroup_id = create_datagroup()
    strategy_name, strategy_desc, strategy_config = load_first_strategy(STRATEGY_FILE)
    update_datagroup(datagroup_id, strategy_name, strategy_desc, strategy_config)






import requests
import json

# === CONFIGURATION ===
BASE_URL = "https://your-api-host.com/agi/igs/api/v1"
APP_ID = "your-app-id"                    # Replace with your actual App ID
DATAGROUP_ID = "your-datagroup-id"        # Replace with your actual Datagroup ID
ACCESS_TOKEN = "your-access-token"        # Replace with your Bearer token
STRATEGY_JSON_PATH = "strategies.json"    # Path to your local JSON file with all 14 strategies

# === HEADERS ===
HEADERS = {
    "Authorization": f"Bearer {ACCESS_TOKEN}",
    "Content-Type": "application/json"
}

# === LOAD STRATEGIES FROM JSON FILE ===
with open(STRATEGY_JSON_PATH, "r") as f:
    data = json.load(f)

# If file structure is {"items": [ ... ]}
strategies = data["items"]
first_strategy_md = json.loads(strategies[0]["md"])  # Load the `md` JSON string from the first item

# === PREPARE API PAYLOAD ===
payload = {
    "name": strategies[0]["name"],  # Using name from strategy JSON
    "description": strategies[0].get("description", "Updated ingestion strategy"),
    "dsType": "BLOB",
    "dataClassification": "general",
    "metadata": {},
    "ingestionStrategy": [
        {
            "strategyType": "RAI_PIPELINE",
            "config": first_strategy_md
        }
    ],
    "config": {}
}

# === SEND UPDATE REQUEST ===
url = f"{BASE_URL}/apps/{APP_ID}/dgs/{DATAGROUP_ID}?action=modify"
response = requests.post(url, headers=HEADERS, json=payload)

# === PRINT RESULT ===
print("Status Code:", response.status_code)
try:
    print("Response:", json.dumps(response.json(), indent=2))
except Exception:
    print("Raw Response:", response.text)





import os
import json
import requests

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <PASTE_YOUR_VALID_MFA_TOKEN_HERE>"
FILE_PATHS = [
    r"c:\\Users\\P872643\\Downloads\\rebal.pdf",
    r"c:\\Users\\P872643\\Downloads\\Centric Wealth Quarterly Distribution SOP (Old CUP tool).pdf"
]

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}

HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

# === LOAD STRATEGY FROM JSON FILE ===
with open("strategies.json", "r") as f:
    STRATEGY_JSON = json.load(f)

FIRST_STRATEGY = STRATEGY_JSON["items"][0]  # Use the first strategy only

# === STEP 1: CREATE DATA GROUP ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": f"DataGroup_Auto",
        "description": f"Created via pipeline",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("[✔] Data Group Created:", response.status_code, response.text)
    response.raise_for_status()
    return response.json()["dsId"]

# === STEP 1.5: UPDATE DATA GROUP STRATEGY ===
def update_ingestion_strategy(datagroup_id, strategy):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}?action=modify"
    payload = {
        "ingestionStrategy": strategy
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("[✔] Strategy Updated:", response.status_code, response.text)
    response.raise_for_status()

# === STEP 2: UPLOAD FILES ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print(f"[→] Uploading to: {url}")
    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf" if filename.endswith(".pdf") else "application/octet-stream"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    user_metadata = {"uploadedBy": "p872643"}
    data = {"userMetadata": json.dumps(user_metadata)}

    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    print("[✔] Upload Response:", response.status_code, response.text)
    response.raise_for_status()

    for _, f in files:
        f[1].close()

    json_resp = response.json()
    file_ids = []
    if isinstance(json_resp, list):
        file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
    elif isinstance(json_resp, dict):
        if "fileId" in json_resp:
            file_ids = [json_resp["fileId"]]
        elif "fileIds" in json_resp:
            file_ids = json_resp["fileIds"]

    if not file_ids:
        raise ValueError(f"No fileId(s) returned. Got: {json_resp}")

    print(f"[✔] Uploaded {len(file_ids)} file(s): {file_ids}")
    return file_ids

# === STEP 3: INGEST DOCUMENTS ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {"documentIds": file_ids}
    print("[→] Ingesting with payload:", json.dumps(payload, indent=2))
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("[✔] Ingest Response:", response.status_code, response.text)
    response.raise_for_status()
    print("[✔] Ingestion triggered successfully.")

# === MAIN DRIVER ===
if __name__ == "__main__":
    try:
        print("=== Starting Ingestion Pipeline with Strategy Update ===")
        datagroup_id = create_data_group()
        update_ingestion_strategy(datagroup_id, FIRST_STRATEGY)
        file_ids = upload_files(datagroup_id, FILE_PATHS)
        ingest_documents(datagroup_id, file_ids)
        print("\n✅ Strategy applied and ingestion complete.")
    except Exception as e:
        print("❌ Pipeline Failed:", str(e))







import os
import json
import requests

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <PASTE_YOUR_VALID_MFA_TOKEN_HERE>"
FILE_PATHS = [
    r"c:\\Users\\P872643\\Downloads\\rebal.pdf",
    r"c:\\Users\\P872643\\Downloads\\Centric Wealth Quarterly Distribution SOP (Old CUP tool).pdf"
]

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}

HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

# === STEP 1: CREATE DATA GROUP ===
def create_data_group(strategy):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": f"DataGroup_{strategy['strategyType']}",
        "description": f"Created for {strategy['strategyType']}",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": strategy,
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("[✔] Data Group Created:", response.status_code, response.text)
    response.raise_for_status()
    return response.json()["dsId"]

# === STEP 2: UPLOAD FILES ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print(f"[→] Uploading to: {url}")
    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf" if filename.endswith(".pdf") else "application/octet-stream"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    user_metadata = {"uploadedBy": "p872643"}
    data = {"userMetadata": json.dumps(user_metadata)}

    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    print("[✔] Upload Response:", response.status_code, response.text)
    response.raise_for_status()

    for _, f in files:
        f[1].close()

    json_resp = response.json()
    file_ids = []
    if isinstance(json_resp, list):
        file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
    elif isinstance(json_resp, dict):
        if "fileId" in json_resp:
            file_ids = [json_resp["fileId"]]
        elif "fileIds" in json_resp:
            file_ids = json_resp["fileIds"]

    if not file_ids:
        raise ValueError(f"No fileId(s) returned. Got: {json_resp}")

    print(f"[✔] Uploaded {len(file_ids)} file(s): {file_ids}")
    return file_ids

# === STEP 3: INGEST DOCUMENTS ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {"documentIds": file_ids}
    print("[→] Ingesting with payload:", json.dumps(payload, indent=2))
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("[✔] Ingest Response:", response.status_code, response.text)
    response.raise_for_status()
    print("[✔] Ingestion triggered successfully.")

# === MAIN DRIVER ===
if __name__ == "__main__":
    try:
        with open("strategies.json", "r") as f:
            strategies = json.load(f)

        print("=== Starting Strategy-Based Ingestion Pipeline ===")

        for strategy in strategies:
            print(f"\n[★] Strategy: {strategy['strategyType']}")
            datagroup_id = create_data_group(strategy)
            file_ids = upload_files(datagroup_id, FILE_PATHS)
            ingest_documents(datagroup_id, file_ids)

        print("\n✅ All strategies executed successfully.")

    except Exception as e:
        print("❌ Pipeline Failed:", str(e))






Latestr.com/ag/ig/swaggert/

Data Ingestion Services

POST

/ngi/igs/api/v1/apps/app/s

THE

POST

GET

POST

POST

POST

AF/AE/api/v2/apps/app/gs/detagroupingsta

GET

GET

/ag1/sgs/api/v2/apps/dgs/(remind)/config

/ag1/3gs/mp3/3/apps/(καρ20)/4gs) (dategroup)/attachments/

call






Latestr.com/ag/ig/swaggert/

Data Ingestion Services

POST

/ngi/igs/api/v1/apps/app/s

THE

POST

GET

POST

POST

POST

AF/AE/api/v2/apps/app/gs/detagroupingsta

GET

GET

/ag1/sgs/api/v2/apps/dgs/(remind)/config

/ag1/3gs/mp3/3/apps/(καρ20)/4gs) (dategroup)/attachments/

call




import requests
import json

BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer YOUR_VALID_TOKEN_HERE"

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}

def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"

    # Ensure it's a list of strings
    if isinstance(file_ids, str):
        file_ids = [file_ids]

    # Final Payload
    payload = {
        "documentIds": file_ids
    }

    print("📤 Ingesting with payload:")
    print(json.dumps(payload, indent=2))

    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("📥 Ingest Response:", response.status_code)
    print("📄 Body:", response.text)

    response.raise_for_status()
    print("✅ Ingestion triggered successfully.")







def list_uploaded_files(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/entities"
    response = requests.get(url, headers=HEADERS_JSON, verify=False)
    response.raise_for_status()
    data = response.json()
    
    if isinstance(data, list):
        return data
    elif isinstance(data, dict) and "entities" in data:
        return data["entities"]
    else:
        print("❌ Unexpected format in entity response:", data)
        return []






AttributeError

Traceback (most recent call last)

Cell In[63], line 126

123 ingest_documents (datagroup_id, to_ingest)

125 # Run it

--> 126 run_pipeline()

Cell In[63], line 118

115 uploaded_file_ids = upload_files(datagroup_id, FILE PA

117 all_files = list_uploaded_files(datagroup_id)

--> 118 to_ingest = [f["fileId"] for f in all_files if f.get("ingestionStatus") == "Uploaded"]

120 if not to_ingest:

121 print("[i] No un-ingested files found.")

Cell In[63], line 118

115 uploaded_file_ids = upload_files(datagro_id, FILE_PATHS)

117 all_files = list_uploaded_files(datagroup_id)

--> 118 to_ingest = [f["fileId"] for f in all_files if f.get("ingestionStatus") == "Uploaded"]

120 if not to_ingest:

121 print("[i] No un-ingested files found.")

AttributeError:

'str' object has no attribute 'get'

Spaces: 4





import os
import requests
import json
from urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# === Configuration ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-7504-4284-98c4-837a249526d7"  # Replace with your actual App ID
AUTH_TOKEN = "Bearer <PASTE_YOUR_VALID_MFA_TOKEN_HERE>"

HEADERS_JSON = {
    "Content-Type": "application/json",
    "Authorization": AUTH_TOKEN
}

HEADERS_MULTIPART = {
    "Authorization": AUTH_TOKEN
}

FILE_PATHS = [  # You can add more files here
    r"C:\Users\P872643\Downloads\example1.pdf",
    r"C:\Users\P872643\Downloads\example2.pdf"
]

# === Step 1: Create Data Group ===
def create_data_group(name="AutoDataGroup"):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": name,
        "description": "Created via Python script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    datagroup_id = response.json().get("dsId")
    print(f"[✔] Data group created: {datagroup_id}")
    return datagroup_id

# === Step 2: Upload Files ===
def upload_files(datagroup_id, file_paths):
    file_ids = []
    for file_path in file_paths:
        url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
        with open(file_path, "rb") as f:
            files = {
                "files": (os.path.basename(file_path), f, "application/pdf"),
                "userMetadata": (None, "0")
            }
            print(f"Uploading to: {url}")
            response = requests.post(url, headers=HEADERS_MULTIPART, files=files, verify=False)
            print("Upload Response:", response.status_code, response.text)
            response.raise_for_status()
            file_id = response.json().get("fileId")
            if file_id:
                file_ids.append(file_id)
    return file_ids

# === Step 3: List Uploaded Files ===
def list_uploaded_files(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/entities"
    response = requests.get(url, headers=HEADERS_JSON, verify=False)
    response.raise_for_status()
    return response.json()

# === Step 4: Ingest Selected Files ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {
        "documentIds": file_ids
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Ingest Response:", response.status_code, response.text)
    response.raise_for_status()
    print(f"[✔] Ingestion triggered successfully.")

# === Main Pipeline ===
def run_pipeline():
    print("=== Starting Full Upload + Ingest Pipeline ===")
    datagroup_id = create_data_group()
    uploaded_file_ids = upload_files(datagroup_id, FILE_PATHS)
    
    all_files = list_uploaded_files(datagroup_id)
    to_ingest = [f["fileId"] for f in all_files if f.get("ingestionStatus") == "Uploaded"]

    if not to_ingest:
        print("[ℹ] No un-ingested files found.")
    else:
        ingest_documents(datagroup_id, to_ingest)

# Run it
run_pipeline()




json_resp = response.json()
file_ids = []

# If the API returned a list of dicts (one per file)
if isinstance(json_resp, list):
    file_ids = [item["fileId"] for item in json_resp if "fileId" in item]
# If it's a dict with fileIds array
elif isinstance(json_resp, dict):
    if "fileId" in json_resp:
        file_ids = [json_resp["fileId"]]
    elif "fileIds" in json_resp:
        file_ids = json_resp["fileIds"]

if not file_ids:
    raise ValueError(f"No fileId(s) returned. Got: {json_resp}")

print(f"[✓] Uploaded {len(file_ids)} file(s): {file_ids}")






import requests
import os
import json

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"  # ✅ Correct App ID
AUTH_TOKEN = "Bearer eyJ...your_full_MFA_token_here"  # ✅ Valid MFA token

HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}

HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

FILE_PATHS = [
    r"C:\Users\P872643\Downloads\rebal.pdf",
    r"C:\Users\P872643\Downloads\NPS Monthly Fee Accre_GD SOP V3 0 1.pdf"
]

# === STEP 1: CREATE DATA GROUP ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "DataGroup_Upload",
        "description": "Created via Python script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Create Data Group:", response.status_code, response.text)
    response.raise_for_status()
    dsId = response.json().get("dsId")
    print(f"[✓] Data group created: {dsId}")
    return dsId

# === STEP 2: UPLOAD FILES ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print(f"Uploading to: {url}")

    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf" if filename.endswith(".pdf") else "application/octet-stream"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    # userMetadata must be JSON string
    user_metadata = {"uploadedBy": "p872643"}
    data = {
        "userMetadata": json.dumps(user_metadata)
    }

    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    print("Upload Response:", response.status_code, response.text)
    response.raise_for_status()

    # Close file handles
    for _, f in files:
        f[1].close()

    json_resp = response.json()
    file_ids = []
    if "fileId" in json_resp:
        file_ids.append(json_resp["fileId"])
    elif "fileIds" in json_resp:
        file_ids.extend(json_resp["fileIds"])
    else:
        raise ValueError(f"No fileId(s) returned. Got: {json_resp}")

    print(f"[✓] Uploaded {len(file_ids)} file(s): {file_ids}")
    return file_ids

# === STEP 3: INGEST DOCUMENTS ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {
        "documentIds": file_ids
    }

    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Ingest Response:", response.status_code, response.text)
    response.raise_for_status()
    print(f"[✓] Ingestion triggered: {response.json().get('message')}")

# === MAIN EXECUTION ===
if __name__ == "__main__":
    try:
        print("=== 🚀 Starting Full Upload & Ingest Pipeline ===")
        datagroup_id = create_data_group()
        file_ids = upload_files(datagroup_id, FILE_PATHS)
        ingest_documents(datagroup_id, file_ids)
        print("✅ Pipeline completed successfully.")
    except Exception as e:
        print("❌ Pipeline failed:", str(e))








import requests
import os

# === CONFIGURATION ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer eyJ...YOUR_FULL_MFA_TOKEN..."  # <-- Replace with valid token
HEADERS_JSON = {
    "Authorization": AUTH_TOKEN,
    "Content-Type": "application/json"
}
HEADERS_FORM = {
    "Authorization": AUTH_TOKEN
}

# === FILE PATHS TO UPLOAD ===
FILE_PATHS = [
    r"C:\Users\P872643\Downloads\rebal.pdf",
    r"C:\Users\P872643\Downloads\NPS Monthly Fee Accre_GD SOP V3 0 1.pdf"
]

# === STEP 1: Create Data Group ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "AutoDataGroup",
        "description": "Created via script",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Create Data Group:", response.status_code, response.text)
    response.raise_for_status()
    dsId = response.json().get("dsId")
    if not dsId:
        raise Exception("No datagroup ID returned")
    print(f"[✓] Data group created: {dsId}")
    return dsId

# === STEP 2: Upload Files ===
def upload_files(datagroup_id, file_paths):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print(f"Uploading to: {url}")

    files = []
    for path in file_paths:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")
        filename = os.path.basename(path)
        mime_type = "application/pdf" if filename.endswith(".pdf") else "application/octet-stream"
        files.append(("files", (filename, open(path, "rb"), mime_type)))

    data = {
        "userMetadata": "0"
    }

    response = requests.post(url, headers=HEADERS_FORM, files=files, data=data, verify=False)
    print("Upload Response:", response.status_code, response.text)
    response.raise_for_status()

    for _, f in files:
        f[1].close()

    file_ids = []
    res_json = response.json()
    if "fileId" in res_json:
        file_ids.append(res_json["fileId"])
    elif "fileIds" in res_json:
        file_ids.extend(res_json["fileIds"])
    else:
        raise Exception("No fileId(s) returned in upload")

    print(f"[✓] Uploaded {len(file_ids)} file(s):", file_ids)
    return file_ids

# === STEP 3: Ingest Uploaded Documents ===
def ingest_documents(datagroup_id, file_ids):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {
        "documentIds": file_ids
    }

    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    print("Ingest Response:", response.status_code, response.text)
    response.raise_for_status()

    msg = response.json().get("message")
    print(f"[✓] Ingestion started: {msg}")

# === MAIN DRIVER ===
if __name__ == "__main__":
    try:
        print("=== 🚀 Starting Full RAI Upload + Ingest Pipeline ===")
        datagroup_id = create_data_group()
        file_ids = upload_files(datagroup_id, FILE_PATHS)
        ingest_documents(datagroup_id, file_ids)
        print("✅ Pipeline completed successfully.")
    except Exception as e:
        print("❌ Pipeline failed:", str(e))









import requests
import os

BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "62603385-7504-4284-98c4-837a249526d7"  # ✅ Corrected: "APP ID" → "APP_ID"
DATAGROUP_ID = "a380e809-60bf-4e65-ad8b-7fe8a3059c4f"
FILE_PATH = r"c:\Users\P872643\Downloads\novo_nordisk_ext.pdf"
AUTH_TOKEN = "Bearer eyJBeXAiOiJKV1QiLCJub25jZSI6IlBjRERMT21mcXZJUnNUbl1vQUlldnZDM3pmMHZGaDZrQ1U3WDJSeVU1UmMiLCJhbGc10275UZI1"  # ✅ Make sure it's fully copied

def upload_file():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{DATAGROUP_ID}/uploadfile"
    print(f"Uploading to: {url}")

    with open(FILE_PATH, "rb") as file:
        files = {
            "files": (os.path.basename(FILE_PATH), file, "application/pdf"),
            "userMetadata": (None, "0")
        }

        response = requests.post(url, headers={"Authorization": AUTH_TOKEN}, files=files, verify=False)

    print("Status:", response.status_code)
    print("Response:", response.text)

    response.raise_for_status()

    file_id = response.json().get("fileId")
    if file_id:
        print(f"✅ File uploaded successfully! fileId: {file_id}")
    else:
        print("⚠️ Upload succeeded but fileId was not returned. Check for duplicates.")
    return file_id

# Run the function
upload_file()






import requests
import os

BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-7504-4284-98c4-837a249526d7"
DATAGROUP_ID = "a380e809-60bf-4e65-ad8b-7fe8a3059c4f"
FILE_PATH = "C:/Users/P872643/Downloads/NPS Monthly Fee Accre GD SOP V301.pdf"

AUTH_TOKEN = "Bearer <PASTE_VALID_MFA_TOKEN_HERE>"

def upload_file():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{DATAGROUP_ID}/uploadfile"
    print(f"Uploading to: {url}")
    
    with open(FILE_PATH, "rb") as file:
        files = {
            "files": (os.path.basename(FILE_PATH), file, "application/pdf"),
            "userMetadata": (None, "0")
        }
        response = requests.post(url, headers={"Authorization": AUTH_TOKEN}, files=files, verify=False)
    
    print("Status:", response.status_code)
    print("Response:", response.text)
    
    response.raise_for_status()
    file_id = response.json().get("fileId")
    print("✅ File Uploaded, fileId:", file_id)
    return file_id

# Run it
upload_file()




import requests
import os

BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <your_token_here>"
FILE_PATH = "/full/path/to/your/file.pdf"

HEADERS = {
    "Authorization": AUTH_TOKEN
}

def upload_file(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    print("Uploading to:", url)

    with open(FILE_PATH, "rb") as file:
        files = {
            "files": (os.path.basename(FILE_PATH), file, "application/pdf"),
            "userMetadata": (None, "0")
        }

        response = requests.post(url, headers=HEADERS, files=files, verify=False)

    print("Upload response:", response.status_code, response.text)
    response.raise_for_status()

    file_id = response.json().get("fileId")
    if not file_id:
        raise ValueError(f"No fileId returned. Got: {response.json()}")
    print(f"[✓] File uploaded successfully. fileId: {file_id}")
    return file_id







import requests
import os

# === CONFIG ===
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"  # Double check this matches Swagger exactly
AUTH_TOKEN = "Bearer <VALID_TOKEN>"
FILE_PATH = "/path/to/your/file.pdf"

HEADERS_JSON = {
    "Content-Type": "application/json",
    "Authorization": AUTH_TOKEN
}

HEADERS_MULTIPART = {
    "Authorization": AUTH_TOKEN
}

# === STEP 1: Create Data Group ===
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "Data2",
        "description": "Data2",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    data = response.json()
    print("[✔] Full Response:", data)
    datagroup_id = data.get("dsId", "").strip().replace('"', '')
    if not datagroup_id:
        raise ValueError("Invalid datagroup_id returned.")
    return datagroup_id

# === STEP 2: Upload File ===
def upload_file(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    with open(FILE_PATH, "rb") as file:
        files = {
            "files": (os.path.basename(FILE_PATH), file, "application/pdf"),
            "userMetadata": (None, "0")
        }
        response = requests.post(url, headers=HEADERS_MULTIPART, files=files, verify=False)
    response.raise_for_status()
    file_id = response.json().get("fileId")
    if not file_id:
        raise ValueError("Invalid fileId returned.")
    print(f"[✔] File Uploaded: {file_id}")
    return file_id

# === STEP 3: Ingest Document ===
def ingest_document(datagroup_id, file_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {
        "documentIds": [file_id]
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    print(f"[✔] Ingestion Triggered: {response.json()['message']}")
    return response.json()

# === DRIVER ===
if __name__ == "__main__":
    try:
        print("=== Starting Document Ingestion Pipeline ===")
        datagroup_id = create_data_group()
        file_id = upload_file(datagroup_id)
        result = ingest_document(datagroup_id, file_id)
        print("=== ✅ Pipeline Completed Successfully ===")
        print(result)
    except Exception as e:
        print("[✗] Pipeline Failed:", str(e))






Starting Document Ingestion Pipeline ===

Full response ('name': 'Data2', 'description': 'Data2', 'dsType': 'FOLDER', 'dataClassification': 'general', 'metadata": (), ingestionStrategy': {}, 'config': (), 'status': '', 'createdBy': 'p872643', 'modifiedBy': 'p872643', 'createdTs': '2025-06-13T10:32:51Z', 'modifiedTs': '2025-06-13T10:32:51Z', 'dsId': '1a12e0f0-f236-4668-b4db-1391ae42de11", 'kbId': 'e954df9f-791a-4680-88c9-0ae8e01affle', 'slug': None)

[X] Pipeline Failed: 404 Client Error: Not Found for url: https://rai-uat.statestr.com/agi/igs/api/v1/apps/62643385-7544-4284-98c4-837-2495207/-

In 2. Col 1

100% Windows

IN







import requests
import os

# CONFIG
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"
AUTH_TOKEN = "Bearer <YOUR_TOKEN_HERE>"  # Replace with your actual token
FILE_PATH = "/path/to/your/file.pdf"     # Replace with full local path to PDF

HEADERS_JSON = {
    "Content-Type": "application/json",
    "Authorization": AUTH_TOKEN
}

HEADERS_MULTIPART = {
    "Authorization": AUTH_TOKEN
}

# Step 1: Create Data Group
def create_data_group():
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    payload = {
        "name": "Data",
        "description": "Data",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": {},
        "config": {}
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    datagroup_id = response.json()["dsId"]
    print(f"[✓] Data Group Created: {datagroup_id}")
    return datagroup_id

# Step 2: Upload File
def upload_file(datagroup_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    with open(FILE_PATH, "rb") as file:
        files = {
            "files": (os.path.basename(FILE_PATH), file, "application/pdf"),
            "userMetadata": (None, "0")
        }
        response = requests.post(url, headers=HEADERS_MULTIPART, files=files, verify=False)
    response.raise_for_status()
    file_id = response.json()["fileId"]
    print(f"[✓] File Uploaded: {file_id}")
    return file_id

# Step 3: Ingest Document
def ingest_document(datagroup_id, file_id):
    url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    payload = {
        "documentIds": [file_id]
    }
    response = requests.post(url, headers=HEADERS_JSON, json=payload, verify=False)
    response.raise_for_status()
    print(f"[✓] Ingestion Triggered: {response.json()['message']}")
    return response.json()

# Driver
if __name__ == "__main__":
    try:
        print("=== Starting Document Ingestion Pipeline ===")
        datagroup_id = create_data_group()
        file_id = upload_file(datagroup_id)
        result = ingest_document(datagroup_id, file_id)
        print("=== Pipeline Completed Successfully ===")
        print(result)
    except Exception as e:
        print("[✗] Pipeline Failed:", str(e))








Request URL

https://rai-uat.statestr.com/agi/igs/api/v1/apps/626d3385-7504-4284-984-837a249526d7/dgs/4e715cc3-d7c7-42a2-bbcb-7ff2d96d4fdd/ingest

Server response

Code

200

Details

Response body

"message": "File ingestion started successfully", "id": "62643385-754-4284-98c4-837a249526d7",

"datagroupId": "4e715cc3-d7c7-42a2-bbcb-7ff2d96d4fdd", "listofFiles": [ 1

"775c190c-255d-4e3e-a644-d1a81b912042"

Response headers

access-control-allow-credentials: true

access-control-allow-origin: https://rai-uat.statestr.com

access-control-expose-headers:

connection: Keep-Alive

content-length: 201

content-type: application/json

date: Fri, 13 Jun 2025 10:06:34 GMT

keep-alive: timeout-5,max-100

server: nginx/1.21.5

strict-transport-security: max-age-31536000; includeSubDomains; preload

vary: Origin

x-content-type-options: nosniff

x-frame-options: SAMEORIGIN

x-xss-protection: 1; mode-block

Responses

Code

200

Description

Successful Response

Media type

application/json






import requests
import json
import os
import urllib3

# Disable SSL verification warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ---------------- CONFIG ----------------
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-754-4284-98c4-837a249526d7"
API_KEY = "Bearer eyJ0eXAiOiJKV1QiLCJub25jZSI6..."  # Replace with valid token
FILE_PATH = r"c:\Users\P872643\Box\AI -India Projects\Gen AI 2025\Domain Finetuning\Data\Disseminetation & Reporting Zurich Daily SOP.pdf"
STRATEGY_FILE = "injection_strategies.json"

headers_json = {
    "Authorization": API_KEY,
    "Content-Type": "application/json"
}
headers_form = {
    "Authorization": API_KEY
}

# ---------------- LOAD FIRST STRATEGY ----------------
with open(STRATEGY_FILE, "r") as f:
    data = json.load(f)

strategy = None
for item in data.get("items", []):
    if "pdf" in item:
        strategy = {"pdf": item["pdf"]}
        strategy_name = item.get("name", "Unnamed")
        break

if not strategy:
    print("❌ No PDF strategy found.")
    exit()

# ---------------- STEP 1: Create DataGroup ----------------
create_dg_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
dg_payload = {
    "name": f"TestDG_{strategy_name}",
    "description": f"Testing ingestion with {strategy_name}",
    "dsType": "FOLDER",
    "dataclassification": "general",
    "metadata": {},
    "ingestionStrategy": strategy,
    "config": {}
}

try:
    dg_response = requests.post(create_dg_url, headers=headers_json, json=dg_payload, verify=False)
    dg_response.raise_for_status()
    datagroup_id = dg_response.json()["dsId"]
    print(f"✅ DataGroup created: {datagroup_id}")
except Exception as e:
    print(f"❌ Failed to create DataGroup: {e}")
    exit()

# ---------------- STEP 2: Upload File ----------------
upload_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"

try:
    with open(FILE_PATH, "rb") as f:
        files = {"files": (os.path.basename(FILE_PATH), f)}
        upload_response = requests.post(upload_url, headers=headers_form, files=files, verify=False)
        upload_response.raise_for_status()
        document_ids = upload_response.json().get("documentIds", [])
        print(f"✅ File uploaded. Document IDs: {document_ids}")
except Exception as e:
    print(f"❌ File upload failed: {e}")
    exit()

# ---------------- STEP 3: Ingest Document ----------------
ingest_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
ingest_payload = {"documentIds": document_ids}

try:
    ingest_response = requests.post(ingest_url, headers=headers_json, json=ingest_payload, verify=False)
    ingest_response.raise_for_status()
    print(f"✅ Ingestion triggered: {ingest_response.json()}")
except Exception as e:
    print(f"❌ Ingestion failed: {e}")






import requests
import json
import os
import urllib3

# Disable SSL warnings (since we're using verify=False for UAT)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ---------------- CONFIG ----------------
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-754-4284-98c4-837a249526d7"
API_KEY = "Bearer eyJ0eXAiOiJKV1QiLCJub25jZSI6..."  # Use full correct token here
FILE_PATH = r"c:\Users\P872643\Box\AI -India Projects\Gen AI 2025\Domain Finetuning\Data\Disseminetation & Reporting Zurich Daily SOP.pdf"
STRATEGY_FILE = "injection_strategies.json"

headers_json = {
    "Authorization": API_KEY,
    "Content-Type": "application/json"
}
headers_form = {
    "Authorization": API_KEY
}

# ---------------- LOAD STRATEGIES ----------------
with open(STRATEGY_FILE, "r") as f:
    strategy_data = json.load(f)

def extract_strategies(data):
    strategies = []
    for item in data.get("items", []):
        for k in ["pdf", "md", "docx", "txt", "csv", "xls", "xlsx", "jpeg", "jpg", "png", "web"]:
            if k in item:
                strategies.append({
                    "name": item.get("name", "Unnamed"),
                    "type": k,
                    "strategy": {k: item[k]}
                })
                break
    return strategies

strategies = extract_strategies(strategy_data)
print(f"🔁 Total strategies found: {len(strategies)}")

# ---------------- LOOP OVER STRATEGIES ----------------
for i, strat in enumerate(strategies):
    print(f"\n▶️ [{i+1}/{len(strategies)}] Applying strategy: {strat['name']} ({strat['type']})")

    # Step 1: Create DataGroup
    create_dg_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    dg_payload = {
        "name": f"DG_{strat['name']}",
        "description": f"Auto-ingestion for {strat['name']}",
        "dsType": "FOLDER",
        "dataClassification": "general",
        "metadata": {},
        "ingestionStrategy": strat["strategy"],
        "config": {}
    }

    try:
        dg_response = requests.post(create_dg_url, headers=headers_json, json=dg_payload, verify=False)
        dg_response.raise_for_status()
        datagroup_id = dg_response.json()["dsId"]
        print(f"✅ Created DataGroup: {datagroup_id}")
    except Exception as e:
        print(f"❌ Failed to create datagroup for {strat['name']}: {e}")
        continue

    # Step 2: Upload File
    upload_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    try:
        with open(FILE_PATH, "rb") as f:
            files = {"files": (os.path.basename(FILE_PATH), f)}
            upload_response = requests.post(upload_url, headers=headers_form, files=files, verify=False)
            upload_response.raise_for_status()
            document_ids = upload_response.json().get("documentIds", [])
            print(f"✅ Uploaded document. IDs: {document_ids}")
    except Exception as e:
        print(f"❌ File upload failed for {strat['name']}: {e}")
        continue

    # Step 3: Ingest Document
    ingest_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    ingest_payload = {"documentIds": document_ids}

    try:
        ingest_response = requests.post(ingest_url, headers=headers_json, json=ingest_payload, verify=False)
        ingest_response.raise_for_status()
        print(f"✅ Ingestion triggered. Response: {ingest_response.json()}")
    except Exception as e:
        print(f"❌ Ingestion failed for {strat['name']}: {e}")








url = "https://rai-uat.statestr.com/agi/igs/api/v1/apps/626d3385-754-4284-98c4-837a249526d7/dgs"

HEADERS = {

"Content-Type": "application/json",

"Authorization": "Bearer eyJ0eXAiOiJKV1QiLCJub25jZSI6InBKWDNVN@p4ej1YVEZhR09xVzZ4SjVoeWRJMFH@Mk9yeWtwdGZxMWZhd@@iLCJhb

}

payload = {

"name": "Data",

"description": "Data",

"dsType": "FOLDER",

"dataClassification": "general",

"metadata": {},

"ingestionStrategy": {},

"config": {}

}

response = requests.post(url, headers=HEADERS, json=payload, verify=False)

print("Status Code:", response.status_code)

print("Response Body:", response.json())

0.85

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to

warnings.warn(

Status Code: 200

Spaces: 4 CRLF Cell 20 of 21

Response Body: {'name': 'Data', 'description': 'Data', 'dsType': 'FOLDodataClassification': 'general', 'metadata": {}

ENG

14:23

9]

20






import requests

url = "https://rai-uat.statestr.com/agi/igs/api/v1/apps/62603385-754-4284-98c4-837a249526d7/dgs"
headers = {
    "Authorization": "Bearer your_token_here",
    "Content-Type": "application/json"
}
payload = {
    "name": "Data",
    "description": "Data",
    "dsType": "FOLDER",
    "dataClassification": "general",
    "metadata": {},
    "ingestionStrategy": {},
    "config": {}
}

response = requests.post(url, headers=headers, json=payload)
print("Status Code:", response.status_code)
print("Response Body:", response.json())






"I didn’t get time to work on this story. Today also, I won’t be able to work on it because I have some work in FTS
My Observations:

The end-to-end FTS pipeline is now functioning well. Previously, I observed a couple of issues:

The total number of original questions generated was lower than expected.

Diverse questions were not being generated accurately.

Both issues have now been resolved.

Today, I plan to test the pipeline using different prompts and configurations. I will share my updated observations by the end of the day (EOD).



import json

json_str = '''
{
    "shipmentId": "8072373880",
    "plantCode": "CDC",
    "productTypeMix": "",
    "palletMix": "",
    "shipmentDestination": "isklad 180127 - Shooos s.r.o.",
    "destinationCountry": "US",
    "vasCode": {
        "cartonizationType": "",
        "palletizationType": ""
    },
    "shipmentDetails": [
        {
            "productCode": "BD7633540",
            "productQuantity": 10,
            "productType": "FTW",
            "productDimension": {
                "length": 5.3,
                "width": 6.2,
                "height": 2.4
            },
            "productVolume": 5000.856,
            "fullCaseDimension": {
                "length": 36.2,
                "width": 29.1,
                "height": 12.1
            },
            "fullCaseQuantity": 10,
            "prePackGroupCode": "",
            "prePackQuantity": 0
        }
    ]
}
'''

# Clean and convert to single line
single_line_json = json.dumps(json.loads(json_str), separators=(',', ':'))
print(single_line_json)







Hi Susanta,

There are still issues in the original QA – the total number of questions generated is less than expected.

Compared to the previous run, the procedural answers are looking better.

We need to generate a higher number of questions to properly evaluate the results. I’m currently waiting for Bhoomika to resolve the issue.
import io
import json
import pandas as pd
import logging
from concurrent.futures import ThreadPoolExecutor
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from question_prompts import DIVERSE_QUESTION_PROMPT
from tenacity import retry, stop_after_attempt, wait_exponential
from flow import tool
from common_svc.storage.storage_config import BlobLocation
from common_svc.storage.blob_client import BlobStorageClient
from common_svc.logger.log_util import configure_loggers
from methods.ssrai_service import SSRAIService
from methods.load_file_blob_service import DataCurationService

# Configure logger
configure_loggers()
logger = logging.getLogger(__name__)

def append_to_blob_csv(diverse_output, filename):
    """Save diverse questions and answers to blob storage"""
    try:
        absolute_path = f"{filename}/diverse_questions.csv"
        blob_location = BlobLocation(container_name="pvt-markets", absolute_path=absolute_path)
        client = BlobStorageClient()

        if client.check_file_exists(storage_location=blob_location).data:
            df = client.load_file_as_df(storage_location=blob_location).data
            temp_df = pd.DataFrame(diverse_output)
            df = pd.concat([df, temp_df], ignore_index=True)
        else:
            df = pd.DataFrame(diverse_output)

        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        DataCurationService().upload_file_blob(
            csv_buffer.getvalue(),
            filename="diverse_questions.csv",
            blobFilePath=blob_location
        )
    except Exception as e:
        logger.error("Error saving to blob CSV", exc_info=True)
        raise e

def clear_output(filename):
    """Remove the existing diverse_questions.csv file if it exists"""
    absolute_path = f"{filename}/diverse_questions.csv"
    blob_location = BlobLocation(container_name="pvt-markets", absolute_path=absolute_path)
    if BlobStorageClient().check_file_exists(storage_location=blob_location).data:
        BlobStorageClient().delete_file(storage_location=blob_location)

def split_into_batches(input_list, num_diverse_qa):
    """Split questions into batches based on their type and batch size."""
    if not isinstance(input_list, list):
        raise ValueError("Input must be a list.")
    if not isinstance(num_diverse_qa, dict):
        raise ValueError("num_diverse_qa must be a dictionary.")

    batches = {key: [] for key in num_diverse_qa.keys()}
    for question in input_list:
        question_type = question.get("Question Type")
        if question_type in num_diverse_qa:
            batches[question_type].append(question)
            if len(batches[question_type]) == num_diverse_qa[question_type]:
                yield batches[question_type]
                batches[question_type] = []
    for question_type, batch in batches.items():
        if batch:
            yield batch

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))
def generate_diverse_questions(input_json, num_diverse_qa, llm, prompt):
    if not isinstance(num_diverse_qa, dict):
        raise ValueError("num_diverse_qa must be a dictionary.")

    question_type = input_json[0].get("Question Type", "")
    num_diverse_questions = num_diverse_qa.get(question_type, 1)
    llm_chain = LLMChain(llm=llm, prompt=prompt)
    response = llm_chain.predict(
        input_json=json.dumps(input_json),
        num_diverse_questions=str(num_diverse_questions)
    )
    return json.loads(response)["response"]

@tool
def diverse_qa_generation(fileData, diverse_qa_batch_size, max_workers, rai_model_name, rai_wrapper_name, diverse_qa_model_kwargs):
    try:
        # Dynamic configuration: factual=2, procedural=3, conceptual=2, inferential=2, reasoning-based=2
        num_diverse_qa = {
            "Factual": 2,
            "Procedural": 3,
            "Conceptual": 2,
            "Inferential": 2,
            "Reasoning-Based": 2
        }

        cli = SSRAIService(model_name=rai_model_name, wrapper_name=rai_wrapper_name, model_kwargs=diverse_qa_model_kwargs)
        llm = cli.get_llm()

        prompt = PromptTemplate(
            input_variables=["input_json", "num_diverse_questions"],
            template=DIVERSE_QUESTION_PROMPT
        )

        for file in fileData:
            clear_output(file["fileName"])
            file["diverseQuestions"] = []
            futures = []

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                for i, batch in enumerate(split_into_batches(file["originalQuestions"], num_diverse_qa), 1):
                    future = executor.submit(generate_diverse_questions, batch, num_diverse_qa, llm, prompt)
                    futures.append((future, i, batch[0].get("Question Type")))

                for future, batch_no, qtype in futures:
                    try:
                        result = future.result()
                        diverse_output = []
                        for response in result:
                            original_id = response["id"]
                            original_question = response["original_question"]
                            original_answer = response["original_answer"]
                            for diverse in response["diverse_questions"]:
                                diverse_output.append({
                                    "ID": original_id,
                                    "Question Type": qtype,
                                    "Original Question": original_question,
                                    "Original Answer": original_answer,
                                    "Diverse Question": diverse["diverse_question"],
                                    "Diverse Answer": diverse["diverse_answer"]
                                })
                        file["diverseQuestions"] += diverse_output
                        append_to_blob_csv(diverse_output, file["fileName"])
                    except Exception as e:
                        logger.error(f"Batch {batch_no} failed: {e}", exc_info=True)
                        raise Exception(str(e))
        return fileData
    except Exception as e:
        logger.error(f"Exception in generating diverse QA: {e}", exc_info=True)
        raise Exception(str(e))

# --- Example Excel Output Format ---
# +----+-----------------+----------------------+---------------------+-----------------------------+-----------------------------+
# | ID | Question Type   | Original Question    | Original Answer     | Diverse Question            | Diverse Answer              |
# +----+-----------------+----------------------+---------------------+-----------------------------+-----------------------------+
# | 1  | Factual         | What is X?           | It is A.            | When was X established?     | It is A.                    |
# | 1  | Factual         | What is X?           | It is A.            | What date marks the start?  | It is A.                    |
# | 2  | Procedural      | How to do Y?         | Step 1, Step 2      | What are the steps to Y?    | Step 1, Step 2              |
# | 2  | Procedural      | How to do Y?         | Step 1, Step 2      | Describe the Y process      | Step 1, Step 2              |
# | 2  | Procedural      | How to do Y?         | Step 1, Step 2      | Instructions for Y?         | Step 1, Step 2              |
# | 3  | Conceptual      | What is concept Z?   | Z is defined as ... | Define the idea of Z        | Z is defined as ...         |
# | 3  | Conceptual      | What is concept Z?   | Z is defined as ... | Explain what Z means        | Z is defined as ...         |
# | 4  | Reasoning-Based | Why is M important?  | Because of reasons. | What justifies M?           | Because of reasons.         |
# | 4  | Reasoning-Based | Why is M important?  | Because of reasons. | How is M logically derived? | Because of reasons.         |
# +----+-----------------+----------------------+---------------------+-----------------------------+-----------------------------+







import requests
import json
import os

# ---------------- CONFIGURATION ----------------
BASE_URL = "https://rai-uat.statestr.com/agi/igs"
APP_ID = "626d3385-75d4-4284-98c4-837a249526d7"  # Update if needed
API_KEY = "your_api_key_here"  # Replace with Bearer token
FILE_PATH = "sample.pdf"  # Can be .pdf, .md, etc.
STRATEGY_FILE = "injection_strategies.json"

# ---------------- HEADERS ----------------
headers_json = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}
headers_form = {
    "Authorization": f"Bearer {API_KEY}"
}

# ---------------- LOAD STRATEGIES ----------------
with open(STRATEGY_FILE, "r") as f:
    strategy_data = json.load(f)

# ---------------- EXTRACT VALID STRATEGIES ----------------
def extract_strategies(data):
    strategies = []
    for item in data.get("items", []):
        for k in item.keys():
            if k in ["pdf", "md", "docx", "txt", "csv", "xls", "xlsx", "jpeg", "jpg", "png", "web"]:
                strategies.append({
                    "name": item.get("name", "Unnamed"),
                    "type": k,
                    "strategy": {k: item[k]}
                })
                break
    return strategies

strategies = extract_strategies(strategy_data)
print(f"🔁 Total strategies found: {len(strategies)}")

# ---------------- LOOP OVER STRATEGIES ----------------
for i, strat in enumerate(strategies):
    print(f"\n▶️ [{i+1}/{len(strategies)}] Applying strategy: {strat['name']} ({strat['type']})")

    # --- Step 1: Create DataGroup ---
    create_dg_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs"
    dg_payload = {
        "name": f"Ingest_{strat['name']}",
        "description": f"Ingest using {strat['name']}",
        "dsType": "BLOB",
        "dataclassification": "general",
        "metadata": {},
        "ingestionstrategy": strat["strategy"],
        "config": {}
    }

    try:
        dg_response = requests.post(create_dg_url, headers=headers_json, json=dg_payload)
        dg_response.raise_for_status()
        datagroup_id = dg_response.json()["id"]
        print(f"✅ Created datagroup: {datagroup_id}")
    except Exception as e:
        print(f"❌ Failed to create datagroup for {strat['name']}: {e}")
        continue

    # --- Step 2: Upload File ---
    upload_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/uploadfile"
    try:
        with open(FILE_PATH, "rb") as f:
            files = {"files": (os.path.basename(FILE_PATH), f)}
            upload_response = requests.post(upload_url, headers=headers_form, files=files)
            upload_response.raise_for_status()
            document_ids = upload_response.json().get("documentIds", [])
            print(f"✅ Uploaded document. IDs: {document_ids}")
    except Exception as e:
        print(f"❌ File upload failed for {strat['name']}: {e}")
        continue

    # --- Step 3: Ingest Document ---
    ingest_url = f"{BASE_URL}/api/v1/apps/{APP_ID}/dgs/{datagroup_id}/ingest"
    ingest_payload = {"documentIds": document_ids}

    try:
        ingest_response = requests.post(ingest_url, headers=headers_json, json=ingest_payload)
        ingest_response.raise_for_status()
        print(f"✅ Ingestion triggered. Response: {ingest_response.json()}")
    except Exception as e:
        print(f"❌ Ingestion failed for {strat['name']}: {e}")





Hi Ritu, good morning,

Could you please help me understand what exactly I should focus on for the injection strategies? I’d like to know your expectations so I can proceed in the right direction.




import pandas as pd
import io
import logging

logger = logging.getLogger(__name__)

def save_questions_to_blob(questions, filename="original_questions.csv"):
    """Save valid questions and answers to blob storage safely without corruption."""

    try:
        # Step 1: Convert to DataFrame
        df = pd.DataFrame(questions)

        # Step 2: Ensure required columns exist
        required_columns = ["Question Type", "Question", "Answer"]
        for col in required_columns:
            if col not in df.columns:
                df[col] = ""

        # Step 3: Clean data types – ensure text fields are strings
        for col in ["Question", "Answer"]:
            df[col] = df[col].astype(str).replace({'\r': '', '\n': ' '}, regex=True)

        # Step 4: Write to in-memory CSV buffer
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')

        # Step 5: Upload to blob
        DataCurationService().upload_file_blob(
            csv_buffer.getvalue(),
            filename=filename,
            blobFilePath="your/blob/path/"  # Replace with actual blob path
        )

    except Exception as e:
        logger.error(f"Error saving questions to blob: {e}", exc_info=True)





Regarding the zip file, it’s not working. I’m receiving error code E401, along with an exception during task creation. Additionally, the LLM gate failed with error 504, and a Guardian error was thrown.

Also, in the results from the original file, the procedural question type is missing. Interestingly, when I run the same file locally, everything works as expected.




Hi Susanta,

I have tested both a single file and a zip file.

For single file uploads, it is working fine. I have tested with multiple individual files, and all of them are functioning as expected.

However, for one specific file — the original question and answer.csv — it seems to be corrupted, which is causing an error during processing.

Regarding the zip file, it is failing with error code E401, and there’s an exception while creating the task. The LLM gateway also returned a 504 Gateway Timeout error.

Additionally, in the results from the original file, the Procedural question type is missing. I ran the same file locally, and it worked correctly.

Please let me know if you need any additional details from my side.

Let me know if you’d like this in a shorter format or for email/chat use.














Hi Susanta,

I have tested the functionality with both a single file and a ZIP file.

For single files, it is working fine. I verified with multiple individual files, and all of them processed successfully.

However, for one specific file—the original "question and answer .CSV"—the file appears to be corrupted, which is why the error occurs.

For ZIP files, it is not working. I encountered an error with code E401, along with an exception during task creation. The LLM gateway also failed with a 504 error.





Hi Priya,
Sangeeth discussed something in the scrum call. Could you please guide me on what exactly I need to do?





Checking the distribution domain document to verify whether the answers are correct or not





pip install --proxy=http://proxy.statestr.com:80 \
  --trusted-host pypi.org \
  --trusted-host pypi.python.org \
  --trusted-host files.pythonhosted.org \
  --no-build-isolation \
  --upgrade \
  transformers==4.39.3 \
  peft==0.10.0 \
  trl==0.8.6 \
  torch==2.3.1+cu121 \
  bitsandbytes==0.43.1 \
  datasets==2.19.1 \
  accelerate==0.30.1 \
  sentencepiece==0.2.0 \
  protobuf==4.25.3











import os
import json
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient

# --- Setup LLM ---
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)
chat_model_name = "ssgpt-40"
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

# --- Prompt Template ---
DIVERSE_BY_TYPE_PROMPT = """
You are a question rephrasing assistant. Your task is to generate diverse versions of the input question-answer pair, but only of the **"{question_type}"** type.

### Input:
Original Question: {original_question}
Original Answer: {original_answer}

### Instructions:
- Generate exactly {num_questions} diverse **{question_type}** questions.
- Ensure the questions are not subsets or minor rewrites.
- Keep the meaning same as the answer.
- If it's Procedural, keep step-by-step formatting.

### Output Format:
{{
  "id": "{id}",
  "question_type": "{question_type}",
  "original_question": "{original_question}",
  "original_answer": "{original_answer}",
  "diverse_questions": [
    {{
      "diverse_question": "...",
      "diverse_answer": "..."
    }},
    ...
  ]
}}

Return only JSON. No markdown, no explanation.
"""

# --- Excel to JSON ---
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "id": str(i + 1),
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for i, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""

# --- JSON Utilities ---
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

def clear_output(path="responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)

def append_to_output(batch_responses, path="responses.json"):
    try:
        existing_data = []
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")

# --- LLM Call per Question Type ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions_per_type(qa, num_questions=3):
    try:
        prompt = PromptTemplate(
            input_variables=["id", "question_type", "original_question", "original_answer", "num_questions"],
            template=DIVERSE_BY_TYPE_PROMPT
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.predict(
            id=qa["id"],
            question_type=qa["Question_type"],
            original_question=qa["Question"],
            original_answer=qa["Answer"],
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except Exception as e:
        print("Parsing error:", e)
        return {}

# --- Batch Processing by Dynamic Type Count ---
def process_per_question(data, type_question_count: dict, output_path="responses.json"):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for qa in data:
            qtype = qa["Question_type"]
            num_q = type_question_count.get(qtype, 0)  # default 0 if not present
            if num_q > 0:
                futures.append(executor.submit(generate_diverse_questions_per_type, qa, num_q))
        for future in futures:
            try:
                result = future.result()
                if result:
                    append_to_output([result], output_path)
            except Exception as e:
                print("Error:", e)

# --- Save to Excel ---
def save_responses_to_excel(output_json_path, excel_file_path="output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for item in data:
        for dq in item["diverse_questions"]:
            excel_data.append({
                "ID": item["id"],
                "Question Type": item["question_type"],
                "Original Question": item["original_question"],
                "Original Answer": item["original_answer"],
                "Diverse Question": dq["diverse_question"],
                "Diverse Answer": dq["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel: {excel_file_path}")

# --- Main Runner ---
if __name__ == "__main__":
    file_path = r"your_input_file.xlsx"
    output_json = r"converted_data.json"
    final_json_output = r"responses_by_type.json"
    excel_output = r"diverse_questions_output.xlsx"

    # Define how many questions per type
    type_question_count = {
        "Factual": 1,
        "Procedural": 3,
        "Conceptual": 1,
        "Reasoning-Based": 3
    }

    json_path = convert_excel_to_json(file_path, output_json)
    data = read_json_file(json_path)

    if data:
        process_per_question(data, type_question_count, output_path=final_json_output)
        save_responses_to_excel(final_json_output, excel_output)







import os
import json
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient

# --- Setup LLM ---
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)
chat_model_name = "ssgpt-40"
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

# --- Prompt Template ---
DIVERSE_BY_TYPE_PROMPT = """
You are a question rephrasing assistant. Your task is to generate diverse versions of the input question-answer pair, but only of the **"{question_type}"** type.

### Input:
Original Question: {original_question}
Original Answer: {original_answer}

### Instructions:
- Generate exactly {num_questions} diverse **{question_type}** questions.
- Ensure the questions are not subsets or minor rewrites.
- Keep the meaning same as the answer.
- If it's Procedural, keep step-by-step formatting.

### Output Format:
{{
  "id": "{id}",
  "question_type": "{question_type}",
  "original_question": "{original_question}",
  "original_answer": "{original_answer}",
  "diverse_questions": [
    {{
      "diverse_question": "...",
      "diverse_answer": "..."
    }},
    ...
  ]
}}

Return only JSON. No markdown, no explanation.
"""

# --- Excel to JSON ---
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "id": str(i + 1),
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for i, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""

# --- JSON Utilities ---
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

def clear_output(path="responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)

def append_to_output(batch_responses, path="responses.json"):
    try:
        existing_data = []
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")

# --- LLM Call per Question Type ---
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions_per_type(qa, num_questions=3):
    try:
        prompt = PromptTemplate(
            input_variables=["id", "question_type", "original_question", "original_answer", "num_questions"],
            template=DIVERSE_BY_TYPE_PROMPT
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.predict(
            id=qa["id"],
            question_type=qa["Question_type"],
            original_question=qa["Question"],
            original_answer=qa["Answer"],
            num_questions=str(num_questions)
        )
        return json.loads(response)
    except Exception as e:
        print("Parsing error:", e)
        return {}

# --- Batch Processing ---
def process_per_question(data, num_questions=3, output_path="responses.json"):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(generate_diverse_questions_per_type, qa, num_questions) for qa in data]
        for future in futures:
            try:
                result = future.result()
                if result:
                    append_to_output([result], output_path)
            except Exception as e:
                print("Error:", e)

# --- Save to Excel ---
def save_responses_to_excel(output_json_path, excel_file_path="output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for item in data:
        for dq in item["diverse_questions"]:
            excel_data.append({
                "ID": item["id"],
                "Question Type": item["question_type"],
                "Original Question": item["original_question"],
                "Original Answer": item["original_answer"],
                "Diverse Question": dq["diverse_question"],
                "Diverse Answer": dq["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel: {excel_file_path}")

# --- Main Runner ---
if __name__ == "__main__":
    file_path = r"your_input_file.xlsx"
    output_json = r"converted_data.json"
    final_json_output = r"responses_by_type.json"
    excel_output = r"diverse_questions_output.xlsx"

    json_path = convert_excel_to_json(file_path, output_json)
    data = read_json_file(json_path)

    if data:
        process_per_question(data, num_questions=3, output_path=final_json_output)
        save_responses_to_excel(final_json_output, excel_output)






DIVERSE_QUESTION_PROMPT = """
You are tasked with generating multiple types of diverse questions and answers for each input question-answer pair.

The input will contain a list of original questions and answers.

For each input pair, generate **separate JSON objects for each question type**, based on the required number below.

## Required distribution:
{type_count_description}

### Response Format:
Return a **list of JSON objects**, one for each question type per input pair. Each object must be in this exact format:

{{
  "id": "<unique_id>",
  "question_type": "<Factual | Procedural | Conceptual | Inferential | Reasoning-Based>",
  "original_question": "<Original Question>",
  "original_answer": "<Original Answer>",
  "diverse_questions": [
    {{
      "diverse_question": "<Alternative question>",
      "diverse_answer": "<Answer>"
    }},
    ...
  ]
}}

### Additional Requirements:
- Do **not** combine different question types into one object.
- Use exactly the number of diverse questions specified per type.
- Procedural answers should be formatted as step-by-step.
- Return only valid JSON array. No markdown or explanations.
- Use double quotes in all JSON strings.
"""






import os
import json
import base64
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from tenacity import retry, stop_after_attempt, wait_exponential
from ssrai import SSRAIClient


# -------- Configuration --------
ssc_rai_client = SSRAIClient(
    host="https://api-uat.statestr.com/",
    eam_consumer_key="YOUR_KEY",
    eam_consumer_secret="YOUR_SECRET",
    auth_type="pat",
    api_version="1.0"
)

chat_model_name = "ssgpt-40"  # Replace with actual
llm = ssc_rai_client.llm(wrapper="langchain", model_name=chat_model_name)

DIVERSE_QUESTION_PROMPT = """
You are tasked to generate diverse question-answer pairs from input question-answer data.

For each original pair, generate diverse questions and answers based on the following distribution:
{type_count_description}

### Question Types:
1. Factual - Direct and fact-based
2. Conceptual - Based on understanding of principles
3. Inferential - Based on logical conclusions
4. Reasoning-Based - Based on logical reasoning
5. Procedural - Step-by-step process

### Requirements:
- Output format:
{{
  "id": "<unique_id>",
  "question_type": "<question_type>",
  "original_question": "<Original Question>",
  "original_answer": "<Original Answer>",
  "diverse_questions": [
    {{
      "diverse_question": "<New Question>",
      "diverse_answer": "<Answer>"
    }},
    ...
  ]
}}
- Procedural answers must retain step-by-step clarity.
- Avoid repeating or subset questions.
"""

prompt = PromptTemplate(
    input_variables=["input_json", "type_count_description"],
    template=DIVERSE_QUESTION_PROMPT
)

llm_chain = LLMChain(llm=llm, prompt=prompt)


# -------- Excel to JSON --------
def convert_excel_to_json(file_path: str, output_json_path: str) -> str:
    try:
        df = pd.read_excel(file_path, dtype=str).fillna("")
        required = {"Question Type", "Question", "Answer"}
        if not required.issubset(df.columns):
            raise KeyError(f"Missing columns. Required: {required}")
        json_data = [
            {
                "Question_type": row["Question Type"],
                "Question": row["Question"],
                "Answer": row["Answer"]
            } for _, row in df.iterrows()
        ]
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        print(f"Saved JSON to {output_json_path}")
        return output_json_path
    except Exception as e:
        print(f"Error: {e}")
        return ""


# -------- JSON Utils --------
def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []


def split_into_batches(input_list, batch_size):
    for i in range(0, len(input_list), batch_size):
        yield input_list[i:i + batch_size]


# -------- Prompt Helper --------
def get_type_count_description(type_counts: dict) -> str:
    return "\n".join([f"- {qtype}: {count}" for qtype, count in type_counts.items()])


# -------- Question Generation --------
@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1))
def generate_diverse_questions(input_json: str, type_counts: dict) -> list:
    try:
        type_desc = get_type_count_description(type_counts)
        response = llm_chain.predict(input_json=input_json, type_count_description=type_desc)
        return json.loads(response)
    except Exception as e:
        print(f"Parsing error: {e}")
        return []


# -------- Output Handling --------
def clear_output(path="incremental_responses.json"):
    with open(path, "w", encoding="utf-8") as f:
        json.dump([], f)


def append_to_output(batch_responses, path="incremental_responses.json"):
    try:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        existing_data.extend(batch_responses)
        with open(path, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
    except Exception as e:
        print(f"Append error: {e}")


def save_responses_to_excel(output_json_path, excel_file_path="diverse_output.xlsx"):
    with open(output_json_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)

    excel_data = []
    for response in data:
        for item in response["diverse_questions"]:
            excel_data.append({
                "ID": response["id"],
                "Question Type": response["question_type"],
                "Original Question": response["original_question"],
                "Original Answer": response["original_answer"],
                "Diverse Question": item["diverse_question"],
                "Diverse Answer": item["diverse_answer"]
            })

    df = pd.DataFrame(excel_data)
    df.sort_values(by=["Question Type", "ID"], inplace=True)
    df.to_excel(excel_file_path, index=False, engine="openpyxl")
    print(f"Saved to Excel at {excel_file_path}")


# -------- Batch Processing --------
def process_batches(data, batch_size, type_counts, output_path="incremental_responses.json", max_workers=3):
    clear_output(output_path)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for i, batch in enumerate(split_into_batches(data, batch_size), 1):
            print(f"Processing Batch {i} with {len(batch)} QA pairs")
            batch_input = json.dumps(batch)
            futures.append(executor.submit(generate_diverse_questions, batch_input, type_counts))
        for future in futures:
            try:
                result = future.result()
                append_to_output(result, output_path)
            except Exception as e:
                print(f"Batch failed: {e}")


# -------- Main Flow --------
if __name__ == "__main__":
    file_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_1.xlsx"
    output_json_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_1.json"
    incremental_output_path = r"ds_modules/incremental_responses_part.json"
    output_excel_path = r"h:\Question and answer\latest\Procedural question with diverse\set2_out.xlsx"

    type_counts = {
        "Factual": 1,
        "Procedural": 3,
        "Conceptual": 2
    }

    json_path = convert_excel_to_json(file_path, output_json_path)
    data = read_json_file(json_path)
    if data:
        process_batches(data, batch_size=2, type_counts=type_counts, output_path=incremental_output_path)
        save_responses_to_excel(incremental_output_path, output_excel_path)







I have created a document related to the SOP, in which I prepared 50 questions divided evenly across different types:

Factual: 10

Procedural: 10

Inferential: 10

Conceptual: 10

Reasoning-based: 10

I used an automated retriever strategy pipeline to generate responses for these questions. Overall, the responses look satisfactory to me. However, I have observed a few key points:

When the input question is phrased correctly, the system returns accurate and relevant answers. If the question is not well-formed or deviates from the expected format, the system fails to respond effectively.

For factual questions, the accuracy is high—around 97%, and the answers are precise.

For procedural questions, the system also performs well, with an accuracy of approximately 95%, often providing exact steps and additional clarifications.

For inferential, conceptual, and reasoning-based questions, the responses are generally good. However, some answers tend to be generalized rather than specific, which slightly affects their usefulness.

Let me know if you'd like to present this as an email update or in a report format.






Is this conversation helpful so far?







Tools



ChatGPT can make mistakes. Chec




import fitz  # PyMuPDF
import pandas as pd
from tqdm import tqdm
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# ---------- Configuration ----------
PDF_PATH = "your_financial_filing.pdf"
EXCEL_OUTPUT_PATH = "mcq_financial_questions.xlsx"

# Azure OpenAI settings
AZURE_DEPLOYMENT_NAME = "your-deployment-name"
AZURE_API_VERSION = "2023-05-15"
AZURE_ENDPOINT = "https://your-resource-name.openai.azure.com"
AZURE_API_KEY = "your-azure-openai-key"

# ---------- LLM Init ----------
llm = AzureChatOpenAI(
    deployment_name=AZURE_DEPLOYMENT_NAME,
    openai_api_version=AZURE_API_VERSION,
    openai_api_key=AZURE_API_KEY,
    openai_api_base=AZURE_ENDPOINT,
    temperature=0.7,
)

# ---------- Prompt Template ----------
prompt_template = """
You are a financial assistant. Based on the following passage from a financial filing, generate:

Multiple-choice Question (MCQ) that require reasoning.

4 answer options (A-D), only one of which is correct.

The correct answer letter.

A short rationale for why that answer is correct.

The question should require inference or trend understanding or complex chain of thought not direct lookup.

Passage:
\"\"\"
{paragraph}
\"\"\"

Output format:

Q: <Question text>

A:

A: <option A>

B: <option B>

C: <option C>

D: <option D>

Correct answer: <A/B/C/D>

Rationale: <why it's correct>
"""

template = PromptTemplate(input_variables=["paragraph"], template=prompt_template)
llm_chain = LLMChain(llm=llm, prompt=template)

# ---------- Extract PDF Text ----------
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    return [page.get_text() for page in doc]

# ---------- Parse MCQ Output ----------
def parse_response(response):
    lines = response.strip().split("\n")
    result = {"Question": "", "Option A": "", "Option B": "", "Option C": "", "Option D": "", "Correct Answer": "", "Rationale": ""}
    
    for line in lines:
        if line.startswith("Q:"):
            result["Question"] = line[2:].strip()
        elif line.startswith("A:") and result["Option A"] == "":
            result["Option A"] = line[2:].strip()
        elif line.startswith("B:"):
            result["Option B"] = line[2:].strip()
        elif line.startswith("C:"):
            result["Option C"] = line[2:].strip()
        elif line.startswith("D:"):
            result["Option D"] = line[2:].strip()
        elif line.startswith("Correct answer:"):
            result["Correct Answer"] = line.split(":")[1].strip()
        elif line.startswith("Rationale:"):
            result["Rationale"] = line.split(":", 1)[1].strip()
    return result

# ---------- Main Function ----------
def generate_mcqs_from_pdf(pdf_path):
    pages = extract_pdf_text(pdf_path)
    all_data = []

    for idx, page_text in enumerate(tqdm(pages, desc="Generating MCQs")):
        if len(page_text.strip()) < 100:
            continue
        try:
            response = llm_chain.predict(paragraph=page_text.strip())
            print(f"\nLLM Response (Page {idx+1}):\n{response}\n")
            parsed = parse_response(response)
            parsed["Page"] = idx + 1
            all_data.append(parsed)
        except Exception as e:
            print(f"Error on page {idx+1}: {e}")

    df = pd.DataFrame(all_data)
    df.to_excel(EXCEL_OUTPUT_PATH, index=False)
    print(f"✅ MCQs saved to: {EXCEL_OUTPUT_PATH}")

# ---------- Run ----------
if __name__ == "__main__":
    generate_mcqs_from_pdf(PDF_PATH)









import fitz  # PyMuPDF
import openai
import pandas as pd
from tqdm import tqdm

# ---------- Configuration ----------
PDF_PATH = "your_financial_filing.pdf"
EXCEL_OUTPUT_PATH = "mcq_financial_questions.xlsx"
OPENAI_API_KEY = "your-api-key"
MODEL = "gpt-4o"  # Or use Azure endpoint and model name
USE_AZURE = False  # Set to True if using Azure OpenAI

# ---------- Prompt Template ----------
PROMPT_TEMPLATE = """
You are a financial assistant. Based on the following passage from a financial filing, generate:

Multiple-choice Question (MCQ) that require reasoning.

4 answer options (A-D), only one of which is correct.

The correct answer letter.

A short rationale for why that answer is correct.

The question should require inference or trend understanding or complex chain of thought not direct lookup.

Passage:
\"\"\"
{page_text}
\"\"\"

Output format:

Q: <Question text>

A:

A: <option A>

B: <option B>

C: <option C>

D: <option D>

Correct answer: <A/B/C/D>

Rationale: <why it's correct>
"""

# ---------- Extract PDF Text ----------
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    return [page.get_text() for page in doc]

# ---------- LLM Call ----------
def call_llm(prompt):
    if USE_AZURE:
        openai.api_type = "azure"
        openai.api_base = "https://<your-azure-endpoint>.openai.azure.com/"
        openai.api_version = "2023-05-15"
        openai.api_key = OPENAI_API_KEY

        response = openai.ChatCompletion.create(
            engine="your-deployment-name",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
    else:
        openai.api_key = OPENAI_API_KEY
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )

    return response['choices'][0]['message']['content']

# ---------- Parse LLM Response ----------
def parse_response(response):
    lines = response.strip().split("\n")
    result = {"Question": "", "Option A": "", "Option B": "", "Option C": "", "Option D": "", "Correct Answer": "", "Rationale": ""}
    
    for line in lines:
        if line.startswith("Q:"):
            result["Question"] = line[2:].strip()
        elif line.startswith("A:") and result["Option A"] == "":
            result["Option A"] = line[2:].strip()
        elif line.startswith("B:"):
            result["Option B"] = line[2:].strip()
        elif line.startswith("C:"):
            result["Option C"] = line[2:].strip()
        elif line.startswith("D:"):
            result["Option D"] = line[2:].strip()
        elif line.startswith("Correct answer:"):
            result["Correct Answer"] = line.split(":")[1].strip()
        elif line.startswith("Rationale:"):
            result["Rationale"] = line.split(":", 1)[1].strip()
    return result

# ---------- Main Execution ----------
def generate_mcqs_from_pdf(pdf_path):
    pages = extract_pdf_text(pdf_path)
    all_data = []

    for idx, page_text in enumerate(tqdm(pages, desc="Processing Pages")):
        if len(page_text.strip()) < 100:
            continue  # Skip blank/empty pages
        prompt = PROMPT_TEMPLATE.format(page_text=page_text.strip())
        try:
            response = call_llm(prompt)
            parsed = parse_response(response)
            parsed["Page"] = idx + 1
            all_data.append(parsed)
        except Exception as e:
            print(f"Error on page {idx+1}: {e}")

    df = pd.DataFrame(all_data)
    df.to_excel(EXCEL_OUTPUT_PATH, index=False)
    print(f"✅ MCQs saved to: {EXCEL_OUTPUT_PATH}")

# ---------- Run ----------
if __name__ == "__main__":
    generate_mcqs_from_pdf(PDF_PATH)







After creating the app in the predev environment, an error message is displayed during the approval process: 'Not approved'. Requesting your assistance in looking into this issue.




I have completed generating original question-answer pairs for three domains. Additionally, yesterday I generated diverse question-answer pairs for the Pre-NAV Analysis domain, resulting in 40,000 samples. This morning, I started generating diverse questions and answers for the Trade Review domain.




While onboarding in the predev environment, the Retriever strategies are not showing up—it displays 'No data'. Could someone please look into this?"





# 2. Ask each question and collect answer
for question in questions:
    try:
        conv_payload = {"userQuery": question}
        conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
        conv_response.raise_for_status()

        response_json = conv_response.json()
        bot_response = response_json.get("botResponse", "")  # Only botResponse
        full_response = json.dumps(response_json)            # Entire raw JSON as string

    except Exception as e:
        bot_response = f"Error: {str(e)}"
        full_response = bot_response

    all_results.append({
        "Retriever Strategy": strategy_name,
        "Retriever Config": strategy_config,
        "Question": question,
        "Bot Response": bot_response,
        "Full Raw Response": full_response
    })






import torch
from torch.distributed import init_process_group, destroy_process_group
from torch.distributed._tensor import DeviceMesh, DTensor

# Initialize process group (required before creating device mesh)
init_process_group(backend="nccl")  # Or "gloo" for CPU testing

# Define a device mesh (e.g., for 2 GPUs)
device_mesh = DeviceMesh("cuda", list(range(torch.cuda.device_count()))[:2])

# Local tensor
local_tensor = torch.randn(4, 4).cuda()

# Distribute the tensor
dtensor = DTensor.from_local(local_tensor, device_mesh, placements=["replicate"])

print(dtensor)

# Clean up
destroy_process_group()
Hi Team,

The retriever strategy evaluation and corresponding configurations are stored in the Box folder shared earlier. I'm still working on this part.

For initial testing, I’m using a different question. I will now proceed to create the app, set up the data group in the predev environment, and fetch the results.

I'll use the six questions provided by the client for testing. Additionally, I will update the Excel sheet today to include the following details: retriever strategy, strategy configuration, question, and corresponding answer








import requests
import json
import copy
import pandas as pd

# --- Constants ---
MGMT_BASE_URL = "http://10.76.22.174:10005"
CONV_BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

UPDATE_ENDPOINT = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
CONV_ENDPOINT = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

HEADERS = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ADMIN_TOKEN"
}

# --- Base app metadata (everything except retrieverConfig) ---
BASE_METADATA = {
    "title": "Fund Accounting Q&A App",
    "prompt": "You are a fund accounting expert. Answer concisely.",
    "description": "Q&A for finance, investment, and NAV-related questions.",
    "category": "Finance",
    "metadata": {},
    "vectorStore": {},
    "introduction": "Welcome to the Fund Accounting LLM assistant.",
    "icon": "",
    "raiclient": {
        "eamClientId": "sample-client-id",
        "raiClientId": "sample-rai-client-id",
        "generative": {
            "model": "gpt-4",
            "params": {},
            "n": 1,
            "temperature": 0.7
        },
        "embedding": {
            "model": "embedding-model",
            "params": {}
        },
        "multimodel": {
            "model": "multi-model",
            "params": {}
        },
        "eamClientSecret": "sample-client-secret"
    },
    "entitlements": {}
}

# --- Load 50 retriever strategies from JSON file ---
with open("50_final_corrected_strategies.json", "r") as f:
    RETRIEVER_STRATEGY = json.load(f)

# --- Sample Questions ---
questions = [
    "What is NAV in fund accounting?",
    "What is the role of a transfer agent?",
    "How is tracking error calculated?",
    "Explain risk-adjusted return.",
    "How does portfolio diversification reduce risk?"
]

# --- Final results container ---
all_results = []

# --- Loop through each retriever strategy ---
for strategy in RETRIEVER_STRATEGY["items"]:
    strategy_name = strategy["name"]
    strategy_config = json.dumps(strategy, indent=2)  # Full strategy including name

    # 1. Update app metadata with this retriever config
    payload = copy.deepcopy(BASE_METADATA)
    payload["retrieverConfig"] = strategy

    print(f"\n[Updating retriever config: {strategy_name}]")
    try:
        update_response = requests.post(UPDATE_ENDPOINT, headers=HEADERS, json=payload, timeout=30)
        print("Update Status:", update_response.status_code)
    except Exception as e:
        print("Failed to update metadata:", str(e))
        continue

    # 2. Ask each question and collect answer
    for question in questions:
        try:
            conv_payload = {"userQuery": question}
            conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
            conv_response.raise_for_status()
            try:
                answer = conv_response.json().get("botResponse", json.dumps(conv_response.json()))
            except:
                answer = conv_response.text
        except Exception as e:
            answer = f"Error: {str(e)}"

        all_results.append({
            "Retriever Strategy": strategy_name,
            "Retriever Config": strategy_config,
            "Question": question,
            "Answer": answer
        })

# --- Save all results to Excel ---
df = pd.DataFrame(all_results)
df.to_excel("retriever_strategy_QA_output.xlsx", index=False)
print("\nSaved results to retriever_strategy_QA_output.xlsx")










# Extend the user's provided 5 strategies into 30 by copying and varying the configuration values

base_strategies = [
    {
        "name": "MultiQueryRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "prompt": (
                    "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives.\n"
                    "The user questions are focused on the documents that have been provided with tables, and text data.\n"
                    "Each query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results. "
                    "Provide these alternative questions separated by newlines Original question: (question)"
                ),
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    },
    {
        "name": "SelfQueryRetriever",
        "config": {
            "kwargs": {
                "score_threshold": 0.8,
                "document_contents": "The document contains tabular and textual information",
                "metadata_field_info": [
                    {"name": "dsId", "type": "string", "description": "The id of the datasource where the document is stored"},
                    {"name": "docId", "type": "string", "description": "An id for the document"},
                    {"name": "fileName", "type": "string", "description": "Name of the file"}
                ]
            },
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    },
    {
        "name": "ContextualCompressionRetriever",
        "config": {
            "kwargs": {
                "filters": "llmextractor",
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "embedding_model": "azure-openai.text-embedding-ada-002",
            "similarity_threshold": 0.76
        }
    },
    {
        "name": "EnsembleRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "weights": [0.9, 0.1],
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "numberOfBM25Documents": 5
        }
    },
    {
        "name": "NaiveRetriever",
        "config": {
            "top_k": 3,
            "kwargs": {
                "score_threshold": 0.8
            },
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            }
        }
    }
]

# Generate 30 variants
all_strategies = []
for i in range(30):
    base = base_strategies[i % len(base_strategies)]
    new_strategy = {
        "name": f"{base['name']}_{i+1}",
        "config": base["config"].copy()
    }
    config = new_strategy["config"]

    # Randomize configuration values
    config["top_k"] = random.choice([3, 5, 7, 10]) if "top_k" in config else 3
    config["search_type"] = random.choice(["similarity", "bm25", "hybrid"]) if "search_type" in config else "similarity"
    config["searchConfig"]["activateHybridSearch"] = random.choice([True, False])
    config["searchConfig"]["numberOfTextBasedResults"] = random.choice([3, 5, 7, 10])
    config["rerankerConfig"]["activateReranker"] = random.choice([True, False])
    config["rerankerConfig"]["numberOfRerankedResults"] = random.choice([3, 5, 7, 10])
    config["kwargs"]["score_threshold"] = round(random.uniform(0.6, 0.9), 2)

    all_strategies.append(new_strategy)

# Convert to DataFrame for preview
df_30_strategies = pd.DataFrame(all_strategies)
from ace_tools import display_dataframe_to_user
display_dataframe_to_user(name="30 Final Retriever Strategies", dataframe=df_30_strategies)





import requests
import json
import copy
import pandas as pd

# --- Constants ---
MGMT_BASE_URL = "http://10.76.22.174:10005"
CONV_BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

UPDATE_ENDPOINT = f"{MGMT_BASE_URL}/agi/mgmt/api/v1/apps/{APP_ID}"
CONV_ENDPOINT = f"{CONV_BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

HEADERS = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ADMIN_TOKEN"
}

# --- Fixed app metadata (other than retrieverConfig) ---
BASE_METADATA = {
    "title": "Fund Accounting Q&A App",
    "prompt": "You are a fund accounting expert. Answer concisely.",
    "description": "Q&A for finance, investment, and NAV-related questions.",
    "category": "Finance",
    "metadata": {},
    "vectorStore": {},
    "introduction": "Welcome to the Fund Accounting LLM assistant.",
    "icon": "",
    "raiclient": {
        "eamClientId": "sample-client-id",
        "raiClientId": "sample-rai-client-id",
        "generative": {
            "model": "gpt-4",
            "params": {},
            "n": 1,
            "temperature": 0.7
        },
        "embedding": {
            "model": "embedding-model",
            "params": {}
        },
        "multimodel": {
            "model": "multi-model",
            "params": {}
        },
        "eamClientSecret": "sample-client-secret"
    },
    "entitlements": {}
}

# --- Questions to send after each retriever update ---
questions = [
    "What is NAV in fund accounting?",
    "What is the role of a transfer agent?",
    "How is tracking error calculated?",
    "Explain risk-adjusted return.",
    "How does portfolio diversification reduce risk?"
]

# --- Retriever Strategies ---
RETRIEVER_STRATEGY = {
    "items": [
        {
            "name": "MultiQueryRetriever",
            "config": {
                "top_k": 3,
                "kwargs": {
                    "prompt": "Generate 3 different search queries from different angles.",
                    "score_threshold": 0.8
                },
                "search_type": "similarity",
                "searchConfig": {
                    "activateHybridSearch": False,
                    "numberOfTextBasedResults": 4
                },
                "rerankerConfig": {
                    "activateReranker": False,
                    "numberOfRerankedResults": 3
                }
            }
        },
        {
            "name": "HybridRetriever",
            "config": {
                "top_k": 5,
                "search_type": "hybrid",
                "searchConfig": {
                    "activateHybridSearch": True,
                    "numberOfTextBasedResults": 5
                },
                "rerankerConfig": {
                    "activateReranker": True,
                    "numberOfRerankedResults": 5
                },
                "kwargs": {
                    "prompt": "Try different angles for query generation.",
                    "score_threshold": 0.7
                }
            }
        }
    ]
}

# --- Final results ---
all_results = []

# --- Loop: update retriever + send questions ---
for strategy in RETRIEVER_STRATEGY["items"]:
    strategy_name = strategy["name"]

    # 1. Update App Metadata with current retriever strategy
    payload = copy.deepcopy(BASE_METADATA)
    payload["retrieverConfig"] = strategy

    print(f"\n[Updating retriever config: {strategy_name}]")
    update_response = requests.post(UPDATE_ENDPOINT, headers=HEADERS, json=payload, timeout=30)
    print("Update Status:", update_response.status_code)

    # 2. Run questions using updated retriever config
    for question in questions:
        try:
            conv_payload = { "userQuery": question }
            conv_response = requests.post(CONV_ENDPOINT, headers=HEADERS, json=conv_payload, timeout=30)
            conv_response.raise_for_status()

            try:
                answer = conv_response.json()
            except:
                answer = {"error": conv_response.text}

        except Exception as e:
            answer = {"error": str(e)}

        all_results.append({
            "Retriever Strategy": strategy_name,
            "Question": question,
            "Answer": json.dumps(answer)
        })

# --- Save final output to Excel ---
df = pd.DataFrame(all_results)
df.to_excel("retriever_strategy_QA_output.xlsx", index=False)
print("\nSaved results to retriever_strategy_QA_output.xlsx")






import requests
import pandas as pd

# API endpoint
ASK_ENDPOINT = "https://rai.it.statestr.com:4327/agi/query"

# Optional headers (include token if needed)
headers = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_ACCESS_TOKEN"
}

# Application ID
app_id = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"

# Sample questions
questions = [
    "What is NAV in fund accounting?",
    "Explain the role of a custodian.",
    "What is tracking error in passive investment?",
]

# Define multiple retriever strategies
retriever_strategies = [
    {
        "name": "MultiQuery Retriever",
        "config": {
            "top_k": 3,
            "search_type": "similarity",
            "searchConfig": {
                "activateHybridSearch": False,
                "numberOfTextBasedResults": 4
            },
            "rerankerConfig": {
                "activateReranker": False,
                "numberOfRerankedResults": 3
            },
            "kwargs": {
                "prompt": "Generate queries from multiple viewpoints.",
                "score_threshold": 0.8
            }
        }
    },
    {
        "name": "BM25 Retriever",
        "config": {
            "top_k": 3,
            "search_type": "bm25",
            "searchConfig": {
                "activateHybridSearch": False
            },
            "rerankerConfig": {
                "activateReranker": False
            },
            "kwargs": {}
        }
    },
    {
        "name": "Hybrid Retriever",
        "config": {
            "top_k": 3,
            "search_type": "hybrid",
            "searchConfig": {
                "activateHybridSearch": True,
                "numberOfTextBasedResults": 3
            },
            "rerankerConfig": {
                "activateReranker": True,
                "numberOfRerankedResults": 3
            },
            "kwargs": {
                "score_threshold": 0.75
            }
        }
    }
]

# Collect results
results = []

for strategy in retriever_strategies:
    strategy_name = strategy["name"]
    
    for question in questions:
        payload = {
            "question": question,
            "appId": app_id,
            "retriever_config": strategy
        }

        try:
            response = requests.post(ASK_ENDPOINT, headers=headers, json=payload, timeout=30, verify=False)
            response.raise_for_status()
            answer = response.json().get("answer", "No answer found")
        except Exception as e:
            answer = f"Error: {str(e)}"

        results.append({
            "Retriever": strategy_name,
            "Question": question,
            "Answer": answer
        })

# Export to Excel
df = pd.DataFrame(results)
df.to_excel("multi_strategy_answers.xlsx", index=False)
print("Saved results to multi_strategy_answers.xlsx")









import requests
import pandas as pd

# Constants
BASE_URL = "http://10.76.22.174:8081"
APP_ID = "cf7938a6-4deb-414c-89d0-d5370cb0fe0f"
ENDPOINT = f"{BASE_URL}/agi/cvs/api/v1/apps/{APP_ID}/conversations"

# Headers (add Authorization if needed)
headers = {
    "Content-Type": "application/json"
    # "Authorization": "Bearer YOUR_TOKEN"  # Uncomment if API is protected
}

# List of questions
questions = [
    "What is NAV in fund accounting?",
    "Explain the role of a transfer agent.",
    "What is tracking error in investment portfolios?",
    "Define risk-adjusted return.",
    "How does portfolio diversification work?"
]

results = []

for question in questions:
    payload = {
        "userQuery": question
    }

    try:
        response = requests.post(ENDPOINT, headers=headers, json=payload, timeout=30)
        response.raise_for_status()

        # Print and store result
        print("Response:", response.json())
        results.append({
            "Question": question,
            "Answer": response.json()
        })

    except Exception as e:
        print("Error:", str(e))
        results.append({
            "Question": question,
            "Answer": f"Error: {str(e)}"
        })

# Save to Excel
df = pd.DataFrame(results)
df.to_excel("conversations_output.xlsx", index=False)
print("Saved results to conversations_output.xlsx")








The original question-answer pairs for the discrimination report domain have been generated. The files are available at the specified location
"I have started generating original question-answer pairs for the Fund Accounting domain."






import pandas as pd
import requests

# Step 1: Define retriever strategies in the new format
retriever_strategy = {
    "items": [
        {
            "name": "MultiQuery Retriever",
            "config": {
                "top_k": 3,
                "kwargs": {
                    "prompt": "Your task is to generate 3 different search queries that aim to answer the user question from multiple perspectives. The user questions are focused on the documents that have been provided with tables and text data. Each query MUST tackle the question from a different viewpoint. We want to get a variety of RELEVANT search results. Provide these alternative questions separated by newlines.\nOriginal question: (question)",
                    "score_threshold": 0.8
                },
                "search_type": "similarity",
                "searchConfig": {
                    "activateHybridSearch": False,
                    "numberOfTextBasedResults": 4
                },
                "rerankerConfig": {
                    "activateReranker": False,
                    "numberOfRerankedResults": 3
                }
            }
        }
        # You can add more strategies to this list
    ]
}

# Step 2: Define evaluation questions
questions = [
    "What is the role of fiduciary duties?",
    "Explain the concept of risk-adjusted returns.",
    "How does SSGA manage index tracking error?",
    "What is the purpose of an investment policy statement?",
    "Describe the difference between active and passive management."
]

# Step 3: Replace with your actual deployed API endpoint
base_url = "http://your-dev-app/api/ask"

# Step 4: Query and collect responses
all_results = []

for question in questions:
    row = {"question": question}
    for strategy in retriever_strategy["items"]:
        try:
            payload = {
                "question": question,
                "retriever_config": strategy
            }
            response = requests.post(base_url, json=payload, timeout=30)
            result = response.json().get("answer", "No response")
        except Exception as e:
            result = f"Error: {str(e)}"
        row[strategy["name"].replace(" ", "_").lower() + "_response"] = result
    all_results.append(row)

# Step 5: Save results to Excel
df = pd.DataFrame(all_results)
df.to_excel("ssgenai_rag_retriever_results.xlsx", index=False)
print("Results saved to ssgenai_rag_retriever_results.xlsx")






row[strategy["name"].replace




import pandas as pd
import requests

# Step 1: Define retriever configurations
retriever_configs = {
    "config_1": {"retriever_type": "faiss", "top_k": 3, "mode": "similarity"},
    "config_2": {"retriever_type": "faiss", "top_k": 5, "mode": "mmr"},
    "config_3": {"retriever_type": "bm25", "top_k": 3, "k1": 1.2, "b": 0.75},
    "config_4": {"retriever_type": "bm25", "top_k": 5, "k1": 2.0, "b": 1.0},
    "config_5": {"retriever_type": "multiquery", "top_k": 5, "merge_strategy": "concatenate", "llm_variant": "gpt-3.5"},
}

# Step 2: Define evaluation questions
questions = [
    "What is the role of fiduciary duties?",
    "Explain the concept of risk-adjusted returns.",
    "How does SSGA manage index tracking error?",
    "What is the purpose of an investment policy statement?",
    "Describe the difference between active and passive management."
]

# Step 3: Replace this with your actual dev endpoint
base_url = "http://your-dev-app/api/ask"

# Step 4: Collect responses
all_results = []

for q in questions:
    row = {"question": q}
    for config_id, config in retriever_configs.items():
        try:
            payload = {
                "question": q,
                "retriever_config": config
            }
            response = requests.post(base_url, json=payload, timeout=30)
            result = response.json().get("answer", "No response")
        except Exception as e:
            result = f"Error: {str(e)}"
        row[config_id + "_response"] = result
    all_results.append(row)

# Step 5: Export to Excel
df = pd.DataFrame(all_results)
df.to_excel("retriever_comparison_results.xlsx", index=False)
print("Saved results to retriever_comparison_results.xlsx")






import pandas as pd
import tiktoken

# Function to calculate tokens using tiktoken
def calculate_tokens(text, model="gpt-3.5-turbo"):
    """
    Calculate the number of tokens in a given text using tiktoken.

    Args:
        text (str): Input text.
        model (str): Model name to determine the encoding (default: "gpt-3.5-turbo").

    Returns:
        int: Number of tokens in the text.
    """
    try:
        if model in ["gpt-3.5-turbo", "gpt-4"]:
            encoding = tiktoken.get_encoding("cl100k_base")
        elif model == "text-davinci-003":
            encoding = tiktoken.get_encoding("p50k_base")
        else:
            raise ValueError(f"Unsupported model: {model}")

        return len(encoding.encode(text))
    except Exception as e:
        print(f"Error calculating tokens: {e}")
        return 0

# Load the Excel file
df = pd.read_excel("your_input_file.xlsx")  # Change to your actual file path

# Calculate tokens for each Answer
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: calculate_tokens(str(x)) if pd.notna(x) else 0)

# Export the updated DataFrame to a new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)





import pandas as pd
import tiktoken  # for token counting

# Load Excel file
df = pd.read_excel("your_file.xlsx")  # Update with your file path

# Choose tokenizer (e.g., for OpenAI's gpt-3.5-turbo or gpt-4)
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

# Count tokens in 'Answer' column
df['Answer_Token_Count'] = df['Answer'].apply(lambda x: len(encoding.encode(str(x))) if pd.notna(x) else 0)

# Save to new Excel file
df.to_excel("output_with_token_counts.xlsx", index=False)



On the call, you can ask me different scenarios, and I will demonstrate them. If it's not up to your expectations, I will stop and work on it. I'm requesting the call because I want to avoid facing similar issues again in the future.





For testing, I injected a document related to the SOP. After the injection, I asked a few questions based on that document, but it did not return the answers. Instead, it gave the following response to the questions:


For testing the procedural questions, I have rerun the pipeline for the Discrimination Report domain. The QA pairs were generated successfully, and I am currently verifying the answers to the procedural questions




# LLaMA-3.1-8B-Instruct Fine-Tuning with LoRA and MLflow

# Step 1: Environment Setup
import random, os, torch, numpy as np, pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
SEED = 42
seed_everything(SEED)

# Step 2: Load Tokenizer and Model
MODEL_PATH = "/path/to/Llama-3.1-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, model_max_length=1024, padding_side="left", truncation=True, padding=True
)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
model.resize_token_embeddings(len(tokenizer))

# Step 3: Load and Format Dataset
df = pd.read_csv("qa_dataset.csv")[["Question", "Answer"]].dropna().sample(frac=1).reset_index(drop=True)
def format_example(row):
    messages = [
        {"role": "system", "content": "Answer the question"},
        {"role": "user", "content": row["Question"]},
        {"role": "assistant", "content": row["Answer"]}
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False)
df["text"] = df.apply(format_example, axis=1)

# Step 4: Token Count Check
def count_tokens(row):
    return len(tokenizer(row["text"], add_special_tokens=True)["input_ids"])
df["token_count"] = df.apply(count_tokens, axis=1)

# Step 5: Dataset Split and Convert to HF Format
val, test = train_test_split(df, test_size=0.2, random_state=SEED)
dataset = DatasetDict({
    "train": Dataset.from_pandas(df),
    "validation": Dataset.from_pandas(val),
    "test": Dataset.from_pandas(test)
})

# Step 6: Configure LoRA
from peft import LoraConfig, TaskType, get_peft_model
lora_config = LoraConfig(
    r=64, lora_alpha=128, target_modules="all-linear", lora_dropout=0.1,
    bias="none", task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# Step 7: Define Training Config with MLflow
from trl import SFTConfig
OUTPUT_DIR = "/path/to/output"
sft_config = SFTConfig(
    output_dir=OUTPUT_DIR, dataset_text_field="text", num_train_epochs=4,
    max_seq_length=1024, gradient_checkpointing=False, per_device_train_batch_size=2,
    per_device_eval_batch_size=2, gradient_accumulation_steps=4, learning_rate=1e-4,
    fp16=True, eval_strategy="steps", logging_steps=10, save_strategy="steps",
    save_steps=0.2, eval_steps=0.2, lr_scheduler_type="cosine", report_to="mlflow", seed=SEED
)

# Step 8: Train with SFTTrainer and Log to MLflow
from trl import SFTTrainer
import mlflow
with mlflow.start_run():
    trainer = SFTTrainer(
        model=model,
        args=sft_config,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        tokenizer=tokenizer
    )
    trainer.train()
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    mlflow.log_params(sft_config.to_dict())
    mlflow.log_artifact(OUTPUT_DIR)

# Step 9: Merge LoRA Adapters
from peft import PeftModel
peft_model = PeftModel.from_pretrained(model, OUTPUT_DIR)
merged_model = peft_model.merge_and_unload()

# Step 10: Register Final Model to MLflow
from transformers import pipeline
mlflow.transformers.log_model(
    transformers_model=pipeline("text-generation", model=merged_model, tokenizer=tokenizer),
    artifact_path="merged_model_without_model_config",
    task="llm/v1/chat"
)

# Step 11: Run Inference Example
mlflow_model = mlflow.pyfunc.load_model("runs:/<run_id>/merged_model_without_model_config")
mlflow_model.predict({
    "messages": [
        {"role": "system", "content": "Answer the question."},
        {"role": "user", "content": "What is LoRA?"}
    ],
    "temperature": 0.5,
    "max_tokens": 100
})

# Step 12: What is LoRA?
# LoRA (Low-Rank Adaptation) updates a small set of trainable matrices A and B:
# W' = W + A @ B
# W = frozen weight matrix
# A = [r x d], B = [d x k], where r is much smaller
# Benefits: 90%+ parameter savings, low memory, no catastrophic forgetting, modular
# Use merge_and_unload() to finalize model after training






### LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)

---

#### 1. Environment Initialization

* **Purpose**: Set up the Python environment and load libraries.
* **Tools**: `transformers`, `datasets`, `torch`, `pandas`, `mlflow`, `seaborn`, `matplotlib`
* **Action**: Import required packages, set a global random seed (`SEED = 42`), and configure plotting.

---

#### 2. Load Tokenizer and Model

* **Purpose**: Initialize tokenizer and quantized base model for fine-tuning.
* **Tools**: `AutoTokenizer`, `AutoModelForCausalLM`
* **Action**:

  * Load tokenizer from the base model path.
  * Set `pad_token` = `eos_token`, padding side = "left" (required for causal LMs).
  * Load the quantized model (e.g., 4-bit with `BitsAndBytesConfig` if required).

---

#### 3. Dataset Preparation

* **Purpose**: Read, clean, and prepare data for fine-tuning.
* **Tools**: `pandas`
* **Action**:

  * Load QA dataset from CSV.
  * Keep only `Question` and `Answer` columns.
  * Drop missing values and shuffle the rows.

---

#### 4. Format Prompts for Chat

* **Purpose**: Convert QA rows into structured multi-turn chat format for LLM training.
* **Tools**: `tokenizer.apply_chat_template()`
* **Action**:

  * Construct `[system → user → assistant]` style chat messages.
  * Generate text prompts for training using tokenizer templates.

---

#### 5. Analyze Token Length

* **Purpose**: Ensure inputs fit within model's `max_seq_length` (1024 tokens).
* **Tools**: `tokenizer`, `matplotlib`, `PercentFormatter`
* **Action**:

  * Tokenize each example to count tokens.
  * Plot histograms to visualize token distribution.

---

#### 6. Split Dataset

* **Purpose**: Create training, validation, and test sets.
* **Tools**: `train_test_split` from `sklearn`
* **Action**:

  * Split into 80% train and 20% validation sets.
  * Convert them into HuggingFace `Dataset` objects using `Dataset.from_pandas()`.

---

#### 7. Configure PEFT (LoRA)

* **Purpose**: Reduce memory cost using parameter-efficient fine-tuning.
* **Tools**: `peft.LoraConfig`, `get_peft_model()`
* **Action**:

  * Apply LoRA only on linear layers.
  * Configure rank (`r`), alpha, dropout, and target modules.
  * Prepare model for training.

---

#### 8. Define Training Configuration

* **Purpose**: Set training hyperparameters and logging behavior.
* **Tools**: `trl.SFTConfig`
* **Key Settings**:

  * `max_seq_length = 1024`
  * `batch_size = 2`
  * `gradient_accumulation = 4`
  * `learning_rate = 1e-4`
  * `eval_steps = 0.2`, `save_steps = 0.2`
  * `fp16 = True`, `lr_scheduler = cosine`
  * `report_to = "mlflow"`

---

#### 9. Train Model

* **Purpose**: Fine-tune the base model with LoRA layers.
* **Tools**: `SFTTrainer`
* **Action**:

  * Train on the `train` set, evaluate on `validation`.
  * Save the model and tokenizer to `OUTPUT_DIR`.
  * Log everything to MLflow inside `with mlflow.start_run()`.

---

#### 10. Merge LoRA with Base Model

* **Purpose**: Convert adapter-based model into a standalone fine-tuned model.
* **Tools**: `PeftModel`, `merge_and_unload()`
* **Action**:

  * Load trained LoRA model.
  * Merge adapters into base model to remove dependency on LoRA.

---

#### 11. Register and Log Model with MLflow

* **Purpose**: Store, version, and serve model in production.
* **Tools**: `mlflow.transformers.log_model()`, `mlflow.register_model()`
* **Action**:

  * Log the `pipeline("text-generation")` model.
  * Register it to Databricks model registry.

---

#### 12. Copy Artifacts to UC Volume (Optional)

* **Purpose**: Store model artifacts in a Unified Catalog (UC) for sharing or backup.
* **Tools**: `mlflow.tracking.MlflowClient()`
* **Action**:

  * Download all model artifacts and copy to desired UC directory path.

---

#### 13. Inference

* **Purpose**: Test the model with a prompt.
* **Tools**: `mlflow_model.predict()`
* **Input**: JSON with `messages` (system, user), `temperature`, and `max_tokens`
* **Output**: Model-generated response.



LLaMA-3.1-8B-Instruct Fine-Tuning Pipeline (PEFT + MLflow)
Environment Setup:
Import all required libraries (transformers, datasets, torch, mlflow, etc.).

Tokenizer & Model Loading:
Load the tokenizer and quantized LLaMA model. Set pad token as EOS and padding side as "left" for causal LM.

Dataset Loading:
Read the question-answer dataset from a CSV file. Drop nulls, shuffle, and retain relevant columns.

Prompt Formatting:
Format data into structured "chat" format (system, user, assistant roles) using the tokenizer’s chat template.

Token Count Analysis:
Calculate and visualize token lengths to ensure most samples are under 1024 tokens (model limit).

Train/Validation Split:
Use train_test_split() to divide data into training and validation sets for evaluation.

Dataset Conversion:
Convert Pandas DataFrames to HuggingFace DatasetDict for training compatibility.

PEFT (LoRA) Configuration:
Define LoraConfig to fine-tune only a small subset of layers efficiently using LoRA adapters.

Training Configuration:
Set hyperparameters like batch size, epochs, learning rate, warmup ratio, and enable MLflow tracking.

Model Training:
Fine-tune the model using SFTTrainer with chat-formatted inputs. Log metrics and artifacts via MLflow.

Merge Adapters:
After training, merge LoRA adapters with the base model to create a fully functional fine-tuned model.

Model Logging:
Log the merged model to MLflow using transformers.log_model() for future deployment.

Model Registration:
Register the model version in Databricks MLflow Registry with a custom name.

Artifact Copy (Optional):
Download and copy model artifacts from MLflow run to a Unified Catalog directory.

Inference:
Load the logged model and run inference using mlflow_model.predict() with system-user prompts.

Let me know if you want this exported to a .md or .pdf file.
















ChatGPT can make mistakes. Check important info. See Cookie Preferences.








Hi Chandreshkumar Dedani, Vivek. The results from the previous run were good. I have now changed the dataset to include 3 diverse procedural questions. Please use this dataset for your training, and also use the parameters: context length = 1024 and epochs = 4."






import pandas as pd

# Load your Excel or CSV file
df = pd.read_excel('your_dataset.xlsx')  # or use read_csv if it's a CSV

# Group by the 'Original Question' column and select the first 3 entries per group
filtered_df = df.groupby('Original Question', group_keys=False).head(3)

# Save the result if needed
filtered_df.to_excel('filtered_output.xlsx', index=False)

print("Filtered dataset with first 3 questions per original question.")





import json
import pandas as pd

# Step 1: Load JSON file
with open('your_file.json', 'r') as file:
    data = json.load(file)

# Step 2: Normalize (flatten) the JSON data
df = pd.json_normalize(data, sep='_')

# Step 3: Save to Excel
df.to_excel('converted_output.xlsx', index=False)

print("Conversion complete. File saved as 'converted_output.xlsx'")






import os
import time
import json
import dotenv
import pandas as pd
from langchain.chat_models import AzureChatOpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed

# Load environment variables
dotenv.load_dotenv()

# Azure LLM Configuration
azure_openai_api_version = os.getenv("AZURE_OPENAI_API_VERSION")
azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

llm = AzureChatOpenAI(
    temperature=0,
    api_version=azure_openai_api_version,
    azure_endpoint=azure_openai_endpoint,
    deployment_name="ssgpt-4"
)

# ========================== Step 1: Convert Excel to JSON ==========================

def convert_excel_to_json(file_path: str, output_json_path: str = "output_data.json") -> str:
    df = pd.read_excel(file_path, dtype=str).fillna("")
    json_data = [{"question": row["Question"], "answer": row["Answer"]} for _, row in df.iterrows()]
    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=4, ensure_ascii=False)
    print(f"Converted Excel to JSON at {output_json_path}")
    return output_json_path

# ========================== Step 2: Read JSON ==========================

def read_json_file(file_path: str) -> list:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"Read {len(data)} records from {file_path}")
        return data
    except Exception as e:
        print(f"Error reading JSON: {e}")
        return []

# ========================== Step 3: Build Prompt ==========================

def build_rating_prompt(question, answer):
    return f"""
Rate each question-answer pair on a scale from 1-10, based on:
- Accuracy (0-3): factual correctness
- Relevance (0-2): relevance to content
- Clarity (0-2): clear language
- Usefulness (0-3): value for model learning

YOU MUST RETURN A VALID JSON OBJECT WITH THIS EXACT SCHEMA:
{{
  "question": "{question}",
  "answer": "{answer}",
  "rating": <1-10>
}}

*** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE ***
"""

# ========================== Step 4: Validate with LLM ==========================

def validate_with_llm(entry, index, retries=3):
    prompt = build_rating_prompt(entry["question"], entry["answer"])
    for attempt in range(retries):
        try:
            response = llm.predict(prompt)
            result = json.loads(response)
            return index, result
        except Exception as e:
            print(f"Retry {attempt+1} failed: {e}")
            time.sleep(2)
    return index, {
        "question": entry["question"],
        "answer": entry["answer"],
        "rating": 0
    }

# ========================== Step 5: Run Pipeline ==========================

def full_pipeline(input_excel, final_excel_output, final_json_output, max_workers=5):
    temp_json_path = convert_excel_to_json(input_excel)
    qa_data = read_json_file(temp_json_path)
    results = [None] * len(qa_data)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(validate_with_llm, entry, i) for i, entry in enumerate(qa_data)]
        for future in as_completed(futures):
            index, result = future.result()
            results[index] = result

    # Save to Excel and JSON
    pd.DataFrame(results).to_excel(final_excel_output, index=False)
    with open(final_json_output, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"Pipeline complete. Results saved to {final_excel_output} and {final_json_output}")









Following up on the productivity improvement, here’s a summary of the value delivered by migrating from the Hugging Face summarization model to the new LLM-based chatbot:

1. Key Difference Observed
Previous (Hugging Face Model): Users faced delays due to slower response times and limited interactivity (only summary generation).

Current (LLM Chatbot): Delivers instant, context-aware answers through natural conversation, significantly reducing turnaround time.

2. Quantified Benefit (Illustrative Example)
Average handling time (per query):

Hugging Face: ~12 mins (includes manual reading + follow-ups)

LLM Chatbot: ~4 mins (direct answers with follow-up support)

Monthly queries handled: 2,000

FTE rate: $30/hour

Productivity Gain = (12 – 4)/60 × 2,000 × $30 = $8,000/month

3. Next Steps
I will engage with actual users to validate the above assumptions and share a data-backed benefit report.

Let me know if you'd like this structured as a slide or summary deck as well.

Best regards,
[Your Name]

Would you like me to help you turn this into a one-slide summary for stakeholders?
















ChatGPT can make mistakes. Check important info. See Cookie Preferences






To quantify productivity improvement from replacing the Hugging Face summarization model with the new LLM-based chatbot, here's the proposed approach:

1. User Experience Comparison
We’re reaching out to users to compare:

Response speed and accuracy before (summarization only) vs. now (interactive LLM chatbot).

How the LLM chatbot helps resolve end-customer queries faster and more completely.

2. Estimated Productivity Gains
Based on early indicators:

Previous model: Provided passive summaries, required human interpretation.

Current LLM chatbot: Enables direct question-answering and multi-turn conversations, reducing manual effort.

3. Quantifiable Impact (Illustrative)
Let’s assume:

Time saved per interaction: Increased from 4 mins (summarization) to 10 mins (LLM chatbot)

Monthly interactions: 2,000

FTE cost/hour: $30

Productivity Gain:
LLM chatbot: (10/60) × 2,000 × $30 = $10,000/month
Hugging Face: (4/60) × 2,000 × $30 = $4,000/month
Net gain: ~$6,000/month

We’ll validate these assumptions with real user input and share a finalized benefit report shortly.









Regarding the productivity improvement through our chatbot (built using Hugging Face summarization models), here's the approach to quantify the benefit:

User Feedback Gathering
I’ll speak with end-users to understand:

How frequently they use the chatbot.

Whether it helps them respond faster and more accurately to customer queries.

Specific tasks where the summarization feature reduces manual effort.

Productivity Quantification
Based on initial assumptions (to be validated with users):

Time saved per interaction: ~5–10 minutes

Monthly usage: e.g., 1,500 summarization requests

FTE cost/hour: $30

Sample benefit calculation:
(8 min saved / 60) × 1,500 × $30 = $6,000/month

This is a conservative estimate. I’ll validate the figures with users and refine the benefit calculation accordingly.

Best regards,
[Your Name]

Would you like help preparing a quick user feedback form or survey to gather this data faster?






Do you like this personality?














ChatGPT can make mistakes. Check important info. See Cookie Preference





# Objective:
You are an AI assistant that extracts data from document images and returns it in structured markdown format, preserving the layout and formatting exactly as shown in the image.

# Instructions:

1. You are provided with images of PDF pages. Go through **each image carefully** and extract **all visible information**.

2. Format the extracted information in markdown style, **replicating the visual layout of the PDF** (text structure, tables, images, bullet points).

3. For **images, tables, and screenshots**, preserve:
   - Borders
   - Cell alignment
   - Font styles (bold, italics if applicable)
   - Highlighted or bold text
   - If present, add an inline description:
     - Use `image_description:` for images
     - Use `table_description:` for tables
     - Use `screenshot_description:` for screenshots

4. Store all extracted images in a folder named `images/` in `.png` format, and insert them in markdown as:

5. **Do not include any footers, headers, or page numbers** from the PDF.

6. **Extract the text exactly** as it appears in the image, including punctuation, symbols, and formatting.

7. If the document contains **errors, warnings, or stack traces**, skip them unless explicitly asked to extract them.

8. **Important Markdown Formatting Rules**:
- Never use `#`, `##`, `###`, or `**bold**` for headings unless the line is a clear section title (e.g., appears top-centered with larger font or boxed).
- If the content is a **list of similar items** (e.g., report names, fund types, field labels), always format them as bullet points using `-`, even if they are bold or appear like headings.
- Example:
  ```
  - Base Equivalent Cash Statement
  - Earned Income by Asset ID
  ```

9. If two or more items appear one after another (e.g., 17 report names), even across pages, **treat them as a continuous bullet list**.

10. Do not return the markdown content inside code blocks (no triple backticks).

# Output:
A structured markdown representation of all PDF content with:
- Correct visual layout
- Tables, images, and screenshots with descriptions
- Lists as bullet points
- No extra formatting like headings unless visually justified





Download 17 MYSS excel reports and run MYSS recon macro for external and internal funds.

Use the same method to create template and download 17 MYSS excel reports as below.

Holdings Alpha

Net Asset Value

Dividend Payable

Dividend Receivable

FX Pending Forward Activity

FX Pending Spot Activity

Interest Payable

Interest Receivable

MBS Interest Payable

MBS Interest Receivable

Open Trades

Tax Expense Payable

Tax Reclaim Receivable

Dividend Income Summary

Realized Gain Loss

Processing Page 8

c:\venv\Lib\site-packages\ssrai\utils\request utils.py:15: PydanticDeprecatedSince20: The `dict' method is depre

session.proxies = proxy.dict()

c:\venv\Lib\site-packages\urllib3\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is b

warnings.warn(

Response: **Base Equivalent Cash Statement**

**Earned Income by Asset ID**







import os
import base64
import pdfplumber
from PIL import Image
from io import BytesIO
from langchain.schema import HumanMessage
from ssrai import SSRAIClient

def extract_text_from_pdf(pdf_path, llm, output_folder="pdf_images"):
    os.makedirs(output_folder, exist_ok=True)
    extract_list = []

    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            print(f"\n--- Processing Page {i + 1} ---\n")

            # Convert page to image
            page_image = page.to_image(resolution=300)
            image_path = os.path.join(output_folder, f"page_{i+1}.png")
            page_image.save(image_path, format="PNG")

            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            message = HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": """You are an AI assistant that extracts data from documents and returns them as structured markdown.
                        
- Go through the image of the PDF page.
- Extract details exactly as seen.
- Preserve tables, styles, bold text, and structure.
- Add short inline descriptions starting with `image_description:`, `table_description:`, etc.
- Return in markdown format.
- Do not use code blocks."""
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            )

            response = llm.invoke([message])
            print("Response:", response.content)
            extract_list.append(response.content)

    return extract_list





import pandas as pd
import ast
from datasets import Dataset
from ragas.metrics import faithfulness, answer_correctness, answer_relevancy
from ragas import evaluate

# === Preprocessing: Extract pageContentExtract ===
def extract_pageContentExtract(meta_val):
    try:
        if pd.isna(meta_val):
            return ""
        if isinstance(meta_val, str):
            parsed = ast.literal_eval(meta_val)
        elif isinstance(meta_val, list):
            parsed = meta_val
        else:
            return ""

        if isinstance(parsed, list) and len(parsed) > 0 and isinstance(parsed[0], dict):
            return parsed[0].get("pageContent Extract", "")
        return ""
    except Exception as e:
        print(f"Failed to parse: {meta_val}\nError: {e}")
        return ""

# === Prepare Dataset for RAGAS ===
def prepare_dataset(df):
    records = {
        "question": df["user_query"].astype(str),
        "answer": df["bot_response"].astype(str),
        "contexts": df["context"].astype(str).apply(lambda x: [x]),  # list of one context string
    }
    return Dataset.from_dict(records)

# === Main RAGAS Evaluation Pipeline ===
def run_ragas_evaluation(input_excel_path, output_excel_path):
    # Load Excel
    df = pd.read_excel(input_excel_path)

    # Extract context using your custom function
    df["context"] = df["source_context"].apply(extract_pageContentExtract)

    # Fill NA to avoid crashes
    df.fillna("", inplace=True)

    # Prepare for RAGAS
    ragas_dataset = prepare_dataset(df)

    # Run evaluation
    results = evaluate(
        ragas_dataset,
        metrics=[faithfulness, answer_correctness, answer_relevancy]
    )

    # Add scores (converted to percentages)
    df["Faithfulness (%)"] = [round(score * 100, 2) for score in results["faithfulness"]]
    df["Answer Correctness (%)"] = [round(score * 100, 2) for score in results["answer_correctness"]]
    df["Answer Relevancy (%)"] = [round(score * 100, 2) for score in results["answer_relevancy"]]

    # Save output
    df.to_excel(output_excel_path, index=False)
    print(f"✅ Evaluation complete! Output saved to: {output_excel_path}")

# === Execute Script ===
if __name__ == "__main__":
    input_excel = r"\\mfgdcu02\p872643\Desktop\ssgenai_evalution\Audit assistance questions.xlsx"
    output_excel = "evaluation_output_ragas.xlsx"
    run_ragas_evaluation(input_excel, output_excel)
